<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>6.6 Métodos de regularización | Aprendizaje Estadístico</title>
  <meta name="description" content="Apuntes de la asignatura de Aprendizaje Estadístico del Máster en Técnicas Estadísticas." />
  <meta name="generator" content="bookdown 0.18 and GitBook 2.6.7" />

  <meta property="og:title" content="6.6 Métodos de regularización | Aprendizaje Estadístico" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Apuntes de la asignatura de Aprendizaje Estadístico del Máster en Técnicas Estadísticas." />
  <meta name="github-repo" content="rubenfcasal/aprendizaje_estadistico" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="6.6 Métodos de regularización | Aprendizaje Estadístico" />
  
  <meta name="twitter:description" content="Apuntes de la asignatura de Aprendizaje Estadístico del Máster en Técnicas Estadísticas." />
  

<meta name="author" content="Rubén Fernández Casal (ruben.fcasal@udc.es), Julián Costa (julian.costa@udc.es)" />


<meta name="date" content="2020-11-17" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="evaluación-de-la-precisión.html"/>
<link rel="next" href="métodos-de-reducción-de-la-dimensión.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />









<script src="libs/htmlwidgets-1.5.1/htmlwidgets.js"></script>
<link href="libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet" />
<script src="libs/datatables-binding-0.13/datatables.js"></script>
<link href="libs/dt-core-1.10.20/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.10.20/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.10.20/js/jquery.dataTables.min.js"></script>
<link href="libs/crosstalk-1.1.0.1/css/crosstalk.css" rel="stylesheet" />
<script src="libs/crosstalk-1.1.0.1/js/crosstalk.min.js"></script>


<style type="text/css">
code.sourceCode > span { display: inline-block; line-height: 1.25; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Aprendizaje Estadístico</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Prólogo</a></li>
<li class="chapter" data-level="1" data-path="intro-AE.html"><a href="intro-AE.html"><i class="fa fa-check"></i><b>1</b> Introducción al Aprendizaje Estadístico</a><ul>
<li class="chapter" data-level="1.1" data-path="aprendizaje-estadístico-vs-aprendizaje-automático.html"><a href="aprendizaje-estadístico-vs-aprendizaje-automático.html"><i class="fa fa-check"></i><b>1.1</b> Aprendizaje Estadístico vs. Aprendizaje Automático</a><ul>
<li class="chapter" data-level="1.1.1" data-path="aprendizaje-estadístico-vs-aprendizaje-automático.html"><a href="aprendizaje-estadístico-vs-aprendizaje-automático.html#machine-learning-vs.-data-mining"><i class="fa fa-check"></i><b>1.1.1</b> Machine Learning vs. Data Mining</a></li>
<li class="chapter" data-level="1.1.2" data-path="aprendizaje-estadístico-vs-aprendizaje-automático.html"><a href="aprendizaje-estadístico-vs-aprendizaje-automático.html#las-dos-culturas-breiman-2001"><i class="fa fa-check"></i><b>1.1.2</b> Las dos culturas (Breiman, 2001)</a></li>
<li class="chapter" data-level="1.1.3" data-path="aprendizaje-estadístico-vs-aprendizaje-automático.html"><a href="aprendizaje-estadístico-vs-aprendizaje-automático.html#machine-learning-vs.-estadística-dunson-2018"><i class="fa fa-check"></i><b>1.1.3</b> Machine Learning vs. Estadística (Dunson, 2018)</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="métodos-de-aprendizaje-estadístico.html"><a href="métodos-de-aprendizaje-estadístico.html"><i class="fa fa-check"></i><b>1.2</b> Métodos de Aprendizaje Estadístico</a><ul>
<li class="chapter" data-level="1.2.1" data-path="métodos-de-aprendizaje-estadístico.html"><a href="métodos-de-aprendizaje-estadístico.html#notacion"><i class="fa fa-check"></i><b>1.2.1</b> Notación y terminología</a></li>
<li class="chapter" data-level="1.2.2" data-path="métodos-de-aprendizaje-estadístico.html"><a href="métodos-de-aprendizaje-estadístico.html#metodos-pkgs"><i class="fa fa-check"></i><b>1.2.2</b> Métodos (de aprendizaje supervisado) y paquetes de R</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="const-eval.html"><a href="const-eval.html"><i class="fa fa-check"></i><b>1.3</b> Construcción y evaluación de los modelos</a><ul>
<li class="chapter" data-level="1.3.1" data-path="const-eval.html"><a href="const-eval.html#bias-variance"><i class="fa fa-check"></i><b>1.3.1</b> Equilibrio entre sesgo y varianza: infraajuste y sobreajuste</a></li>
<li class="chapter" data-level="1.3.2" data-path="const-eval.html"><a href="const-eval.html#entrenamiento-test"><i class="fa fa-check"></i><b>1.3.2</b> Datos de entrenamiento y datos de test</a></li>
<li class="chapter" data-level="1.3.3" data-path="const-eval.html"><a href="const-eval.html#cv"><i class="fa fa-check"></i><b>1.3.3</b> Validación cruzada</a></li>
<li class="chapter" data-level="1.3.4" data-path="const-eval.html"><a href="const-eval.html#eval-reg"><i class="fa fa-check"></i><b>1.3.4</b> Evaluación de un método de regresión</a></li>
<li class="chapter" data-level="1.3.5" data-path="const-eval.html"><a href="const-eval.html#eval-class"><i class="fa fa-check"></i><b>1.3.5</b> Evaluación de un método de clasificación</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="la-maldición-de-la-dimensionalidad.html"><a href="la-maldición-de-la-dimensionalidad.html"><i class="fa fa-check"></i><b>1.4</b> La maldición de la dimensionalidad</a></li>
<li class="chapter" data-level="1.5" data-path="analisis-modelos.html"><a href="analisis-modelos.html"><i class="fa fa-check"></i><b>1.5</b> Análisis e interpretación de los modelos</a></li>
<li class="chapter" data-level="1.6" data-path="caret.html"><a href="caret.html"><i class="fa fa-check"></i><b>1.6</b> Introducción al paquete <code>caret</code></a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="trees.html"><a href="trees.html"><i class="fa fa-check"></i><b>2</b> Árboles de decisión</a><ul>
<li class="chapter" data-level="2.1" data-path="árboles-de-regresión-cart.html"><a href="árboles-de-regresión-cart.html"><i class="fa fa-check"></i><b>2.1</b> Árboles de regresión CART</a></li>
<li class="chapter" data-level="2.2" data-path="árboles-de-clasificación-cart.html"><a href="árboles-de-clasificación-cart.html"><i class="fa fa-check"></i><b>2.2</b> Árboles de clasificación CART</a></li>
<li class="chapter" data-level="2.3" data-path="cart-con-el-paquete-rpart.html"><a href="cart-con-el-paquete-rpart.html"><i class="fa fa-check"></i><b>2.3</b> CART con el paquete <code>rpart</code></a><ul>
<li class="chapter" data-level="2.3.1" data-path="cart-con-el-paquete-rpart.html"><a href="cart-con-el-paquete-rpart.html#ejemplo-regresión"><i class="fa fa-check"></i><b>2.3.1</b> Ejemplo: regresión</a></li>
<li class="chapter" data-level="2.3.2" data-path="cart-con-el-paquete-rpart.html"><a href="cart-con-el-paquete-rpart.html#class-rpart"><i class="fa fa-check"></i><b>2.3.2</b> Ejemplo: modelo de clasificación</a></li>
<li class="chapter" data-level="2.3.3" data-path="cart-con-el-paquete-rpart.html"><a href="cart-con-el-paquete-rpart.html#interfaz-de-caret"><i class="fa fa-check"></i><b>2.3.3</b> Interfaz de <code>caret</code></a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="alternativas-a-los-árboles-cart.html"><a href="alternativas-a-los-árboles-cart.html"><i class="fa fa-check"></i><b>2.4</b> Alternativas a los árboles CART</a><ul>
<li class="chapter" data-level="2.4.1" data-path="alternativas-a-los-árboles-cart.html"><a href="alternativas-a-los-árboles-cart.html#ejemplo"><i class="fa fa-check"></i><b>2.4.1</b> Ejemplo</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="bagging-boosting.html"><a href="bagging-boosting.html"><i class="fa fa-check"></i><b>3</b> Bagging y Boosting</a><ul>
<li class="chapter" data-level="3.1" data-path="bagging.html"><a href="bagging.html"><i class="fa fa-check"></i><b>3.1</b> Bagging</a></li>
<li class="chapter" data-level="3.2" data-path="bosques-aleatorios.html"><a href="bosques-aleatorios.html"><i class="fa fa-check"></i><b>3.2</b> Bosques aleatorios</a></li>
<li class="chapter" data-level="3.3" data-path="bagging-rf-r.html"><a href="bagging-rf-r.html"><i class="fa fa-check"></i><b>3.3</b> Bagging y bosques aleatorios en R</a><ul>
<li class="chapter" data-level="3.3.1" data-path="bagging-rf-r.html"><a href="bagging-rf-r.html#ejemplo-clasificación-con-bagging"><i class="fa fa-check"></i><b>3.3.1</b> Ejemplo: Clasificación con bagging</a></li>
<li class="chapter" data-level="3.3.2" data-path="bagging-rf-r.html"><a href="bagging-rf-r.html#ejemplo-clasificación-con-bosques-aleatorios"><i class="fa fa-check"></i><b>3.3.2</b> Ejemplo: Clasificación con bosques aleatorios</a></li>
<li class="chapter" data-level="3.3.3" data-path="bagging-rf-r.html"><a href="bagging-rf-r.html#ejemplo-bosques-aleatorios-con-caret"><i class="fa fa-check"></i><b>3.3.3</b> Ejemplo: bosques aleatorios con <code>caret</code></a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="boosting.html"><a href="boosting.html"><i class="fa fa-check"></i><b>3.4</b> Boosting</a></li>
<li class="chapter" data-level="3.5" data-path="boosting-en-r.html"><a href="boosting-en-r.html"><i class="fa fa-check"></i><b>3.5</b> Boosting en R</a><ul>
<li class="chapter" data-level="3.5.1" data-path="boosting-en-r.html"><a href="boosting-en-r.html#ejemplo-clasificación-con-el-paquete-ada"><i class="fa fa-check"></i><b>3.5.1</b> Ejemplo: clasificación con el paquete <code>ada</code></a></li>
<li class="chapter" data-level="3.5.2" data-path="boosting-en-r.html"><a href="boosting-en-r.html#ejemplo-regresión-con-el-paquete-gbm"><i class="fa fa-check"></i><b>3.5.2</b> Ejemplo: regresión con el paquete <code>gbm</code></a></li>
<li class="chapter" data-level="3.5.3" data-path="boosting-en-r.html"><a href="boosting-en-r.html#ejemplo-xgboost-con-el-paquete-caret"><i class="fa fa-check"></i><b>3.5.3</b> Ejemplo: XGBoost con el paquete <code>caret</code></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="svm.html"><a href="svm.html"><i class="fa fa-check"></i><b>4</b> Máquinas de soporte vectorial</a><ul>
<li class="chapter" data-level="4.1" data-path="clasificadores-de-máximo-margen.html"><a href="clasificadores-de-máximo-margen.html"><i class="fa fa-check"></i><b>4.1</b> Clasificadores de máximo margen</a></li>
<li class="chapter" data-level="4.2" data-path="clasificadores-de-soporte-vectorial.html"><a href="clasificadores-de-soporte-vectorial.html"><i class="fa fa-check"></i><b>4.2</b> Clasificadores de soporte vectorial</a></li>
<li class="chapter" data-level="4.3" data-path="máquinas-de-soporte-vectorial.html"><a href="máquinas-de-soporte-vectorial.html"><i class="fa fa-check"></i><b>4.3</b> Máquinas de soporte vectorial</a><ul>
<li class="chapter" data-level="4.3.1" data-path="máquinas-de-soporte-vectorial.html"><a href="máquinas-de-soporte-vectorial.html#clasificación-con-más-de-dos-categorías"><i class="fa fa-check"></i><b>4.3.1</b> Clasificación con más de dos categorías</a></li>
<li class="chapter" data-level="4.3.2" data-path="máquinas-de-soporte-vectorial.html"><a href="máquinas-de-soporte-vectorial.html#regresión"><i class="fa fa-check"></i><b>4.3.2</b> Regresión</a></li>
<li class="chapter" data-level="4.3.3" data-path="máquinas-de-soporte-vectorial.html"><a href="máquinas-de-soporte-vectorial.html#ventajas-e-incovenientes"><i class="fa fa-check"></i><b>4.3.3</b> Ventajas e incovenientes</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="svm-con-el-paquete-kernlab.html"><a href="svm-con-el-paquete-kernlab.html"><i class="fa fa-check"></i><b>4.4</b> SVM con el paquete <code>kernlab</code></a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="class-otros.html"><a href="class-otros.html"><i class="fa fa-check"></i><b>5</b> Otros métodos de clasificación</a><ul>
<li class="chapter" data-level="5.1" data-path="análisis-discriminate-lineal.html"><a href="análisis-discriminate-lineal.html"><i class="fa fa-check"></i><b>5.1</b> Análisis discriminate lineal</a><ul>
<li class="chapter" data-level="5.1.1" data-path="análisis-discriminate-lineal.html"><a href="análisis-discriminate-lineal.html#ejemplo-masslda"><i class="fa fa-check"></i><b>5.1.1</b> Ejemplo <code>MASS::lda</code></a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="análisis-discriminante-cuadrático.html"><a href="análisis-discriminante-cuadrático.html"><i class="fa fa-check"></i><b>5.2</b> Análisis discriminante cuadrático</a><ul>
<li class="chapter" data-level="5.2.1" data-path="análisis-discriminante-cuadrático.html"><a href="análisis-discriminante-cuadrático.html#ejemplo-massqda"><i class="fa fa-check"></i><b>5.2.1</b> Ejemplo <code>MASS::qda</code></a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="naive-bayes.html"><a href="naive-bayes.html"><i class="fa fa-check"></i><b>5.3</b> Naive Bayes</a><ul>
<li class="chapter" data-level="5.3.1" data-path="naive-bayes.html"><a href="naive-bayes.html#ejemplo-e1071naivebayes"><i class="fa fa-check"></i><b>5.3.1</b> Ejemplo <code>e1071::naiveBayes</code></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="modelos-lineales.html"><a href="modelos-lineales.html"><i class="fa fa-check"></i><b>6</b> Modelos lineales y extensiones</a><ul>
<li class="chapter" data-level="6.1" data-path="reg-multiple.html"><a href="reg-multiple.html"><i class="fa fa-check"></i><b>6.1</b> Regresión lineal múltiple</a><ul>
<li class="chapter" data-level="6.1.1" data-path="reg-multiple.html"><a href="reg-multiple.html#ajuste-función-lm"><i class="fa fa-check"></i><b>6.1.1</b> Ajuste: función <code>lm</code></a></li>
<li class="chapter" data-level="6.1.2" data-path="reg-multiple.html"><a href="reg-multiple.html#ejemplo-1"><i class="fa fa-check"></i><b>6.1.2</b> Ejemplo</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="multicolinealidad.html"><a href="multicolinealidad.html"><i class="fa fa-check"></i><b>6.2</b> El problema de la multicolinelidad</a></li>
<li class="chapter" data-level="6.3" data-path="seleccion-reg-lineal.html"><a href="seleccion-reg-lineal.html"><i class="fa fa-check"></i><b>6.3</b> Selección de variables explicativas</a><ul>
<li class="chapter" data-level="6.3.1" data-path="seleccion-reg-lineal.html"><a href="seleccion-reg-lineal.html#búsqueda-exhaustiva"><i class="fa fa-check"></i><b>6.3.1</b> Búsqueda exhaustiva</a></li>
<li class="chapter" data-level="6.3.2" data-path="seleccion-reg-lineal.html"><a href="seleccion-reg-lineal.html#selección-por-pasos"><i class="fa fa-check"></i><b>6.3.2</b> Selección por pasos</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="análisis-e-interpretación-del-modelo.html"><a href="análisis-e-interpretación-del-modelo.html"><i class="fa fa-check"></i><b>6.4</b> Análisis e interpretación del modelo</a></li>
<li class="chapter" data-level="6.5" data-path="evaluación-de-la-precisión.html"><a href="evaluación-de-la-precisión.html"><i class="fa fa-check"></i><b>6.5</b> Evaluación de la precisión</a></li>
<li class="chapter" data-level="6.6" data-path="métodos-de-regularización.html"><a href="métodos-de-regularización.html"><i class="fa fa-check"></i><b>6.6</b> Métodos de regularización</a><ul>
<li class="chapter" data-level="6.6.1" data-path="métodos-de-regularización.html"><a href="métodos-de-regularización.html#implementación-en-r"><i class="fa fa-check"></i><b>6.6.1</b> Implementación en R</a></li>
<li class="chapter" data-level="6.6.2" data-path="métodos-de-regularización.html"><a href="métodos-de-regularización.html#ejemplo-ridge-regression"><i class="fa fa-check"></i><b>6.6.2</b> Ejemplo: Ridge Regression</a></li>
<li class="chapter" data-level="6.6.3" data-path="métodos-de-regularización.html"><a href="métodos-de-regularización.html#ejemplo-lasso"><i class="fa fa-check"></i><b>6.6.3</b> Ejemplo: Lasso</a></li>
<li class="chapter" data-level="6.6.4" data-path="métodos-de-regularización.html"><a href="métodos-de-regularización.html#ejemplo-elastic-net"><i class="fa fa-check"></i><b>6.6.4</b> Ejemplo: Elastic Net</a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="métodos-de-reducción-de-la-dimensión.html"><a href="métodos-de-reducción-de-la-dimensión.html"><i class="fa fa-check"></i><b>6.7</b> Métodos de reducción de la dimensión</a></li>
<li class="chapter" data-level="6.8" data-path="reg-glm.html"><a href="reg-glm.html"><i class="fa fa-check"></i><b>6.8</b> Modelos lineales generalizados</a><ul>
<li class="chapter" data-level="6.8.1" data-path="reg-glm.html"><a href="reg-glm.html#ajuste-función-glm"><i class="fa fa-check"></i><b>6.8.1</b> Ajuste: función <code>glm</code></a></li>
<li class="chapter" data-level="6.8.2" data-path="reg-glm.html"><a href="reg-glm.html#ejemplo-regresión-logística"><i class="fa fa-check"></i><b>6.8.2</b> Ejemplo: Regresión logística</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="" data-path="referencias.html"><a href="referencias.html"><i class="fa fa-check"></i>Referencias</a><ul>
<li class="chapter" data-level="" data-path="bibliografía-básica.html"><a href="bibliografía-básica.html"><i class="fa fa-check"></i>Bibliografía básica</a></li>
<li class="chapter" data-level="" data-path="bibliografía-complementaria.html"><a href="bibliografía-complementaria.html"><i class="fa fa-check"></i>Bibliografía complementaria</a><ul>
<li class="chapter" data-level="" data-path="bibliografía-complementaria.html"><a href="bibliografía-complementaria.html#libros"><i class="fa fa-check"></i>Libros</a></li>
<li class="chapter" data-level="" data-path="bibliografía-complementaria.html"><a href="bibliografía-complementaria.html#artículos"><i class="fa fa-check"></i>Artículos</a></li>
</ul></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Aprendizaje Estadístico</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="métodos-de-regularización" class="section level2">
<h2><span class="header-section-number">6.6</span> Métodos de regularización</h2>
<p>Como ya se comentó, el procedimiento habitual para ajustar un modelo de regresión lineal es emplear mínimos cuadrados, es decir, utilizar como criterio de error la suma de cuadrados residual
<span class="math display">\[\mbox{RSS} = \sum\limits_{i=1}^{n}\left(  y_{i} - \beta_0 - \boldsymbol{\beta}^t \mathbf{x}_{i} \right)^{2}\]</span></p>
<p>Si el modelo lineal es razonablemente adecuado, utilizar <span class="math inline">\(\mbox{RSS}\)</span> va a dar lugar a estimaciones con poco sesgo, y si además <span class="math inline">\(n\gg p\)</span>, entonces el modelo también va a tener poca varianza (bajo las hipótesis estructurales, la estimación es insesgada y además de varianza mínima entre todas las técnicas insesgadas).
Las dificultades surgen cuando <span class="math inline">\(p\)</span> es grande o cuando hay correlaciones altas entre las variables predictoras: tener muchas variables dificulta la interpretación del modelo, y si además hay problemas de colinealidad o se incumple <span class="math inline">\(n\gg p\)</span>, entonces la estimación del modelo va a tener muchas varianza y el modelo estará sobreajustado.
La solución pasa por forzar a que el modelo tenga menos complejidad para así reducir su varianza.
Una forma de conseguirlo es mediante la regularización (<em>regularization</em> o <em>shrinkage</em>) de la estimación de los parámetros <span class="math inline">\(\beta_1, \beta_2,\ldots, \beta_p\)</span> que consiste en considerar todas las variables predictoras pero forzando a que algunos de los parámetros se estimen mediante valores muy próximos a cero, o directamente con ceros.
Esta técnica va a provocar un pequeño aumento en el sesgo pero a cambio una notable reducción en la varianza y una interpretación más sencilla del modelo resultante.</p>
<p>Hay dos formas básicas de lograr esta simplificación de los parámetros (con la consiguiente simplificación del modelo), utilizando una penalización cuadrática (norma <span class="math inline">\(L_2\)</span>) o en valor absoluto (norma <span class="math inline">\(L_1\)</span>):</p>
<ul>
<li><p><em>Ridge regression</em> (Hoerl, 1970)
<span class="math display">\[\mbox{min}_{\beta_0, \boldsymbol{\beta}} \mbox{RSS} + \lambda\sum_{j=1}^{p}\beta_{j}^{2}\]</span></p>
<p>Equivalentemente,
<span class="math display">\[\mbox{min}_{\beta_0, \boldsymbol{\beta}} \mbox{RSS}\]</span>
sujeto a
<span class="math display">\[\sum_{j=1}^{p}\beta_{j}^{2} \le s\]</span></p></li>
<li><p><em>Lasso</em> (<em>least absolute shrinkage and selection operator</em>, Tibshirani, 1996)
<span class="math display">\[\mbox{min}_{\beta_0, \boldsymbol{\beta}} RSS + \lambda\sum_{j=1}^{p}|\beta_{j}|\]</span></p>
<p>Equivalentemente,
<span class="math display">\[\mbox{min}_{\beta_0, \boldsymbol{\beta}} \mbox{RSS}\]</span>
sujeto a
<span class="math display">\[\sum_{j=1}^{p}|\beta_{j}| \le s\]</span></p></li>
</ul>
<p>Una formulación unificada consiste en considerar el problema
<span class="math display">\[\mbox{min}_{\beta_0, \boldsymbol{\beta}} RSS + \lambda\sum_{j=1}^{p}|\beta_{j}|^d\]</span></p>
<p>Si <span class="math inline">\(d=0\)</span>, la penalización consiste en el número de variables utilizadas, por tanto se corresponde con el problema de selección de variables; <span class="math inline">\(d=1\)</span> se corresponde con <em>lasso</em> y <span class="math inline">\(d=2\)</span> con <em>ridge</em>.</p>
<p>La ventaja de utilizar <em>lasso</em> es que va a forzar a que algunos parámetros sean cero, con lo cual también se realiza una selección de las variables más influyentes.
Por el contrario, <em>ridge regression</em> va a incluir todas las variables predictoras en el modelo final, si bien es cierto que algunas con parámetros muy próximos a cero: de este modo va a reducir el riesgo del sobreajuste, pero no resuelve el problema de la interpretabilidad.
Otra ventaja de utilizar <em>lasso</em> es que hace un mejor tratamiento de las variables predictoras correlacionadas al tener tendencia a seleccionar una y anular las demás (esto también puede verse como un inconveniente, ya que pequeños cambios en los datos pueden dar lugar a distintos modelos).</p>
<p>Dos generalizaciones de <em>lasso</em> son <em>least angle regression</em> (LARS, Efron et al., 2004) y <em>elastic net</em> (Zou y Hastie, 2005).
<em>Elastic net</em> combina las ventajas de <em>ridge</em> y <em>lasso</em>, minimizando
<span class="math display">\[\mbox{min}_{\beta_0, \boldsymbol{\beta}} \ \mbox{RSS} + \lambda \left( \frac{1 - \alpha}{2}\sum_{j=1}^{p}\beta_{j}^{2} + \alpha \sum_{j=1}^{p}|\beta_{j}| \right)\]</span>
con <span class="math inline">\(0 \leq \alpha \leq 1\)</span>.</p>
<!-- LARS parte de coeficientes nulos y, simplificando, los va aumentando en la dirección de mínimos cuadrados (o minimizando otro criterio de error) de forma incremental, añadiendo secuencialmente el coeficiente de la variable que está más correlacionada con los residuos -->
<p>Es muy importante estandarizar (centrar y reescalar) las variables predictoras antes de realizar estas técnicas.
Fijémonos en que, así como <span class="math inline">\(\mbox{RSS}\)</span> es insensible a los cambios de escala, la penalización es muy sensible.
Previa estandarización, el término independiente <span class="math inline">\(\beta_0\)</span> (que no interviene en la penalización) tiene una interpretación muy directa, ya que
<span class="math display">\[\widehat \beta_0 = \bar y =\sum_{i=1}^n \frac{y_i}{n}\]</span></p>
<p>Los dos métodos de regularización comentados dependen del hiperparámetro <span class="math inline">\(\lambda\)</span> (equivalentemente, <span class="math inline">\(s\)</span>).
Es muy importante seleccionar adecuadamente el valor del hiperparámetro, por ejemplo utilizando <em>validación cruzada</em>.
Hay algoritmos muy eficientes que permiten el ajuste, tanto de <em>ridge regression</em> como de <em>lasso</em> de forma conjunta (simultánea) para todos los valores de <span class="math inline">\(\lambda\)</span>.</p>
<div id="implementación-en-r" class="section level3">
<h3><span class="header-section-number">6.6.1</span> Implementación en R</h3>
<p>Hay varios paquetes que implementan estos métodos: <code>h2o</code>, <code>elasticnet</code>, <code>penalized</code>, <code>lasso2</code>, <code>biglasso</code>, etc., pero el paquete <a href="https://glmnet.stanford.edu"><code>glmnet</code></a> utiliza una de las más eficientes.</p>
<div class="sourceCode" id="cb306"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb306-1"><a href="métodos-de-regularización.html#cb306-1"></a><span class="kw">library</span>(glmnet)</span></code></pre></div>
<p>El paquete <code>glmnet</code> no emplea formulación de modelos, hay que establecer la respuesta
<code>y</code> y una matriz o data.frame con las variables explicativas <code>x</code>.
Además, no admite predictores categóricos ni datos faltantes, por lo que puede ser recomendable emplear la función <code>model.matrix()</code> para construir la matriz de diseño <code>x</code> (o <code>Matrix::sparse.model.matrix()</code> si el conjunto de datos es muy grande) a partir de una fórmula (alternativamente se pueden emplear la herramientas implementadas en el paquete <code>caret</code>).</p>
<p>La función principal es:</p>
<div class="sourceCode" id="cb307"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb307-1"><a href="métodos-de-regularización.html#cb307-1"></a><span class="kw">glmnet</span>(x, y, family, <span class="dt">alpha =</span> <span class="dv">1</span>, <span class="dt">lambda =</span> <span class="ot">NULL</span>, ...)</span></code></pre></div>
<ul>
<li><p><code>family</code>: familia del modelo lineal generalizado (ver Sección <a href="reg-glm.html#reg-glm">6.8</a>); por defecto <code>"gaussian"</code> (modelo lineal con ajuste cuadrático), también admite <code>"binomial"</code>, <code>"poisson"</code>, <code>"multinomial"</code>, <code>"cox"</code> o <code>"mgaussian"</code> (modelo lineal con respuesta multivariante).</p></li>
<li><p><code>alpha</code>: parámetro <span class="math inline">\(\alpha\)</span> de elasticnet <span class="math inline">\(0 \leq \alpha \leq 1\)</span>. Por defecto <code>alpha = 1</code> penalización <em>lasso</em> (<code>alpha = 0</code> para <em>ridge regression</em>).</p></li>
<li><p><code>lambda</code>: secuencia (opcional) de valores de <span class="math inline">\(\lambda\)</span>; si no se especifica se establece una secuencia por defecto (en base a los argumentos adicionales <code>nlambda</code> y <code>lambda.min.ratio</code>). Se devolverán los ajustes para todos los valores de esta secuencia (también se podrán obtener posteriormente para otros valores).</p></li>
</ul>
<p>Entre los métodos genéricos disponibles del objeto resultante, <code>coef()</code> y <code>predict()</code> permiten obtener los coeficientes y las predicciones para un valor concreto de <span class="math inline">\(\lambda\)</span>, que se debe especificar mediante el argumento <code>s = valor</code> (“For historical reasons we use the symbol ‘s’ rather than ‘lambda’”).</p>
<p>Aunque para seleccionar el un valor “óptimo” del hiperparámetro <span class="math inline">\(\lambda\)</span> (mediante validación cruzada) se puede emplear:</p>
<div class="sourceCode" id="cb308"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb308-1"><a href="métodos-de-regularización.html#cb308-1"></a><span class="kw">cv.glmnet</span>(x, y, family, alpha, lambda, <span class="dt">type.measure =</span> <span class="st">&quot;default&quot;</span>, <span class="dt">nfolds =</span> <span class="dv">10</span>, ...)</span></code></pre></div>
<p>Esta función también devuelve los ajustes con toda la muestra de entrenamiento (en la componente <code>$glmnet.fit</code>) y se puede emplear el resultado directamente para predecir o obtener los coeficientes del modelo.
Por defecto seleccionando <span class="math inline">\(\lambda\)</span> mediante la regla de “un error estándar” de Breiman et al. (1984) (componente <code>$lambda.1se</code>), aunque también calcula el valor óptimo (componente <code>$lambda.min</code>; que se puede seleccionar con estableciendo <code>s = "lambda.min"</code>).</p>
<p>Para más detalles consultar la vignette del paquete <a href="https://glmnet.stanford.edu/articles/glmnet.html">An Introduction to glmnet</a>.</p>
<p>Continuaremos con el ejemplo de los datos de clientes de la compañía de distribución industrial HBAT (en este caso todos los predictores son numéricos y no hay datos faltantes):</p>
<div class="sourceCode" id="cb309"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb309-1"><a href="métodos-de-regularización.html#cb309-1"></a>x &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(train[, <span class="dv">-14</span>])</span>
<span id="cb309-2"><a href="métodos-de-regularización.html#cb309-2"></a>y &lt;-<span class="st"> </span>train<span class="op">$</span>fidelida</span></code></pre></div>
</div>
<div id="ejemplo-ridge-regression" class="section level3">
<h3><span class="header-section-number">6.6.2</span> Ejemplo: Ridge Regression</h3>
<p>Ajustamos los modelos de regresión ridge (con la secuencia de valores de <span class="math inline">\(\lambda\)</span> por defecto) con la función <code>glmnet()</code> con <code>alpha=0</code> (ridge penalty):</p>
<div class="sourceCode" id="cb310"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb310-1"><a href="métodos-de-regularización.html#cb310-1"></a>fit.ridge &lt;-<span class="st"> </span><span class="kw">glmnet</span>(x, y, <span class="dt">alpha =</span> <span class="dv">0</span>)</span>
<span id="cb310-2"><a href="métodos-de-regularización.html#cb310-2"></a><span class="kw">plot</span>(fit.ridge, <span class="dt">xvar =</span> <span class="st">&quot;lambda&quot;</span>, <span class="dt">label =</span> <span class="ot">TRUE</span>)</span></code></pre></div>
<p><img src="06-modelos_lineales_files/figure-html/unnamed-chunk-28-1.png" width="80%" style="display: block; margin: auto;" /></p>
<p>Podemos obtener el modelo o predicciones para un valor concreto de <span class="math inline">\(\lambda\)</span>:</p>
<div class="sourceCode" id="cb311"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb311-1"><a href="métodos-de-regularización.html#cb311-1"></a><span class="kw">coef</span>(fit.ridge, <span class="dt">s =</span> <span class="dv">2</span>) <span class="co"># lambda = 2</span></span></code></pre></div>
<pre><code>## 14 x 1 sparse Matrix of class &quot;dgCMatrix&quot;
##                       1
## (Intercept)  3.56806743
## calidadp     2.41027431
## web          0.94414628
## soporte     -0.22183509
## quejas       1.08417665
## publi        0.20121976
## producto     1.41018809
## imgfvent     0.21140360
## precio       0.26171759
## garantia     0.07110803
## nprod        0.04859325
## facturac     0.22695054
## flexprec     0.37732748
## velocida     3.11101217</code></pre>
<p>Para seleccionar el parámetro de penalización por validación cruzada empleamos <code>cv.glmnet()</code>:</p>
<div class="sourceCode" id="cb313"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb313-1"><a href="métodos-de-regularización.html#cb313-1"></a><span class="kw">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb313-2"><a href="métodos-de-regularización.html#cb313-2"></a>cv.ridge &lt;-<span class="st"> </span><span class="kw">cv.glmnet</span>(x, y, <span class="dt">alpha =</span> <span class="dv">0</span>)</span>
<span id="cb313-3"><a href="métodos-de-regularización.html#cb313-3"></a><span class="kw">plot</span>(cv.ridge)</span></code></pre></div>
<p><img src="06-modelos_lineales_files/figure-html/unnamed-chunk-30-1.png" width="80%" style="display: block; margin: auto;" /></p>
<p>En este caso el parámetro óptimo (según la regla de un error estándar) sería:</p>
<div class="sourceCode" id="cb314"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb314-1"><a href="métodos-de-regularización.html#cb314-1"></a>cv.ridge<span class="op">$</span>lambda<span class="fl">.1</span>se</span></code></pre></div>
<pre><code>## [1] 3.413705</code></pre>
<div class="sourceCode" id="cb316"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb316-1"><a href="métodos-de-regularización.html#cb316-1"></a><span class="co"># cv.ridge$lambda.min</span></span></code></pre></div>
<p>y el correspondiente modelo contiene todas las variables explicativas:</p>
<div class="sourceCode" id="cb317"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb317-1"><a href="métodos-de-regularización.html#cb317-1"></a><span class="kw">coef</span>(cv.ridge) <span class="co"># s = &quot;lambda.1se&quot;</span></span></code></pre></div>
<pre><code>## 14 x 1 sparse Matrix of class &quot;dgCMatrix&quot;
##                       1
## (Intercept)  8.38314273
## calidadp     2.06713538
## web          0.84771656
## soporte     -0.17674892
## quejas       1.08099022
## publi        0.25926570
## producto     1.34198207
## imgfvent     0.21510001
## precio       0.15194226
## garantia     0.05417865
## nprod        0.08252518
## facturac     0.45964418
## flexprec     0.24646749
## velocida     2.70697234</code></pre>
<div class="sourceCode" id="cb319"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb319-1"><a href="métodos-de-regularización.html#cb319-1"></a><span class="co"># coef(cv.ridge, s = &quot;lambda.min&quot;)</span></span></code></pre></div>
<p>Finalmente evaluamos la precisión en la muestra de test:</p>
<div class="sourceCode" id="cb320"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb320-1"><a href="métodos-de-regularización.html#cb320-1"></a>newx &lt;-<span class="st"> </span><span class="kw">as.matrix</span>(test[, <span class="dv">-14</span>])</span>
<span id="cb320-2"><a href="métodos-de-regularización.html#cb320-2"></a>pred &lt;-<span class="st"> </span><span class="kw">predict</span>(cv.ridge, <span class="dt">newx =</span> newx) <span class="co"># s = &quot;lambda.1se&quot;</span></span>
<span id="cb320-3"><a href="métodos-de-regularización.html#cb320-3"></a><span class="kw">accuracy</span>(pred, obs)</span></code></pre></div>
<pre><code>##         me       rmse        mae        mpe       mape  r.squared 
## 0.74752331 5.04159165 4.08299692 0.03577857 7.26473444 0.75087456</code></pre>
</div>
<div id="ejemplo-lasso" class="section level3">
<h3><span class="header-section-number">6.6.3</span> Ejemplo: Lasso</h3>
<p>También podríamos ajustar modelos lasso con la opción por defecto de <code>glmnet()</code> (<code>alpha = 1</code>, lasso penalty).
Pero en este caso lo haremos al mismo tiempo que seleccionamos el parámetro de penalización por validación cruzada:</p>
<div class="sourceCode" id="cb322"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb322-1"><a href="métodos-de-regularización.html#cb322-1"></a><span class="kw">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb322-2"><a href="métodos-de-regularización.html#cb322-2"></a>cv.lasso &lt;-<span class="st"> </span><span class="kw">cv.glmnet</span>(x,y)</span>
<span id="cb322-3"><a href="métodos-de-regularización.html#cb322-3"></a><span class="kw">plot</span>(cv.lasso)</span></code></pre></div>
<p><img src="06-modelos_lineales_files/figure-html/unnamed-chunk-34-1.png" width="80%" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb323"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb323-1"><a href="métodos-de-regularización.html#cb323-1"></a><span class="kw">plot</span>(cv.lasso<span class="op">$</span>glmnet.fit, <span class="dt">xvar =</span> <span class="st">&quot;lambda&quot;</span>, <span class="dt">label =</span> <span class="ot">TRUE</span>)    </span>
<span id="cb323-2"><a href="métodos-de-regularización.html#cb323-2"></a><span class="kw">abline</span>(<span class="dt">v =</span> <span class="kw">log</span>(cv.lasso<span class="op">$</span>lambda<span class="fl">.1</span>se), <span class="dt">lty =</span> <span class="dv">2</span>)</span>
<span id="cb323-3"><a href="métodos-de-regularización.html#cb323-3"></a><span class="kw">abline</span>(<span class="dt">v =</span> <span class="kw">log</span>(cv.lasso<span class="op">$</span>lambda.min), <span class="dt">lty =</span> <span class="dv">3</span>)</span></code></pre></div>
<p><img src="06-modelos_lineales_files/figure-html/unnamed-chunk-34-2.png" width="80%" style="display: block; margin: auto;" /></p>
<p>El modelo resultante (oneSE rule) solo contiene 4 variables explicativas:</p>
<div class="sourceCode" id="cb324"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb324-1"><a href="métodos-de-regularización.html#cb324-1"></a><span class="kw">coef</span>(cv.lasso) <span class="co"># s = &quot;lambda.1se&quot;</span></span></code></pre></div>
<pre><code>## 14 x 1 sparse Matrix of class &quot;dgCMatrix&quot;
##                      1
## (Intercept) 12.0485398
## calidadp     2.4673862
## web          0.3498592
## soporte      .        
## quejas       .        
## publi        .        
## producto     0.3227830
## imgfvent     .        
## precio       .        
## garantia     .        
## nprod        .        
## facturac     .        
## flexprec     .        
## velocida     6.1011015</code></pre>
<p>Por tanto este método también podría ser empleando para la selección de variables (puede hacerse automáticamente estableciendo <code>relax = TRUE</code>, ajustará los modelos sin regularización).</p>
<p>Finalmente evaluamos también la precisión en la muestra de test:</p>
<div class="sourceCode" id="cb326"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb326-1"><a href="métodos-de-regularización.html#cb326-1"></a>pred &lt;-<span class="st"> </span><span class="kw">predict</span>(cv.lasso, <span class="dt">newx =</span> newx)</span>
<span id="cb326-2"><a href="métodos-de-regularización.html#cb326-2"></a><span class="kw">accuracy</span>(pred, obs)</span></code></pre></div>
<pre><code>##         me       rmse        mae        mpe       mape  r.squared 
##  0.4895391  4.8572144  3.8870383 -0.4192005  6.9713208  0.7687630</code></pre>
</div>
<div id="ejemplo-elastic-net" class="section level3">
<h3><span class="header-section-number">6.6.4</span> Ejemplo: Elastic Net</h3>
<p>Podemos ajustar modelos elastic net para un valor concreto de <code>alpha</code> empleando la función <code>glmnet()</code>, pero las opciones del paquete no incluyen la selección de este hiperparámetro.
Aunque se podría implementar fácilmente (como se muestra en <code>help(cv.glmnet)</code>), resulta mucho más cómodo emplear el método <code>"glmnet"</code> de <code>caret</code>:</p>
<div class="sourceCode" id="cb328"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb328-1"><a href="métodos-de-regularización.html#cb328-1"></a><span class="kw">library</span>(caret)</span>
<span id="cb328-2"><a href="métodos-de-regularización.html#cb328-2"></a><span class="kw">modelLookup</span>(<span class="st">&quot;glmnet&quot;</span>) </span></code></pre></div>
<pre><code>##    model parameter                    label forReg forClass probModel
## 1 glmnet     alpha        Mixing Percentage   TRUE     TRUE      TRUE
## 2 glmnet    lambda Regularization Parameter   TRUE     TRUE      TRUE</code></pre>
<div class="sourceCode" id="cb330"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb330-1"><a href="métodos-de-regularización.html#cb330-1"></a><span class="kw">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb330-2"><a href="métodos-de-regularización.html#cb330-2"></a><span class="co"># Se podría emplear train(fidelida ~ ., data = train, ...)</span></span>
<span id="cb330-3"><a href="métodos-de-regularización.html#cb330-3"></a>caret.glmnet &lt;-<span class="st"> </span><span class="kw">train</span>(x, y, <span class="dt">method =</span> <span class="st">&quot;glmnet&quot;</span>,</span>
<span id="cb330-4"><a href="métodos-de-regularización.html#cb330-4"></a>    <span class="dt">preProc =</span> <span class="kw">c</span>(<span class="st">&quot;zv&quot;</span>, <span class="st">&quot;center&quot;</span>, <span class="st">&quot;scale&quot;</span>),</span>
<span id="cb330-5"><a href="métodos-de-regularización.html#cb330-5"></a>    <span class="dt">trControl =</span> <span class="kw">trainControl</span>(<span class="dt">method =</span> <span class="st">&quot;cv&quot;</span>, <span class="dt">number =</span> <span class="dv">5</span>),</span>
<span id="cb330-6"><a href="métodos-de-regularización.html#cb330-6"></a>    <span class="dt">tuneLength =</span> <span class="dv">5</span>)</span>
<span id="cb330-7"><a href="métodos-de-regularización.html#cb330-7"></a></span>
<span id="cb330-8"><a href="métodos-de-regularización.html#cb330-8"></a></span>
<span id="cb330-9"><a href="métodos-de-regularización.html#cb330-9"></a>caret.glmnet</span></code></pre></div>
<pre><code>## glmnet 
## 
## 160 samples
##  13 predictor
## 
## Pre-processing: centered (13), scaled (13) 
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 129, 129, 127, 127, 128 
## Resampling results across tuning parameters:
## 
##   alpha  lambda       RMSE      Rsquared   MAE     
##   0.100  0.005410604  4.581364  0.7148069  3.414825
##   0.100  0.025113801  4.576940  0.7153275  3.408862
##   0.100  0.116567945  4.545239  0.7187940  3.361951
##   0.100  0.541060495  4.474562  0.7284099  3.295198
##   0.100  2.511380465  4.704071  0.7187452  3.594686
##   0.325  0.005410604  4.573738  0.7157479  3.408931
##   0.325  0.025113801  4.564560  0.7167890  3.397543
##   0.325  0.116567945  4.500834  0.7241961  3.326005
##   0.325  0.541060495  4.438653  0.7349191  3.306102
##   0.325  2.511380465  4.881621  0.7184709  3.757854
##   0.550  0.005410604  4.573800  0.7157344  3.411370
##   0.550  0.025113801  4.552473  0.7182118  3.386635
##   0.550  0.116567945  4.462650  0.7291272  3.299872
##   0.550  0.541060495  4.459588  0.7344030  3.358370
##   0.550  2.511380465  5.140746  0.7128471  3.964142
##   0.775  0.005410604  4.570751  0.7161237  3.409145
##   0.775  0.025113801  4.542225  0.7194584  3.378410
##   0.775  0.116567945  4.430677  0.7334438  3.277212
##   0.775  0.541060495  4.495356  0.7323161  3.413533
##   0.775  2.511380465  5.410928  0.7138082  4.213179
##   1.000  0.005410604  4.569043  0.7162973  3.407715
##   1.000  0.025113801  4.532524  0.7206448  3.371146
##   1.000  0.116567945  4.420602  0.7349329  3.279275
##   1.000  0.541060495  4.525359  0.7308248  3.449277
##   1.000  2.511380465  5.730967  0.7102963  4.473639
## 
## RMSE was used to select the optimal model using the smallest value.
## The final values used for the model were alpha = 1 and lambda = 0.1165679.</code></pre>
<div class="sourceCode" id="cb332"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb332-1"><a href="métodos-de-regularización.html#cb332-1"></a><span class="kw">ggplot</span>(caret.glmnet)</span></code></pre></div>
<p><img src="06-modelos_lineales_files/figure-html/unnamed-chunk-37-1.png" width="80%" style="display: block; margin: auto;" /></p>
<div class="sourceCode" id="cb333"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb333-1"><a href="métodos-de-regularización.html#cb333-1"></a>pred &lt;-<span class="st"> </span><span class="kw">predict</span>(caret.glmnet, <span class="dt">newdata =</span> test)</span>
<span id="cb333-2"><a href="métodos-de-regularización.html#cb333-2"></a><span class="kw">accuracy</span>(pred, obs)</span></code></pre></div>
<pre><code>##          me        rmse         mae         mpe        mape   r.squared 
##  0.49843131  4.28230542  3.43805154 -0.02851825  6.15711129  0.82026278</code></pre>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="evaluación-de-la-precisión.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="métodos-de-reducción-de-la-dimensión.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rubenfcasal/aprendizaje_estadistico/edit/master/06-modelos_lineales.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["aprendizaje_estadistico.pdf"],
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
