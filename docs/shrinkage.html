<!DOCTYPE html>
<html lang="es" xml:lang="es">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>6.1 Métodos de regularización | Métodos predictivos de aprendizaje estadístico</title>
  <meta name="description" content="6.1 Métodos de regularización | Métodos predictivos de aprendizaje estadístico con R." />
  <meta name="generator" content="bookdown 0.36 and GitBook 2.6.7" />

  <meta property="og:title" content="6.1 Métodos de regularización | Métodos predictivos de aprendizaje estadístico" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="6.1 Métodos de regularización | Métodos predictivos de aprendizaje estadístico con R." />
  <meta name="github-repo" content="rubenfcasal/aprendizaje_estadistico" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="6.1 Métodos de regularización | Métodos predictivos de aprendizaje estadístico" />
  
  <meta name="twitter:description" content="6.1 Métodos de regularización | Métodos predictivos de aprendizaje estadístico con R." />
  

<meta name="author" content="Rubén Fernández Casal (ruben.fcasal@udc.es)" />
<meta name="author" content="Julián Costa Bouzas (julian.costa@udc.es)" />
<meta name="author" content="Manuel Oviedo de la Fuente (manuel.oviedo@udc.es)" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="ext-glm.html"/>
<link rel="next" href="pca-pls.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/htmlwidgets-1.6.2/htmlwidgets.js"></script>
<link href="libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet" />
<script src="libs/datatables-binding-0.30/datatables.js"></script>
<link href="libs/dt-core-1.13.4/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.13.4/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.13.4/js/jquery.dataTables.min.js"></script>
<link href="libs/crosstalk-1.2.0/css/crosstalk.min.css" rel="stylesheet" />
<script src="libs/crosstalk-1.2.0/js/crosstalk.min.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Métodos predictivos de aprendizaje estadístico</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Bienvenida</a></li>
<li class="chapter" data-level="" data-path="prólogo.html"><a href="prólogo.html"><i class="fa fa-check"></i>Prólogo</a>
<ul>
<li class="chapter" data-level="" data-path="el-lenguaje-de-programación-r.html"><a href="el-lenguaje-de-programación-r.html"><i class="fa fa-check"></i>El lenguaje de programación R</a></li>
<li class="chapter" data-level="" data-path="organización.html"><a href="organización.html"><i class="fa fa-check"></i>Organización</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="intro-AE.html"><a href="intro-AE.html"><i class="fa fa-check"></i><b>1</b> Introducción al aprendizaje estadístico</a>
<ul>
<li class="chapter" data-level="1.1" data-path="aprendizaje-estadístico-vs.-aprendizaje-automático.html"><a href="aprendizaje-estadístico-vs.-aprendizaje-automático.html"><i class="fa fa-check"></i><b>1.1</b> Aprendizaje estadístico vs. aprendizaje automático</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="aprendizaje-estadístico-vs.-aprendizaje-automático.html"><a href="aprendizaje-estadístico-vs.-aprendizaje-automático.html#las-dos-culturas"><i class="fa fa-check"></i><b>1.1.1</b> Las dos culturas</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="métodos-de-aprendizaje-estadístico.html"><a href="métodos-de-aprendizaje-estadístico.html"><i class="fa fa-check"></i><b>1.2</b> Métodos de aprendizaje estadístico</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="métodos-de-aprendizaje-estadístico.html"><a href="métodos-de-aprendizaje-estadístico.html#notacion"><i class="fa fa-check"></i><b>1.2.1</b> Notación y terminología</a></li>
<li class="chapter" data-level="1.2.2" data-path="métodos-de-aprendizaje-estadístico.html"><a href="métodos-de-aprendizaje-estadístico.html#metodos-pkgs"><i class="fa fa-check"></i><b>1.2.2</b> Métodos (de aprendizaje supervisado) y paquetes de R</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="const-eval.html"><a href="const-eval.html"><i class="fa fa-check"></i><b>1.3</b> Construcción y evaluación de los modelos</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="const-eval.html"><a href="const-eval.html#bias-variance"><i class="fa fa-check"></i><b>1.3.1</b> Equilibrio entre sesgo y varianza: infraajuste y sobreajuste</a></li>
<li class="chapter" data-level="1.3.2" data-path="const-eval.html"><a href="const-eval.html#entrenamiento-test"><i class="fa fa-check"></i><b>1.3.2</b> Datos de entrenamiento y datos de test</a></li>
<li class="chapter" data-level="1.3.3" data-path="const-eval.html"><a href="const-eval.html#cv"><i class="fa fa-check"></i><b>1.3.3</b> Selección de hiperparámetros mediante validación cruzada</a></li>
<li class="chapter" data-level="1.3.4" data-path="const-eval.html"><a href="const-eval.html#eval-reg"><i class="fa fa-check"></i><b>1.3.4</b> Evaluación de un método de regresión</a></li>
<li class="chapter" data-level="1.3.5" data-path="const-eval.html"><a href="const-eval.html#eval-class"><i class="fa fa-check"></i><b>1.3.5</b> Evaluación de un método de clasificación</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="dimen-curse.html"><a href="dimen-curse.html"><i class="fa fa-check"></i><b>1.4</b> La maldición de la dimensionalidad</a></li>
<li class="chapter" data-level="1.5" data-path="analisis-modelos.html"><a href="analisis-modelos.html"><i class="fa fa-check"></i><b>1.5</b> Análisis e interpretación de los modelos</a></li>
<li class="chapter" data-level="1.6" data-path="caret.html"><a href="caret.html"><i class="fa fa-check"></i><b>1.6</b> Introducción al paquete <code>caret</code></a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="clasicos.html"><a href="clasicos.html"><i class="fa fa-check"></i><b>2</b> Métodos clásicos de estadística</a>
<ul>
<li class="chapter" data-level="2.1" data-path="rlm.html"><a href="rlm.html"><i class="fa fa-check"></i><b>2.1</b> Regresión lineal múltiple</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="rlm.html"><a href="rlm.html#colinealidad"><i class="fa fa-check"></i><b>2.1.1</b> El problema de la colinealidad</a></li>
<li class="chapter" data-level="2.1.2" data-path="rlm.html"><a href="rlm.html#seleccion-rlm"><i class="fa fa-check"></i><b>2.1.2</b> Selección de variables explicativas</a></li>
<li class="chapter" data-level="2.1.3" data-path="rlm.html"><a href="rlm.html#analisis-rlm"><i class="fa fa-check"></i><b>2.1.3</b> Análisis e interpretación del modelo</a></li>
<li class="chapter" data-level="2.1.4" data-path="rlm.html"><a href="rlm.html#eval-rlm"><i class="fa fa-check"></i><b>2.1.4</b> Evaluación de la precisión</a></li>
<li class="chapter" data-level="2.1.5" data-path="rlm.html"><a href="rlm.html#selec-ae-rlm"><i class="fa fa-check"></i><b>2.1.5</b> Selección del modelo mediante remuestreo</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="reg-glm.html"><a href="reg-glm.html"><i class="fa fa-check"></i><b>2.2</b> Modelos lineales generalizados</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="reg-glm.html"><a href="reg-glm.html#seleccion-glm"><i class="fa fa-check"></i><b>2.2.1</b> Selección de variables explicativas</a></li>
<li class="chapter" data-level="2.2.2" data-path="reg-glm.html"><a href="reg-glm.html#analisis-glm"><i class="fa fa-check"></i><b>2.2.2</b> Análisis e interpretación del modelo</a></li>
<li class="chapter" data-level="2.2.3" data-path="reg-glm.html"><a href="reg-glm.html#glm-bfan"><i class="fa fa-check"></i><b>2.2.3</b> Evaluación de la precisión</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="generadores.html"><a href="generadores.html"><i class="fa fa-check"></i><b>2.3</b> Otros métodos de clasificación</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="generadores.html"><a href="generadores.html#clas-lda"><i class="fa fa-check"></i><b>2.3.1</b> Análisis discriminante lineal</a></li>
<li class="chapter" data-level="2.3.2" data-path="generadores.html"><a href="generadores.html#clas-qda"><i class="fa fa-check"></i><b>2.3.2</b> Análisis discriminante cuadrático</a></li>
<li class="chapter" data-level="2.3.3" data-path="generadores.html"><a href="generadores.html#bayes"><i class="fa fa-check"></i><b>2.3.3</b> Bayes naíf</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="trees.html"><a href="trees.html"><i class="fa fa-check"></i><b>3</b> Árboles de decisión</a>
<ul>
<li class="chapter" data-level="3.1" data-path="árboles-de-regresión-cart.html"><a href="árboles-de-regresión-cart.html"><i class="fa fa-check"></i><b>3.1</b> Árboles de regresión CART</a></li>
<li class="chapter" data-level="3.2" data-path="árboles-de-clasificación-cart.html"><a href="árboles-de-clasificación-cart.html"><i class="fa fa-check"></i><b>3.2</b> Árboles de clasificación CART</a></li>
<li class="chapter" data-level="3.3" data-path="tree-rpart.html"><a href="tree-rpart.html"><i class="fa fa-check"></i><b>3.3</b> CART con el paquete <code>rpart</code></a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="tree-rpart.html"><a href="tree-rpart.html#reg-rpart"><i class="fa fa-check"></i><b>3.3.1</b> Ejemplo: regresión</a></li>
<li class="chapter" data-level="3.3.2" data-path="tree-rpart.html"><a href="tree-rpart.html#class-rpart"><i class="fa fa-check"></i><b>3.3.2</b> Ejemplo: modelo de clasificación</a></li>
<li class="chapter" data-level="3.3.3" data-path="tree-rpart.html"><a href="tree-rpart.html#interfaz-de-caret"><i class="fa fa-check"></i><b>3.3.3</b> Interfaz de <code>caret</code></a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="alternativas-a-los-árboles-cart.html"><a href="alternativas-a-los-árboles-cart.html"><i class="fa fa-check"></i><b>3.4</b> Alternativas a los árboles CART</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="bagging-boosting.html"><a href="bagging-boosting.html"><i class="fa fa-check"></i><b>4</b> Bagging y boosting</a>
<ul>
<li class="chapter" data-level="4.1" data-path="bagging.html"><a href="bagging.html"><i class="fa fa-check"></i><b>4.1</b> Bagging</a></li>
<li class="chapter" data-level="4.2" data-path="rf.html"><a href="rf.html"><i class="fa fa-check"></i><b>4.2</b> Bosques aleatorios</a></li>
<li class="chapter" data-level="4.3" data-path="bagging-rf-r.html"><a href="bagging-rf-r.html"><i class="fa fa-check"></i><b>4.3</b> Bagging y bosques aleatorios en R</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="bagging-rf-r.html"><a href="bagging-rf-r.html#ejemplo-clasificación-con-bagging"><i class="fa fa-check"></i><b>4.3.1</b> Ejemplo: clasificación con bagging</a></li>
<li class="chapter" data-level="4.3.2" data-path="bagging-rf-r.html"><a href="bagging-rf-r.html#ejemplo-clasif-rf"><i class="fa fa-check"></i><b>4.3.2</b> Ejemplo: clasificación con bosques aleatorios</a></li>
<li class="chapter" data-level="4.3.3" data-path="bagging-rf-r.html"><a href="bagging-rf-r.html#ejemplo-bosques-aleatorios-con-caret"><i class="fa fa-check"></i><b>4.3.3</b> Ejemplo: bosques aleatorios con <code>caret</code></a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="boosting.html"><a href="boosting.html"><i class="fa fa-check"></i><b>4.4</b> Boosting</a></li>
<li class="chapter" data-level="4.5" data-path="boosting-r.html"><a href="boosting-r.html"><i class="fa fa-check"></i><b>4.5</b> Boosting en R</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="boosting-r.html"><a href="boosting-r.html#ejemplo-clasificación-con-el-paquete-ada"><i class="fa fa-check"></i><b>4.5.1</b> Ejemplo: clasificación con el paquete <code>ada</code></a></li>
<li class="chapter" data-level="4.5.2" data-path="boosting-r.html"><a href="boosting-r.html#ejemplo-regresión-con-el-paquete-gbm"><i class="fa fa-check"></i><b>4.5.2</b> Ejemplo: regresión con el paquete <code>gbm</code></a></li>
<li class="chapter" data-level="4.5.3" data-path="boosting-r.html"><a href="boosting-r.html#xgb-caret"><i class="fa fa-check"></i><b>4.5.3</b> Ejemplo: XGBoost con el paquete <code>caret</code></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="svm.html"><a href="svm.html"><i class="fa fa-check"></i><b>5</b> Máquinas de soporte vectorial</a>
<ul>
<li class="chapter" data-level="5.1" data-path="clasificadores-de-máximo-margen.html"><a href="clasificadores-de-máximo-margen.html"><i class="fa fa-check"></i><b>5.1</b> Clasificadores de máximo margen</a></li>
<li class="chapter" data-level="5.2" data-path="clasificadores-de-soporte-vectorial.html"><a href="clasificadores-de-soporte-vectorial.html"><i class="fa fa-check"></i><b>5.2</b> Clasificadores de soporte vectorial</a></li>
<li class="chapter" data-level="5.3" data-path="máquinas-de-soporte-vectorial.html"><a href="máquinas-de-soporte-vectorial.html"><i class="fa fa-check"></i><b>5.3</b> Máquinas de soporte vectorial</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="máquinas-de-soporte-vectorial.html"><a href="máquinas-de-soporte-vectorial.html#regresión-con-svm"><i class="fa fa-check"></i><b>5.3.1</b> Regresión con SVM</a></li>
<li class="chapter" data-level="5.3.2" data-path="máquinas-de-soporte-vectorial.html"><a href="máquinas-de-soporte-vectorial.html#ventajas-e-incovenientes"><i class="fa fa-check"></i><b>5.3.2</b> Ventajas e incovenientes</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="svm-kernlab.html"><a href="svm-kernlab.html"><i class="fa fa-check"></i><b>5.4</b> SVM en R</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="ext-glm.html"><a href="ext-glm.html"><i class="fa fa-check"></i><b>6</b> Extensiones de los modelos lineales (generalizados)</a>
<ul>
<li class="chapter" data-level="6.1" data-path="shrinkage.html"><a href="shrinkage.html"><i class="fa fa-check"></i><b>6.1</b> Métodos de regularización</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="shrinkage.html"><a href="shrinkage.html#implementación-en-r"><i class="fa fa-check"></i><b>6.1.1</b> Implementación en R</a></li>
<li class="chapter" data-level="6.1.2" data-path="shrinkage.html"><a href="shrinkage.html#ejemplo-ridge-regression"><i class="fa fa-check"></i><b>6.1.2</b> Ejemplo: <em>ridge regression</em></a></li>
<li class="chapter" data-level="6.1.3" data-path="shrinkage.html"><a href="shrinkage.html#ejemplo-lasso"><i class="fa fa-check"></i><b>6.1.3</b> Ejemplo: LASSO</a></li>
<li class="chapter" data-level="6.1.4" data-path="shrinkage.html"><a href="shrinkage.html#ejemplo-elastic-net"><i class="fa fa-check"></i><b>6.1.4</b> Ejemplo: <em>elastic net</em></a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="pca-pls.html"><a href="pca-pls.html"><i class="fa fa-check"></i><b>6.2</b> Métodos de reducción de la dimensión</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="pca-pls.html"><a href="pca-pls.html#regresión-por-componentes-principales-pcr"><i class="fa fa-check"></i><b>6.2.1</b> Regresión por componentes principales (PCR)</a></li>
<li class="chapter" data-level="6.2.2" data-path="pca-pls.html"><a href="pca-pls.html#regresión-por-mínimos-cuadrados-parciales-plsr"><i class="fa fa-check"></i><b>6.2.2</b> Regresión por mínimos cuadrados parciales (PLSR)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="reg-np.html"><a href="reg-np.html"><i class="fa fa-check"></i><b>7</b> Regresión no paramétrica</a>
<ul>
<li class="chapter" data-level="7.1" data-path="reg-local.html"><a href="reg-local.html"><i class="fa fa-check"></i><b>7.1</b> Regresión local</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="reg-local.html"><a href="reg-local.html#reg-knn"><i class="fa fa-check"></i><b>7.1.1</b> Vecinos más próximos</a></li>
<li class="chapter" data-level="7.1.2" data-path="reg-local.html"><a href="reg-local.html#reg-locpol"><i class="fa fa-check"></i><b>7.1.2</b> Regresión polinómica local</a></li>
<li class="chapter" data-level="7.1.3" data-path="reg-local.html"><a href="reg-local.html#regresión-polinómica-local-robusta"><i class="fa fa-check"></i><b>7.1.3</b> Regresión polinómica local robusta</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="splines.html"><a href="splines.html"><i class="fa fa-check"></i><b>7.2</b> Splines</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="splines.html"><a href="splines.html#reg-splines"><i class="fa fa-check"></i><b>7.2.1</b> Splines de regresión</a></li>
<li class="chapter" data-level="7.2.2" data-path="splines.html"><a href="splines.html#splines-de-suavizado"><i class="fa fa-check"></i><b>7.2.2</b> Splines de suavizado</a></li>
<li class="chapter" data-level="7.2.3" data-path="splines.html"><a href="splines.html#splines-penalizados"><i class="fa fa-check"></i><b>7.2.3</b> Splines penalizados</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="reg-gam.html"><a href="reg-gam.html"><i class="fa fa-check"></i><b>7.3</b> Modelos aditivos</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="reg-gam.html"><a href="reg-gam.html#superficies-de-predicción"><i class="fa fa-check"></i><b>7.3.1</b> Superficies de predicción</a></li>
<li class="chapter" data-level="7.3.2" data-path="reg-gam.html"><a href="reg-gam.html#anova-gam"><i class="fa fa-check"></i><b>7.3.2</b> Comparación y selección de modelos</a></li>
<li class="chapter" data-level="7.3.3" data-path="reg-gam.html"><a href="reg-gam.html#mgcv-diagnosis"><i class="fa fa-check"></i><b>7.3.3</b> Diagnosis del modelo</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="mars.html"><a href="mars.html"><i class="fa fa-check"></i><b>7.4</b> Regresión spline adaptativa multivariante</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="mars.html"><a href="mars.html#mars-con-el-paquete-earth"><i class="fa fa-check"></i><b>7.4.1</b> MARS con el paquete <code>earth</code></a></li>
<li class="chapter" data-level="7.4.2" data-path="mars.html"><a href="mars.html#mars-con-el-paquete-caret"><i class="fa fa-check"></i><b>7.4.2</b> MARS con el paquete <code>caret</code></a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="pursuit.html"><a href="pursuit.html"><i class="fa fa-check"></i><b>7.5</b> Projection pursuit</a>
<ul>
<li class="chapter" data-level="7.5.1" data-path="pursuit.html"><a href="pursuit.html#ppr"><i class="fa fa-check"></i><b>7.5.1</b> Regresión por projection pursuit</a></li>
<li class="chapter" data-level="7.5.2" data-path="pursuit.html"><a href="pursuit.html#implementación-en-r-1"><i class="fa fa-check"></i><b>7.5.2</b> Implementación en R</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="neural-nets.html"><a href="neural-nets.html"><i class="fa fa-check"></i><b>8</b> Redes neuronales</a>
<ul>
<li class="chapter" data-level="8.1" data-path="single-hidden-layer-feedforward-network.html"><a href="single-hidden-layer-feedforward-network.html"><i class="fa fa-check"></i><b>8.1</b> Single-hidden-layer feedforward network</a></li>
<li class="chapter" data-level="8.2" data-path="clasificación-con-ann.html"><a href="clasificación-con-ann.html"><i class="fa fa-check"></i><b>8.2</b> Clasificación con ANN</a></li>
<li class="chapter" data-level="8.3" data-path="implementación-en-r-2.html"><a href="implementación-en-r-2.html"><i class="fa fa-check"></i><b>8.3</b> Implementación en R</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliografía.html"><a href="bibliografía.html"><i class="fa fa-check"></i>Bibliografía</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Métodos predictivos de aprendizaje estadístico</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="shrinkage" class="section level2 hasAnchor" number="6.1">
<h2><span class="header-section-number">6.1</span> Métodos de regularización<a href="shrinkage.html#shrinkage" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p>Como ya se comentó en el Capítulo <a href="clasicos.html#clasicos">2</a>, el procedimiento habitual para ajustar un modelo de regresión lineal es emplear mínimos cuadrados, es decir, utilizar como criterio de error la suma de cuadrados residual
<span class="math display">\[\mbox{RSS} = \sum\limits_{i=1}^{n}\left(  y_{i} - \beta_0 - \boldsymbol{\beta}^t \mathbf{x}_{i} \right)^{2}\]</span></p>
<p>Si el modelo lineal es razonablemente adecuado, utilizar <span class="math inline">\(\mbox{RSS}\)</span> va a dar lugar a estimaciones con poco sesgo, y si además <span class="math inline">\(n\gg p\)</span>, entonces el modelo también va a tener poca varianza (bajo las hipótesis estructurales, la estimación es insesgada y además de varianza mínima entre todas las técnicas insesgadas).
Las dificultades surgen cuando <span class="math inline">\(p\)</span> es grande o cuando hay correlaciones altas entre las variables predictoras: tener muchas variables dificulta la interpretación del modelo, y si además hay problemas de colinealidad o se incumple <span class="math inline">\(n\gg p\)</span>, entonces la estimación del modelo va a tener mucha varianza y el modelo estará sobreajustado.
La solución pasa por forzar a que el modelo tenga menos complejidad para así reducir su varianza.
Una forma de conseguirlo es mediante la regularización (<em>regularization</em> o <em>shrinkage</em>) de la estimación de los parámetros <span class="math inline">\(\beta_1, \beta_2,\ldots, \beta_p\)</span>, que consiste en considerar todas las variables predictoras, pero forzando a que algunos de los parámetros se estimen mediante valores muy próximos a cero, o directamente con ceros.
Esta técnica va a provocar un pequeño aumento en el sesgo, pero a cambio una notable reducción en la varianza y una interpretación más sencilla del modelo resultante.</p>
<p>Hay dos formas básicas de lograr esta simplificación de los parámetros (con la consiguiente simplificación del modelo), utilizando una penalización cuadrática (norma <span class="math inline">\(L_2\)</span>) o en valor absoluto (norma <span class="math inline">\(L_1\)</span>):</p>
<ul>
<li><p><em>Ridge regression</em> <span class="citation">(<a href="#ref-hoerl1970ridge" role="doc-biblioref">Hoerl y Kennard, 1970</a>)</span>
<span class="math display">\[\mbox{min}_{\beta_0, \boldsymbol{\beta}} \mbox{RSS} + \lambda\sum_{j=1}^{p}\beta_{j}^{2}\]</span></p>
<p>Equivalentemente,
<span class="math display">\[\mbox{min}_{\beta_0, \boldsymbol{\beta}} \mbox{RSS}\]</span>
sujeto a
<span class="math display">\[\sum_{j=1}^{p}\beta_{j}^{2} \le s\]</span></p></li>
<li><p>LASSO <span class="citation">(<em>least absolute shrinkage and selection operator</em>, <a href="#ref-tibshirani1996regression" role="doc-biblioref">Tibshirani, 1996</a>)</span>
<span class="math display">\[\mbox{min}_{\beta_0, \boldsymbol{\beta}} RSS + \lambda\sum_{j=1}^{p}|\beta_{j}|\]</span></p>
<p>Equivalentemente,
<span class="math display">\[\mbox{min}_{\beta_0, \boldsymbol{\beta}} \mbox{RSS}\]</span>
sujeto a
<span class="math display">\[\sum_{j=1}^{p}|\beta_{j}| \le s\]</span></p></li>
</ul>
<p>Una formulación unificada consiste en considerar el problema
<span class="math display">\[\mbox{min}_{\beta_0, \boldsymbol{\beta}} RSS + \lambda\sum_{j=1}^{p}|\beta_{j}|^d\]</span></p>
<p>Si <span class="math inline">\(d=0\)</span>, la penalización consiste en el número de variables utilizadas, por tanto se corresponde con el problema de selección de variables; <span class="math inline">\(d=1\)</span> se corresponde con LASSO y <span class="math inline">\(d=2\)</span> con <em>ridge</em>.</p>
<p>La ventaja de utilizar LASSO es que tiende a forzar a que algunos parámetros sean cero, con lo cual también se realiza una selección de las variables más influyentes.
Por el contrario, <em>ridge regression</em> tiende a incluir todas las variables predictoras en el modelo final, si bien es cierto que algunas con parámetros muy próximos a cero: de este modo va a reducir el riesgo del sobreajuste, pero no resuelve el problema de la interpretabilidad.
Otra posible ventaja de utilizar LASSO es que cuando hay variables predictoras correlacionadas tiene tendencia a seleccionar una y anular las demás (esto también se puede ver como un inconveniente, ya que pequeños cambios en los datos pueden dar lugar a modelos distintos), mientras que <em>ridge</em> tiende a darles igual peso.</p>
<p>Dos generalizaciones de LASSO son <em>least angle regression</em> <span class="citation">(LARS, <a href="#ref-efron2004least" role="doc-biblioref">Efron et al., 2004</a>)</span> y <em>elastic net</em> <span class="citation">(<a href="#ref-zou2005regularization" role="doc-biblioref">Zou y Hastie, 2005</a>)</span>.
<em>Elastic net</em> combina las ventajas de <em>ridge</em> y LASSO, minimizando
<span class="math display">\[\mbox{min}_{\beta_0, \boldsymbol{\beta}} \ \mbox{RSS} + \lambda \left( \frac{1 - \alpha}{2}\sum_{j=1}^{p}\beta_{j}^{2} + \alpha \sum_{j=1}^{p}|\beta_{j}| \right)\]</span>
siendo <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(0 \leq \alpha \leq 1\)</span>, un hiperparámetro adicional que determina la combinación lineal de ambas penalizaciones.</p>
<!-- LARS parte de coeficientes nulos y, simplificando, los va aumentando en la dirección de mínimos cuadrados (o minimizando otro criterio de error) de forma incremental, añadiendo secuencialmente el coeficiente de la variable que está más correlacionada con los residuos -->
<p>Es muy importante estandarizar (centrar y reescalar) las variables predictoras antes de realizar estas técnicas.
Fijémonos en que, así como <span class="math inline">\(\mbox{RSS}\)</span> es insensible a los cambios de escala, la penalización es muy sensible.
Previa estandarización, el término independiente <span class="math inline">\(\beta_0\)</span> (que no interviene en la penalización) tiene una interpretación muy directa, ya que
<span class="math display">\[\widehat \beta_0 = \bar y =\sum_{i=1}^n \frac{y_i}{n}\]</span></p>
<p>Los dos métodos de regularización comentados dependen del hiperparámetro <span class="math inline">\(\lambda\)</span> (equivalentemente, <span class="math inline">\(s\)</span>).
Es importante seleccionar adecuadamente el valor del hiperparámetro, por ejemplo utilizando validación cruzada.
Hay algoritmos muy eficientes que permiten el ajuste, tanto de <em>ridge regression</em> como de LASSO, de forma conjunta (simultánea) para todos los valores de <span class="math inline">\(\lambda\)</span>.</p>
<div id="implementación-en-r" class="section level3 hasAnchor" number="6.1.1">
<h3><span class="header-section-number">6.1.1</span> Implementación en R<a href="shrinkage.html#implementación-en-r" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Hay varios paquetes que implementan estos métodos: <code>h2o</code>, <code>elasticnet</code>, <code>penalized</code>, <code>lasso2</code>, <code>biglasso</code>, etc., pero el paquete <a href="https://glmnet.stanford.edu"><code>glmnet</code></a> <span class="citation">(<a href="#ref-R-glmnet" role="doc-biblioref">Friedman et al., 2023</a>)</span> utiliza una de las más eficientes.
Sin embargo, este paquete no emplea formulación de modelos, hay que establecer la respuesta <code>y</code> y la matriz numérica <code>x</code> correspondiente a las variables explicativas.
Por tanto, no se pueden incluir directamente predictores categóricos, hay que codificarlos empleando variables auxiliares numéricas.
Se puede emplear la función <a href="https://rdrr.io/r/stats/model.matrix.html"><code>model.matrix()</code></a> (o <a href="https://rdrr.io/pkg/Matrix/man/sparse.model.matrix.html"><code>Matrix::sparse.model.matrix()</code></a> si el conjunto de datos es muy grande) para construir la matriz de diseño <code>x</code> a partir de una fórmula (alternativamente, se pueden emplear las herramientas implementadas en el paquete <code>caret</code>).
Además, tampoco admite datos faltantes.</p>
<p>La función principal es <a href="https://glmnet.stanford.edu/reference/glmnet.html"><code>glmnet()</code></a>:</p>
<div class="sourceCode" id="cb328"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb328-1"><a href="shrinkage.html#cb328-1" aria-hidden="true" tabindex="-1"></a><span class="fu">glmnet</span>(x, y, family, <span class="at">alpha =</span> <span class="dv">1</span>, <span class="at">lambda =</span> <span class="cn">NULL</span>, ...)</span></code></pre></div>
<ul>
<li><p><code>family</code>: familia del modelo lineal generalizado (ver Sección <a href="reg-glm.html#reg-glm">2.2</a>); por defecto <code>"gaussian"</code> (modelo lineal con ajuste cuadrático), también admite <code>"binomial"</code>, <code>"poisson"</code>, <code>"multinomial"</code>, <code>"cox"</code> o <code>"mgaussian"</code> (modelo lineal con respuesta multivariante).</p></li>
<li><p><code>alpha</code>: parámetro <span class="math inline">\(\alpha\)</span> de elasticnet <span class="math inline">\(0 \leq \alpha \leq 1\)</span>. Por defecto <code>alpha = 1</code> penalización LASSO (<code>alpha = 0</code> para <em>ridge regression</em>).</p></li>
<li><p><code>lambda</code>: secuencia (opcional) de valores de <span class="math inline">\(\lambda\)</span>; si no se especifica se establece una secuencia por defecto (en base a los argumentos adicionales <code>nlambda</code> y <code>lambda.min.ratio</code>). Se devolverán los ajustes para todos los valores de esta secuencia (también se podrán obtener posteriormente para otros valores).</p></li>
</ul>
<p>Entre los métodos disponibles para el objeto resultante, <code>coef()</code> y <code>predict()</code> permiten obtener los coeficientes y las predicciones para un valor concreto de <span class="math inline">\(\lambda\)</span>, que se debe especificar mediante el argumento<a href="#fn55" class="footnote-ref" id="fnref55"><sup>55</sup></a> <code>s = valor</code>.</p>
<p>Para seleccionar el valor “óptimo” del hiperparámetro <span class="math inline">\(\lambda\)</span> (mediante validación cruzada) se puede emplear <a href="https://glmnet.stanford.edu/reference/cv.glmnet.html"><code>cv.glmnet()</code></a>:</p>
<div class="sourceCode" id="cb329"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb329-1"><a href="shrinkage.html#cb329-1" aria-hidden="true" tabindex="-1"></a><span class="fu">cv.glmnet</span>(x, y, family, alpha, lambda, <span class="at">type.measure =</span> <span class="st">&quot;default&quot;</span>, </span>
<span id="cb329-2"><a href="shrinkage.html#cb329-2" aria-hidden="true" tabindex="-1"></a>          <span class="at">nfolds =</span> <span class="dv">10</span>, ...)</span></code></pre></div>
<p>Esta función también devuelve los ajustes con toda la muestra de entrenamiento (en la componente <code>$glmnet.fit</code>) y se puede emplear el resultado directamente para predecir o obtener los coeficientes del modelo.
Por defecto, selecciona <span class="math inline">\(\lambda\)</span> mediante la regla de “un error estándar” de <span class="citation">Breiman et al. (<a href="#ref-breiman1984classification" role="doc-biblioref">1984</a>)</span> (componente <code>$lambda.1se</code>), aunque también calcula el valor óptimo (componente <code>$lambda.min</code>; que se puede seleccionar estableciendo <code>s = "lambda.min"</code>).
Para más detalles, consultar la <em>vignette</em> del paquete <a href="https://glmnet.stanford.edu/articles/glmnet.html"><em>An Introduction to glmnet</em></a>.</p>
<p>Continuaremos con el ejemplo de los datos de grasa corporal empleado en la Sección <a href="rlm.html#rlm">2.1</a> (con predictores numéricos y sin datos faltantes):</p>
<div class="sourceCode" id="cb330"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb330-1"><a href="shrinkage.html#cb330-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(glmnet)</span>
<span id="cb330-2"><a href="shrinkage.html#cb330-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(mpae)</span>
<span id="cb330-3"><a href="shrinkage.html#cb330-3" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(bodyfat)</span>
<span id="cb330-4"><a href="shrinkage.html#cb330-4" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> bodyfat</span>
<span id="cb330-5"><a href="shrinkage.html#cb330-5" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb330-6"><a href="shrinkage.html#cb330-6" aria-hidden="true" tabindex="-1"></a>nobs <span class="ot">&lt;-</span> <span class="fu">nrow</span>(df)</span>
<span id="cb330-7"><a href="shrinkage.html#cb330-7" aria-hidden="true" tabindex="-1"></a>itrain <span class="ot">&lt;-</span> <span class="fu">sample</span>(nobs, <span class="fl">0.8</span> <span class="sc">*</span> nobs)</span>
<span id="cb330-8"><a href="shrinkage.html#cb330-8" aria-hidden="true" tabindex="-1"></a>train <span class="ot">&lt;-</span> df[itrain, ]</span>
<span id="cb330-9"><a href="shrinkage.html#cb330-9" aria-hidden="true" tabindex="-1"></a>test <span class="ot">&lt;-</span> df[<span class="sc">-</span>itrain, ]</span>
<span id="cb330-10"><a href="shrinkage.html#cb330-10" aria-hidden="true" tabindex="-1"></a>x <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(train[<span class="sc">-</span><span class="dv">1</span>])</span>
<span id="cb330-11"><a href="shrinkage.html#cb330-11" aria-hidden="true" tabindex="-1"></a>y <span class="ot">&lt;-</span> train<span class="sc">$</span>bodyfat</span></code></pre></div>
</div>
<div id="ejemplo-ridge-regression" class="section level3 hasAnchor" number="6.1.2">
<h3><span class="header-section-number">6.1.2</span> Ejemplo: <em>ridge regression</em><a href="shrinkage.html#ejemplo-ridge-regression" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Podemos ajustar modelos de regresión ridge (con la secuencia de valores de <span class="math inline">\(\lambda\)</span> por defecto) con la función <a href="https://glmnet.stanford.edu/reference/glmnet.html"><code>glmnet()</code></a> con <code>alpha=0</code> (<em>ridge penalty</em>).
Con el método <a href="https://glmnet.stanford.edu/reference/plot.glmnet.html"><code>plot()</code></a>, podemos representar la evolución de los coeficientes en función de la penalización (etiquetando las curvas con el índice de la variable si <code>label = TRUE</code>; ver Figura <a href="shrinkage.html#fig:ridge-fit">6.1</a>).</p>
<div class="sourceCode" id="cb331"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb331-1"><a href="shrinkage.html#cb331-1" aria-hidden="true" tabindex="-1"></a>fit.ridge <span class="ot">&lt;-</span> <span class="fu">glmnet</span>(x, y, <span class="at">alpha =</span> <span class="dv">0</span>)</span>
<span id="cb331-2"><a href="shrinkage.html#cb331-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(fit.ridge, <span class="at">xvar =</span> <span class="st">&quot;lambda&quot;</span>, <span class="at">label =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ridge-fit"></span>
<img src="06-ext_glm_files/figure-html/ridge-fit-1.png" alt="Gráfico de perfil de la evolución de los coeficientes en función del logaritmo de la penalización del ajuste ridge." width="75%" />
<p class="caption">
Figura 6.1: Gráfico de perfil de la evolución de los coeficientes en función del logaritmo de la penalización del ajuste ridge.
</p>
</div>
<p>Podemos obtener el modelo o predicciones para un valor concreto de <span class="math inline">\(\lambda\)</span>:</p>
<div class="sourceCode" id="cb332"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb332-1"><a href="shrinkage.html#cb332-1" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(fit.ridge, <span class="at">s =</span> <span class="dv">2</span>) <span class="co"># lambda = 2</span></span></code></pre></div>
<pre><code>## 14 x 1 sparse Matrix of class &quot;dgCMatrix&quot;
##                    s1
## (Intercept) -2.948568
## age          0.079808
## weight       0.066058
## height      -0.163590
## neck        -0.018690
## chest        0.135327
## abdomen      0.344659
## hip          0.110577
## thigh        0.145096
## knee         0.113076
## ankle       -0.206822
## biceps       0.072182
## forearm      0.073053
## wrist       -1.441640</code></pre>
<p>Para seleccionar el parámetro de penalización por validación cruzada, empleamos <a href="https://glmnet.stanford.edu/reference/cv.glmnet.html"><code>cv.glmnet()</code></a> (normalmente emplearíamos esta función en lugar de <code>glmnet()</code>).
El correspondiente método <a href="https://glmnet.stanford.edu/reference/plot.cv.glmnet.html"><code>plot()</code></a> muestra la evolución de los errores de validación cruzada en función de la penalización, incluyendo las bandas de un error estándar de Breiman (ver Figura <a href="shrinkage.html#fig:ridge-cv">6.2</a>).</p>

<div class="sourceCode" id="cb334"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb334-1"><a href="shrinkage.html#cb334-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb334-2"><a href="shrinkage.html#cb334-2" aria-hidden="true" tabindex="-1"></a>cv.ridge <span class="ot">&lt;-</span> <span class="fu">cv.glmnet</span>(x, y, <span class="at">alpha =</span> <span class="dv">0</span>)</span>
<span id="cb334-3"><a href="shrinkage.html#cb334-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(cv.ridge)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ridge-cv"></span>
<img src="06-ext_glm_files/figure-html/ridge-cv-1.png" alt="Error cuadrático medio de validación cruzada en función del logaritmo de la penalización del ajuste ridge, junto con los intervalos de un error estándar. Las líneas verticales se corresponden con lambda.min y lambda.1se." width="75%" />
<p class="caption">
Figura 6.2: Error cuadrático medio de validación cruzada en función del logaritmo de la penalización del ajuste ridge, junto con los intervalos de un error estándar. Las líneas verticales se corresponden con <code>lambda.min</code> y <code>lambda.1se</code>.
</p>
</div>
<p>En este caso el parámetro óptimo, según la regla de un error estándar de Breiman, sería<a href="#fn56" class="footnote-ref" id="fnref56"><sup>56</sup></a>:</p>
<div class="sourceCode" id="cb335"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb335-1"><a href="shrinkage.html#cb335-1" aria-hidden="true" tabindex="-1"></a>cv.ridge<span class="sc">$</span>lambda<span class="fl">.1</span>se</span></code></pre></div>
<pre><code>## [1] 1.6984</code></pre>
<p>y el correspondiente modelo contiene todas las variables explicativas:</p>
<div class="sourceCode" id="cb337"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb337-1"><a href="shrinkage.html#cb337-1" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(cv.ridge) <span class="co"># s = &quot;lambda.1se&quot;</span></span></code></pre></div>
<pre><code>## 14 x 1 sparse Matrix of class &quot;dgCMatrix&quot;
##                    s1
## (Intercept) -1.344135
## age          0.080359
## weight       0.066044
## height      -0.164795
## neck        -0.035658
## chest        0.129400
## abdomen      0.368416
## hip          0.102600
## thigh        0.145259
## knee         0.105121
## ankle       -0.200403
## biceps       0.069859
## forearm      0.084012
## wrist       -1.532999</code></pre>
<p>Finalmente, evaluamos la precisión en la muestra de test:</p>
<div class="sourceCode" id="cb339"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb339-1"><a href="shrinkage.html#cb339-1" aria-hidden="true" tabindex="-1"></a>obs <span class="ot">&lt;-</span> test<span class="sc">$</span>bodyfat</span>
<span id="cb339-2"><a href="shrinkage.html#cb339-2" aria-hidden="true" tabindex="-1"></a>newx <span class="ot">&lt;-</span> <span class="fu">as.matrix</span>(test[<span class="sc">-</span><span class="dv">1</span>])</span>
<span id="cb339-3"><a href="shrinkage.html#cb339-3" aria-hidden="true" tabindex="-1"></a>pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(cv.ridge, <span class="at">newx =</span> newx) <span class="co"># s = &quot;lambda.1se&quot;</span></span>
<span id="cb339-4"><a href="shrinkage.html#cb339-4" aria-hidden="true" tabindex="-1"></a><span class="fu">accuracy</span>(pred, obs)</span></code></pre></div>
<pre><code>##        me      rmse       mae       mpe      mape r.squared 
##   1.17762   4.42419   3.81770  -4.64274  24.66151   0.69743</code></pre>
<!-- 
Aunque en este caso no mejoraríamos el ajuste lineal sin penalización o seleccionando el valor el mínimo global [^nota-glmnet-1].
-->
</div>
<div id="ejemplo-lasso" class="section level3 hasAnchor" number="6.1.3">
<h3><span class="header-section-number">6.1.3</span> Ejemplo: LASSO<a href="shrinkage.html#ejemplo-lasso" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Podemos ajustar modelos LASSO con la opción por defecto de <code>glmnet()</code> (<code>alpha = 1</code>, <em>LASSO penalty</em>).
Pero en este caso lo haremos al mismo tiempo que seleccionamos el parámetro de penalización por validación cruzada (ver Figura <a href="shrinkage.html#fig:lasso-cv">6.3</a>):</p>

<div class="sourceCode" id="cb341"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb341-1"><a href="shrinkage.html#cb341-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb341-2"><a href="shrinkage.html#cb341-2" aria-hidden="true" tabindex="-1"></a>cv.lasso <span class="ot">&lt;-</span> <span class="fu">cv.glmnet</span>(x,y)</span>
<span id="cb341-3"><a href="shrinkage.html#cb341-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(cv.lasso)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:lasso-cv"></span>
<img src="06-ext_glm_files/figure-html/lasso-cv-1.png" alt="Error cuadrático medio de validación cruzada en función del logaritmo de la penalización del ajuste LASSO, junto con los intervalos de un error estándar. Las líneas verticales se corresponden con lambda.min y lambda.1se." width="75%" />
<p class="caption">
Figura 6.3: Error cuadrático medio de validación cruzada en función del logaritmo de la penalización del ajuste LASSO, junto con los intervalos de un error estándar. Las líneas verticales se corresponden con <code>lambda.min</code> y <code>lambda.1se</code>.
</p>
</div>
<p>También podemos generar el gráfico con la evolución de los componentes a partir del ajuste almacenado en la componente <code>$glmnet.fit</code>:</p>

<div class="sourceCode" id="cb342"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb342-1"><a href="shrinkage.html#cb342-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(cv.lasso<span class="sc">$</span>glmnet.fit, <span class="at">xvar =</span> <span class="st">&quot;lambda&quot;</span>, <span class="at">label =</span> <span class="cn">TRUE</span>)    </span>
<span id="cb342-2"><a href="shrinkage.html#cb342-2" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v =</span> <span class="fu">log</span>(cv.lasso<span class="sc">$</span>lambda<span class="fl">.1</span>se), <span class="at">lty =</span> <span class="dv">2</span>)</span>
<span id="cb342-3"><a href="shrinkage.html#cb342-3" aria-hidden="true" tabindex="-1"></a><span class="fu">abline</span>(<span class="at">v =</span> <span class="fu">log</span>(cv.lasso<span class="sc">$</span>lambda.min), <span class="at">lty =</span> <span class="dv">3</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:lasso-fit"></span>
<img src="06-ext_glm_files/figure-html/lasso-fit-1.png" alt="Evolución de los coeficientes en función del logaritmo de la penalización del ajuste LASSO. Las líneas verticales se corresponden con lambda.min y lambda.1se." width="75%" />
<p class="caption">
Figura 6.4: Evolución de los coeficientes en función del logaritmo de la penalización del ajuste LASSO. Las líneas verticales se corresponden con <code>lambda.min</code> y <code>lambda.1se</code>.
</p>
</div>
<p>Como podemos observar en la Figura <a href="shrinkage.html#fig:lasso-fit">6.4</a>, la penalización LASSO tiende a forzar que las estimaciones de los coeficientes sean exactamente cero cuando el parámetro de penalización <span class="math inline">\(\lambda\)</span> es suficientemente grande.
En este caso, el modelo resultante (empleando la regla <em>oneSE</em>) solo contiene 3 variables explicativas:</p>
<div class="sourceCode" id="cb343"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb343-1"><a href="shrinkage.html#cb343-1" aria-hidden="true" tabindex="-1"></a><span class="fu">coef</span>(cv.lasso) <span class="co"># s = &quot;lambda.1se&quot;</span></span></code></pre></div>
<pre><code>## 14 x 1 sparse Matrix of class &quot;dgCMatrix&quot;
##                   s1
## (Intercept) -9.58463
## age          .      
## weight       .      
## height      -0.10060
## neck         .      
## chest        .      
## abdomen      0.66075
## hip          .      
## thigh        .      
## knee         .      
## ankle        .      
## biceps       .      
## forearm      .      
## wrist       -0.80327</code></pre>
<p>Por tanto, este método también podría ser empleado para la selección de variables.
Si se quisiera ajustar el modelo sin regularización con estas variables, solo habría que establecer <code>relax = TRUE</code> en la llamada a <code>glmnet()</code> o <code>cv.glmnet()</code>.</p>
<p>Finalmente, evaluamos también la precisión en la muestra de test:</p>
<div class="sourceCode" id="cb345"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb345-1"><a href="shrinkage.html#cb345-1" aria-hidden="true" tabindex="-1"></a>pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(cv.lasso, <span class="at">newx =</span> newx)</span>
<span id="cb345-2"><a href="shrinkage.html#cb345-2" aria-hidden="true" tabindex="-1"></a><span class="fu">accuracy</span>(pred, obs)</span></code></pre></div>
<pre><code>##        me      rmse       mae       mpe      mape r.squared 
##   1.32227   4.29096   3.73005  -3.38653  23.84939   0.71538</code></pre>
</div>
<div id="ejemplo-elastic-net" class="section level3 hasAnchor" number="6.1.4">
<h3><span class="header-section-number">6.1.4</span> Ejemplo: <em>elastic net</em><a href="shrinkage.html#ejemplo-elastic-net" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>Podemos ajustar modelos <em>elastic net</em> para un valor concreto de <code>alpha</code> empleando la función <code>glmnet()</code>, pero las opciones del paquete no incluyen la selección de este hiperparámetro.
Aunque se podría implementar fácilmente (como se muestra en <a href="https://glmnet.stanford.edu/reference/cv.glmnet.html"><code>help(cv.glmnet)</code></a>), resulta mucho más cómodo emplear el método <code>"glmnet"</code> de <code>caret</code>:</p>
<div class="sourceCode" id="cb347"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb347-1"><a href="shrinkage.html#cb347-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(caret)</span>
<span id="cb347-2"><a href="shrinkage.html#cb347-2" aria-hidden="true" tabindex="-1"></a><span class="fu">modelLookup</span>(<span class="st">&quot;glmnet&quot;</span>) </span></code></pre></div>
<pre><code>##    model parameter                    label forReg forClass probModel
## 1 glmnet     alpha        Mixing Percentage   TRUE     TRUE      TRUE
## 2 glmnet    lambda Regularization Parameter   TRUE     TRUE      TRUE</code></pre>
<div class="sourceCode" id="cb349"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb349-1"><a href="shrinkage.html#cb349-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb349-2"><a href="shrinkage.html#cb349-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Se podría emplear una fórmula: train(bodyfat ~ ., data = train, ...)</span></span>
<span id="cb349-3"><a href="shrinkage.html#cb349-3" aria-hidden="true" tabindex="-1"></a>caret.glmnet <span class="ot">&lt;-</span> <span class="fu">train</span>(x, y, <span class="at">method =</span> <span class="st">&quot;glmnet&quot;</span>, </span>
<span id="cb349-4"><a href="shrinkage.html#cb349-4" aria-hidden="true" tabindex="-1"></a>                      <span class="at">preProc =</span> <span class="fu">c</span>(<span class="st">&quot;zv&quot;</span>, <span class="st">&quot;center&quot;</span>, <span class="st">&quot;scale&quot;</span>), <span class="at">tuneLength =</span> <span class="dv">5</span>,</span>
<span id="cb349-5"><a href="shrinkage.html#cb349-5" aria-hidden="true" tabindex="-1"></a>                      <span class="at">trControl =</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">&quot;cv&quot;</span>, <span class="at">number =</span> <span class="dv">5</span>))</span>
<span id="cb349-6"><a href="shrinkage.html#cb349-6" aria-hidden="true" tabindex="-1"></a>caret.glmnet</span></code></pre></div>
<pre><code>## glmnet 
## 
## 196 samples
##  13 predictor
## 
## Pre-processing: centered (13), scaled (13) 
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 159, 157, 156, 156, 156 
## Resampling results across tuning parameters:
## 
##   alpha  lambda     RMSE    Rsquared  MAE   
##   0.100  0.0062187  4.4663  0.71359   3.6756
##   0.100  0.0288646  4.4595  0.71434   3.6741
##   0.100  0.1339777  4.4570  0.71443   3.6906
##   0.100  0.6218696  4.5473  0.70262   3.7527
##   0.100  2.8864628  4.9398  0.66058   4.0574
##   0.325  0.0062187  4.4646  0.71383   3.6735
##   0.325  0.0288646  4.4515  0.71542   3.6676
##   0.325  0.1339777  4.4547  0.71472   3.6885
##   0.325  0.6218696  4.5082  0.70919   3.7079
##   0.325  2.8864628  5.1988  0.63972   4.2495
##   0.550  0.0062187  4.4609  0.71432   3.6695
##   0.550  0.0288646  4.4459  0.71622   3.6637
##  [ reached getOption(&quot;max.print&quot;) -- omitted 13 rows ]
## 
## RMSE was used to select the optimal model using the smallest value.
## The final values used for the model were alpha = 1 and lambda = 0.13398.</code></pre>
<!-- 
Pendiente: probar a incluir rejilla con lambda = 0 (sin penalización)
-->
<p>Los resultados de la selección de los hiperparámetros <span class="math inline">\(\alpha\)</span> y <span class="math inline">\(\lambda\)</span> de regularización se muestran en la Figura <a href="shrinkage.html#fig:elastic-caret">6.5</a>:</p>

<div class="sourceCode" id="cb351"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb351-1"><a href="shrinkage.html#cb351-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ggplot</span>(caret.glmnet, <span class="at">highlight =</span> <span class="cn">TRUE</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:elastic-caret"></span>
<img src="06-ext_glm_files/figure-html/elastic-caret-1.png" alt="Errores RMSE de validación cruzada de los modelos elastic net en función de los hiperparámetros de regularización." width="75%" />
<p class="caption">
Figura 6.5: Errores RMSE de validación cruzada de los modelos <em>elastic net</em> en función de los hiperparámetros de regularización.
</p>
</div>
<p>Finalmente, se evalúan las predicciones en la muestra de test del modelo ajustado (que en esta ocasión mejoran los resultados del modelo LASSO obtenido en la sección anterior):</p>
<div class="sourceCode" id="cb352"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb352-1"><a href="shrinkage.html#cb352-1" aria-hidden="true" tabindex="-1"></a>pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(caret.glmnet, <span class="at">newdata =</span> test)</span>
<span id="cb352-2"><a href="shrinkage.html#cb352-2" aria-hidden="true" tabindex="-1"></a><span class="fu">accuracy</span>(pred, obs)</span></code></pre></div>
<pre><code>##        me      rmse       mae       mpe      mape r.squared 
##   1.36601   4.09331   3.50120  -1.25401  21.20872   0.74099</code></pre>
<!-- 
Aunque tampoco mejoraríamos el ajuste lineal sin penalización.
-->
<div class="exercise">
<p><span id="exr:bfan-glmnet" class="exercise"><strong>Ejercicio 6.1  </strong></span>Continuando con el conjunto de datos <a href="https://rubenfcasal.github.io/mpae/reference/bfan.html"><code>mpae::bfan</code></a> empleado en ejercicios de capítulos anteriores, particiona los datos y clasifica los individuos según su nivel de grasa corporal (<code>bfan</code>) mediante modelos logísticos:</p>
<ol style="list-style-type: lower-alpha">
<li><p>Con penalización <em>ridge</em>, seleccionada mediante validación cruzada, empleando el paquete <code>glmnet</code>.</p></li>
<li><p>Con penalización LASSO, seleccionada mediante validación cruzada, empleando el paquete <code>glmnet</code>.</p></li>
<li><p>Con penalización <em>elastic net</em>, seleccionando los valores óptimos de los hiperparámetros, empleando <code>caret</code>.</p></li>
<li><p>Evalúa la precisión de las predicciones de los modelos en la muestra de test y compara los resultados.</p></li>
</ol>
</div>
</div>
</div>
<h3>Bibliografía<a href="bibliografía.html#bibliografía" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent" line-spacing="2">
<div id="ref-breiman1984classification" class="csl-entry">
Breiman, L., Friedman, J., Stone, C. J., y Olshen, R. A. (1984). <em>Classification and Regression Trees</em>. Taylor; Francis.
</div>
<div id="ref-efron2004least" class="csl-entry">
Efron, B., Hastie, T., Johnstone, I., y Tibshirani, R. (2004). Least angle regression. <em>The Annals of Statistics</em>, <em>32</em>(2), 407-499. <a href="https://doi.org/10.1214/009053604000000067">https://doi.org/10.1214/009053604000000067</a>
</div>
<div id="ref-R-glmnet" class="csl-entry">
Friedman, J., Hastie, T., Tibshirani, R., Narasimhan, B., Tay, K., Simon, N., Qian, J., y Yang, J. (2023). <em><span>glmnet: Lasso and Elastic-Net Regularized Generalized Linear Models</span></em>. <a href="https://cran.r-project.org/package=glmnet">https://cran.r-project.org/package=glmnet</a>
</div>
<div id="ref-hoerl1970ridge" class="csl-entry">
Hoerl, A. E., y Kennard, R. W. (1970). Ridge regression: Biased estimation for nonorthogonal problems. <em>Technometrics</em>, <em>12</em>(1), 55-67. <a href="https://doi.org/10.1080/00401706.2000.10485983">https://doi.org/10.1080/00401706.2000.10485983</a>
</div>
<div id="ref-tibshirani1996regression" class="csl-entry">
Tibshirani, R. (1996). Regression shrinkage and selection via the lasso. <em>Journal of the Royal Statistical Society: Series B (Methodological)</em>, <em>58</em>(1), 267-288. <a href="https://doi.org/10.1111/j.2517-6161.1996.tb02080.x">https://doi.org/10.1111/j.2517-6161.1996.tb02080.x</a>
</div>
<div id="ref-zou2005regularization" class="csl-entry">
Zou, H., y Hastie, T. (2005). Regularization and Variable Selection via the Elastic Net. <em>Journal of the Royal Statistical Society, Series B (Statistical Methodology)</em>, <em>67</em>(2), 301-320. <a href="https://doi.org/10.1111/j.1467-9868.2005.00503.x">https://doi.org/10.1111/j.1467-9868.2005.00503.x</a>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="55">
<li id="fn55"><p>Los autores afirman que utilizan <code>s</code> en lugar de <code>lambda</code> por motivos históricos.<a href="shrinkage.html#fnref55" class="footnote-back">↩︎</a></p></li>
<li id="fn56"><p>Para obtener el valor óptimo global podemos emplear <code>cv.ridge$lambda.min</code>, y añadir el argumento <code>s = "lambda.min"</code> a los métodos <code>coef()</code> y <code>predict()</code> para obtener los correspondientes coeficientes y predicciones.<a href="shrinkage.html#fnref56" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="ext-glm.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="pca-pls.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rubenfcasal/aprendizaje_estadistico/edit/master/06-ext_glm.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["aprendizaje_estadistico.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
