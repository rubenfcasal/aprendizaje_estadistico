<!DOCTYPE html>
<html lang="es" xml:lang="es">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>3.3 Bagging y bosques aleatorios en R | Aprendizaje Estadístico</title>
  <meta name="description" content="Apuntes de la asignatura de Aprendizaje Estadístico del Máster en Técnicas Estadísticas." />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="3.3 Bagging y bosques aleatorios en R | Aprendizaje Estadístico" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="Apuntes de la asignatura de Aprendizaje Estadístico del Máster en Técnicas Estadísticas." />
  <meta name="github-repo" content="rubenfcasal/aprendizaje_estadistico" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="3.3 Bagging y bosques aleatorios en R | Aprendizaje Estadístico" />
  
  <meta name="twitter:description" content="Apuntes de la asignatura de Aprendizaje Estadístico del Máster en Técnicas Estadísticas." />
  

<meta name="author" content="Rubén Fernández Casal (ruben.fcasal@udc.es)" />
<meta name="author" content="Julián Costa Bouzas (julian.costa@udc.es)" />
<meta name="author" content="Manuel Oviedo de la Fuente (manuel.oviedo@udc.es)" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="bosques-aleatorios.html"/>
<link rel="next" href="boosting.html"/>
<script src="libs/header-attrs-2.9/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>
<script src="libs/htmlwidgets-1.5.4/htmlwidgets.js"></script>
<link href="libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet" />
<script src="libs/datatables-binding-0.19/datatables.js"></script>
<link href="libs/dt-core-1.10.20/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.10.20/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.10.20/js/jquery.dataTables.min.js"></script>
<link href="libs/crosstalk-1.2.0/css/crosstalk.min.css" rel="stylesheet" />
<script src="libs/crosstalk-1.2.0/js/crosstalk.min.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Aprendizaje Estadístico</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Prólogo</a></li>
<li class="chapter" data-level="1" data-path="intro-AE.html"><a href="intro-AE.html"><i class="fa fa-check"></i><b>1</b> Introducción al Aprendizaje Estadístico</a>
<ul>
<li class="chapter" data-level="1.1" data-path="aprendizaje-estadístico-vs.html"><a href="aprendizaje-estadístico-vs.html"><i class="fa fa-check"></i><b>1.1</b> Aprendizaje Estadístico vs. Aprendizaje Automático</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="aprendizaje-estadístico-vs.html"><a href="aprendizaje-estadístico-vs.html#machine-learning-vs.-data-mining"><i class="fa fa-check"></i><b>1.1.1</b> Machine Learning vs. Data Mining</a></li>
<li class="chapter" data-level="1.1.2" data-path="aprendizaje-estadístico-vs.html"><a href="aprendizaje-estadístico-vs.html#las-dos-culturas-breiman2001statistical"><i class="fa fa-check"></i><b>1.1.2</b> Las dos culturas <span class="citation">(<span>Breiman, 2001b</span>)</span></a></li>
<li class="chapter" data-level="1.1.3" data-path="aprendizaje-estadístico-vs.html"><a href="aprendizaje-estadístico-vs.html#machine-learning-vs.-estadística-dunson2018statistics"><i class="fa fa-check"></i><b>1.1.3</b> Machine Learning vs. Estadística <span class="citation">(<span>Dunson, 2018</span>)</span></a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="métodos-de-aprendizaje-estadístico.html"><a href="métodos-de-aprendizaje-estadístico.html"><i class="fa fa-check"></i><b>1.2</b> Métodos de Aprendizaje Estadístico</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="métodos-de-aprendizaje-estadístico.html"><a href="métodos-de-aprendizaje-estadístico.html#notacion"><i class="fa fa-check"></i><b>1.2.1</b> Notación y terminología</a></li>
<li class="chapter" data-level="1.2.2" data-path="métodos-de-aprendizaje-estadístico.html"><a href="métodos-de-aprendizaje-estadístico.html#metodos-pkgs"><i class="fa fa-check"></i><b>1.2.2</b> Métodos (de aprendizaje supervisado) y paquetes de R</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="const-eval.html"><a href="const-eval.html"><i class="fa fa-check"></i><b>1.3</b> Construcción y evaluación de los modelos</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="const-eval.html"><a href="const-eval.html#bias-variance"><i class="fa fa-check"></i><b>1.3.1</b> Equilibrio entre sesgo y varianza: infraajuste y sobreajuste</a></li>
<li class="chapter" data-level="1.3.2" data-path="const-eval.html"><a href="const-eval.html#entrenamiento-test"><i class="fa fa-check"></i><b>1.3.2</b> Datos de entrenamiento y datos de test</a></li>
<li class="chapter" data-level="1.3.3" data-path="const-eval.html"><a href="const-eval.html#cv"><i class="fa fa-check"></i><b>1.3.3</b> Validación cruzada</a></li>
<li class="chapter" data-level="1.3.4" data-path="const-eval.html"><a href="const-eval.html#eval-reg"><i class="fa fa-check"></i><b>1.3.4</b> Evaluación de un método de regresión</a></li>
<li class="chapter" data-level="1.3.5" data-path="const-eval.html"><a href="const-eval.html#eval-class"><i class="fa fa-check"></i><b>1.3.5</b> Evaluación de un método de clasificación</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="dimen-curse.html"><a href="dimen-curse.html"><i class="fa fa-check"></i><b>1.4</b> La maldición de la dimensionalidad</a></li>
<li class="chapter" data-level="1.5" data-path="analisis-modelos.html"><a href="analisis-modelos.html"><i class="fa fa-check"></i><b>1.5</b> Análisis e interpretación de los modelos</a></li>
<li class="chapter" data-level="1.6" data-path="caret.html"><a href="caret.html"><i class="fa fa-check"></i><b>1.6</b> Introducción al paquete <code>caret</code></a>
<ul>
<li class="chapter" data-level="1.6.1" data-path="caret.html"><a href="caret.html#métodos-implementados"><i class="fa fa-check"></i><b>1.6.1</b> Métodos implementados</a></li>
<li class="chapter" data-level="1.6.2" data-path="caret.html"><a href="caret.html#herramientas"><i class="fa fa-check"></i><b>1.6.2</b> Herramientas</a></li>
<li class="chapter" data-level="1.6.3" data-path="caret.html"><a href="caret.html#ejemplo"><i class="fa fa-check"></i><b>1.6.3</b> Ejemplo</a></li>
<li class="chapter" data-level="1.6.4" data-path="caret.html"><a href="caret.html#desarrollo-futuro"><i class="fa fa-check"></i><b>1.6.4</b> Desarrollo futuro</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="trees.html"><a href="trees.html"><i class="fa fa-check"></i><b>2</b> Árboles de decisión</a>
<ul>
<li class="chapter" data-level="2.1" data-path="árboles-de-regresión-cart.html"><a href="árboles-de-regresión-cart.html"><i class="fa fa-check"></i><b>2.1</b> Árboles de regresión CART</a></li>
<li class="chapter" data-level="2.2" data-path="árboles-de-clasificación-cart.html"><a href="árboles-de-clasificación-cart.html"><i class="fa fa-check"></i><b>2.2</b> Árboles de clasificación CART</a></li>
<li class="chapter" data-level="2.3" data-path="cart-con-el-paquete-rpart.html"><a href="cart-con-el-paquete-rpart.html"><i class="fa fa-check"></i><b>2.3</b> CART con el paquete <code>rpart</code></a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="cart-con-el-paquete-rpart.html"><a href="cart-con-el-paquete-rpart.html#ejemplo-regresión"><i class="fa fa-check"></i><b>2.3.1</b> Ejemplo: regresión</a></li>
<li class="chapter" data-level="2.3.2" data-path="cart-con-el-paquete-rpart.html"><a href="cart-con-el-paquete-rpart.html#class-rpart"><i class="fa fa-check"></i><b>2.3.2</b> Ejemplo: modelo de clasificación</a></li>
<li class="chapter" data-level="2.3.3" data-path="cart-con-el-paquete-rpart.html"><a href="cart-con-el-paquete-rpart.html#interfaz-de-caret"><i class="fa fa-check"></i><b>2.3.3</b> Interfaz de <code>caret</code></a></li>
</ul></li>
<li class="chapter" data-level="2.4" data-path="alternativas-a-los-árboles-cart.html"><a href="alternativas-a-los-árboles-cart.html"><i class="fa fa-check"></i><b>2.4</b> Alternativas a los árboles CART</a>
<ul>
<li class="chapter" data-level="2.4.1" data-path="alternativas-a-los-árboles-cart.html"><a href="alternativas-a-los-árboles-cart.html#ejemplo-1"><i class="fa fa-check"></i><b>2.4.1</b> Ejemplo</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="bagging-boosting.html"><a href="bagging-boosting.html"><i class="fa fa-check"></i><b>3</b> Bagging y Boosting</a>
<ul>
<li class="chapter" data-level="3.1" data-path="bagging.html"><a href="bagging.html"><i class="fa fa-check"></i><b>3.1</b> Bagging</a></li>
<li class="chapter" data-level="3.2" data-path="bosques-aleatorios.html"><a href="bosques-aleatorios.html"><i class="fa fa-check"></i><b>3.2</b> Bosques aleatorios</a></li>
<li class="chapter" data-level="3.3" data-path="bagging-rf-r.html"><a href="bagging-rf-r.html"><i class="fa fa-check"></i><b>3.3</b> Bagging y bosques aleatorios en R</a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="bagging-rf-r.html"><a href="bagging-rf-r.html#ejemplo-clasificación-con-bagging"><i class="fa fa-check"></i><b>3.3.1</b> Ejemplo: Clasificación con bagging</a></li>
<li class="chapter" data-level="3.3.2" data-path="bagging-rf-r.html"><a href="bagging-rf-r.html#ejemplo-clasif-rf"><i class="fa fa-check"></i><b>3.3.2</b> Ejemplo: Clasificación con bosques aleatorios</a></li>
<li class="chapter" data-level="3.3.3" data-path="bagging-rf-r.html"><a href="bagging-rf-r.html#ejemplo-bosques-aleatorios-con-caret"><i class="fa fa-check"></i><b>3.3.3</b> Ejemplo: bosques aleatorios con <code>caret</code></a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="boosting.html"><a href="boosting.html"><i class="fa fa-check"></i><b>3.4</b> Boosting</a></li>
<li class="chapter" data-level="3.5" data-path="boosting-en-r.html"><a href="boosting-en-r.html"><i class="fa fa-check"></i><b>3.5</b> Boosting en R</a>
<ul>
<li class="chapter" data-level="3.5.1" data-path="boosting-en-r.html"><a href="boosting-en-r.html#ejemplo-clasificación-con-el-paquete-ada"><i class="fa fa-check"></i><b>3.5.1</b> Ejemplo: clasificación con el paquete <code>ada</code></a></li>
<li class="chapter" data-level="3.5.2" data-path="boosting-en-r.html"><a href="boosting-en-r.html#ejemplo-regresión-con-el-paquete-gbm"><i class="fa fa-check"></i><b>3.5.2</b> Ejemplo: regresión con el paquete <code>gbm</code></a></li>
<li class="chapter" data-level="3.5.3" data-path="boosting-en-r.html"><a href="boosting-en-r.html#ejemplo-xgboost-con-el-paquete-caret"><i class="fa fa-check"></i><b>3.5.3</b> Ejemplo: XGBoost con el paquete <code>caret</code></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="svm.html"><a href="svm.html"><i class="fa fa-check"></i><b>4</b> Máquinas de soporte vectorial</a>
<ul>
<li class="chapter" data-level="4.1" data-path="clasificadores-de-máximo-margen.html"><a href="clasificadores-de-máximo-margen.html"><i class="fa fa-check"></i><b>4.1</b> Clasificadores de máximo margen</a></li>
<li class="chapter" data-level="4.2" data-path="clasificadores-de-soporte-vectorial.html"><a href="clasificadores-de-soporte-vectorial.html"><i class="fa fa-check"></i><b>4.2</b> Clasificadores de soporte vectorial</a></li>
<li class="chapter" data-level="4.3" data-path="máquinas-de-soporte-vectorial.html"><a href="máquinas-de-soporte-vectorial.html"><i class="fa fa-check"></i><b>4.3</b> Máquinas de soporte vectorial</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="máquinas-de-soporte-vectorial.html"><a href="máquinas-de-soporte-vectorial.html#clasificación-con-más-de-dos-categorías"><i class="fa fa-check"></i><b>4.3.1</b> Clasificación con más de dos categorías</a></li>
<li class="chapter" data-level="4.3.2" data-path="máquinas-de-soporte-vectorial.html"><a href="máquinas-de-soporte-vectorial.html#regresión"><i class="fa fa-check"></i><b>4.3.2</b> Regresión</a></li>
<li class="chapter" data-level="4.3.3" data-path="máquinas-de-soporte-vectorial.html"><a href="máquinas-de-soporte-vectorial.html#ventajas-e-incovenientes"><i class="fa fa-check"></i><b>4.3.3</b> Ventajas e incovenientes</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="svm-con-el-paquete-kernlab.html"><a href="svm-con-el-paquete-kernlab.html"><i class="fa fa-check"></i><b>4.4</b> SVM con el paquete <code>kernlab</code></a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="class-otros.html"><a href="class-otros.html"><i class="fa fa-check"></i><b>5</b> Otros métodos de clasificación</a>
<ul>
<li class="chapter" data-level="5.1" data-path="análisis-discriminate-lineal.html"><a href="análisis-discriminate-lineal.html"><i class="fa fa-check"></i><b>5.1</b> Análisis discriminate lineal</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="análisis-discriminate-lineal.html"><a href="análisis-discriminate-lineal.html#ejemplo-masslda"><i class="fa fa-check"></i><b>5.1.1</b> Ejemplo <code>MASS::lda</code></a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="análisis-discriminante-cuadrático.html"><a href="análisis-discriminante-cuadrático.html"><i class="fa fa-check"></i><b>5.2</b> Análisis discriminante cuadrático</a>
<ul>
<li class="chapter" data-level="5.2.1" data-path="análisis-discriminante-cuadrático.html"><a href="análisis-discriminante-cuadrático.html#ejemplo-massqda"><i class="fa fa-check"></i><b>5.2.1</b> Ejemplo <code>MASS::qda</code></a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="naive-bayes.html"><a href="naive-bayes.html"><i class="fa fa-check"></i><b>5.3</b> Naive Bayes</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="naive-bayes.html"><a href="naive-bayes.html#ejemplo-e1071naivebayes"><i class="fa fa-check"></i><b>5.3.1</b> Ejemplo <code>e1071::naiveBayes</code></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="modelos-lineales.html"><a href="modelos-lineales.html"><i class="fa fa-check"></i><b>6</b> Modelos lineales y extensiones</a>
<ul>
<li class="chapter" data-level="6.1" data-path="reg-multiple.html"><a href="reg-multiple.html"><i class="fa fa-check"></i><b>6.1</b> Regresión lineal múltiple</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="reg-multiple.html"><a href="reg-multiple.html#ajuste-función-lm"><i class="fa fa-check"></i><b>6.1.1</b> Ajuste: función <code>lm</code></a></li>
<li class="chapter" data-level="6.1.2" data-path="reg-multiple.html"><a href="reg-multiple.html#ejemplo-2"><i class="fa fa-check"></i><b>6.1.2</b> Ejemplo</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="colinealidad.html"><a href="colinealidad.html"><i class="fa fa-check"></i><b>6.2</b> El problema de la colinealidad</a></li>
<li class="chapter" data-level="6.3" data-path="seleccion-reg-lineal.html"><a href="seleccion-reg-lineal.html"><i class="fa fa-check"></i><b>6.3</b> Selección de variables explicativas</a>
<ul>
<li class="chapter" data-level="6.3.1" data-path="seleccion-reg-lineal.html"><a href="seleccion-reg-lineal.html#búsqueda-exhaustiva"><i class="fa fa-check"></i><b>6.3.1</b> Búsqueda exhaustiva</a></li>
<li class="chapter" data-level="6.3.2" data-path="seleccion-reg-lineal.html"><a href="seleccion-reg-lineal.html#selección-por-pasos"><i class="fa fa-check"></i><b>6.3.2</b> Selección por pasos</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="analisis-reg-multiple.html"><a href="analisis-reg-multiple.html"><i class="fa fa-check"></i><b>6.4</b> Análisis e interpretación del modelo</a></li>
<li class="chapter" data-level="6.5" data-path="evaluación-de-la-precisión.html"><a href="evaluación-de-la-precisión.html"><i class="fa fa-check"></i><b>6.5</b> Evaluación de la precisión</a></li>
<li class="chapter" data-level="6.6" data-path="shrinkage.html"><a href="shrinkage.html"><i class="fa fa-check"></i><b>6.6</b> Métodos de regularización</a>
<ul>
<li class="chapter" data-level="6.6.1" data-path="shrinkage.html"><a href="shrinkage.html#implementación-en-r"><i class="fa fa-check"></i><b>6.6.1</b> Implementación en R</a></li>
<li class="chapter" data-level="6.6.2" data-path="shrinkage.html"><a href="shrinkage.html#ejemplo-ridge-regression"><i class="fa fa-check"></i><b>6.6.2</b> Ejemplo: Ridge Regression</a></li>
<li class="chapter" data-level="6.6.3" data-path="shrinkage.html"><a href="shrinkage.html#ejemplo-lasso"><i class="fa fa-check"></i><b>6.6.3</b> Ejemplo: Lasso</a></li>
<li class="chapter" data-level="6.6.4" data-path="shrinkage.html"><a href="shrinkage.html#ejemplo-elastic-net"><i class="fa fa-check"></i><b>6.6.4</b> Ejemplo: Elastic Net</a></li>
</ul></li>
<li class="chapter" data-level="6.7" data-path="pca-pls.html"><a href="pca-pls.html"><i class="fa fa-check"></i><b>6.7</b> Métodos de reducción de la dimensión</a>
<ul>
<li class="chapter" data-level="6.7.1" data-path="pca-pls.html"><a href="pca-pls.html#regresión-por-componentes-principales-pcr"><i class="fa fa-check"></i><b>6.7.1</b> Regresión por componentes principales (PCR)</a></li>
<li class="chapter" data-level="6.7.2" data-path="pca-pls.html"><a href="pca-pls.html#regresión-por-mínimos-cuadrados-parciales-plsr"><i class="fa fa-check"></i><b>6.7.2</b> Regresión por mínimos cuadrados parciales (PLSR)</a></li>
</ul></li>
<li class="chapter" data-level="6.8" data-path="reg-glm.html"><a href="reg-glm.html"><i class="fa fa-check"></i><b>6.8</b> Modelos lineales generalizados</a>
<ul>
<li class="chapter" data-level="6.8.1" data-path="reg-glm.html"><a href="reg-glm.html#ajuste-función-glm"><i class="fa fa-check"></i><b>6.8.1</b> Ajuste: función <code>glm</code></a></li>
<li class="chapter" data-level="6.8.2" data-path="reg-glm.html"><a href="reg-glm.html#ejemplo-regresión-logística"><i class="fa fa-check"></i><b>6.8.2</b> Ejemplo: Regresión logística</a></li>
<li class="chapter" data-level="6.8.3" data-path="reg-glm.html"><a href="reg-glm.html#selección-de-variables-explicativas"><i class="fa fa-check"></i><b>6.8.3</b> Selección de variables explicativas</a></li>
<li class="chapter" data-level="6.8.4" data-path="reg-glm.html"><a href="reg-glm.html#analisis-glm"><i class="fa fa-check"></i><b>6.8.4</b> Análisis e interpretación del modelo</a></li>
<li class="chapter" data-level="6.8.5" data-path="reg-glm.html"><a href="reg-glm.html#evaluación-de-la-precisión-1"><i class="fa fa-check"></i><b>6.8.5</b> Evaluación de la precisión</a></li>
<li class="chapter" data-level="6.8.6" data-path="reg-glm.html"><a href="reg-glm.html#extensiones"><i class="fa fa-check"></i><b>6.8.6</b> Extensiones</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="reg-np.html"><a href="reg-np.html"><i class="fa fa-check"></i><b>7</b> Regresión no paramétrica</a>
<ul>
<li class="chapter" data-level="7.1" data-path="reg-local.html"><a href="reg-local.html"><i class="fa fa-check"></i><b>7.1</b> Regresión local</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="reg-local.html"><a href="reg-local.html#reg-knn"><i class="fa fa-check"></i><b>7.1.1</b> Vecinos más próximos</a></li>
<li class="chapter" data-level="7.1.2" data-path="reg-local.html"><a href="reg-local.html#reg-locpol"><i class="fa fa-check"></i><b>7.1.2</b> Regresión polinómica local</a></li>
<li class="chapter" data-level="7.1.3" data-path="reg-local.html"><a href="reg-local.html#regresión-polinómica-local-robusta"><i class="fa fa-check"></i><b>7.1.3</b> Regresión polinómica local robusta</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="splines.html"><a href="splines.html"><i class="fa fa-check"></i><b>7.2</b> Splines</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="splines.html"><a href="splines.html#reg-splines"><i class="fa fa-check"></i><b>7.2.1</b> Regression splines</a></li>
<li class="chapter" data-level="7.2.2" data-path="splines.html"><a href="splines.html#smoothing-splines"><i class="fa fa-check"></i><b>7.2.2</b> Smoothing splines</a></li>
<li class="chapter" data-level="7.2.3" data-path="splines.html"><a href="splines.html#splines-penalizados"><i class="fa fa-check"></i><b>7.2.3</b> Splines penalizados</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="modelos-aditivos.html"><a href="modelos-aditivos.html"><i class="fa fa-check"></i><b>7.3</b> Modelos aditivos</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="modelos-aditivos.html"><a href="modelos-aditivos.html#ajuste-función-gam"><i class="fa fa-check"></i><b>7.3.1</b> Ajuste: función <code>gam</code></a></li>
<li class="chapter" data-level="7.3.2" data-path="modelos-aditivos.html"><a href="modelos-aditivos.html#ejemplo-3"><i class="fa fa-check"></i><b>7.3.2</b> Ejemplo</a></li>
<li class="chapter" data-level="7.3.3" data-path="modelos-aditivos.html"><a href="modelos-aditivos.html#superficies-de-predicción"><i class="fa fa-check"></i><b>7.3.3</b> Superficies de predicción</a></li>
<li class="chapter" data-level="7.3.4" data-path="modelos-aditivos.html"><a href="modelos-aditivos.html#comparación-y-selección-de-modelos"><i class="fa fa-check"></i><b>7.3.4</b> Comparación y selección de modelos</a></li>
<li class="chapter" data-level="7.3.5" data-path="modelos-aditivos.html"><a href="modelos-aditivos.html#mgcv-diagnosis"><i class="fa fa-check"></i><b>7.3.5</b> Diagnosis del modelo</a></li>
<li class="chapter" data-level="7.3.6" data-path="modelos-aditivos.html"><a href="modelos-aditivos.html#gam-en-caret"><i class="fa fa-check"></i><b>7.3.6</b> GAM en <code>caret</code></a></li>
<li class="chapter" data-level="7.3.7" data-path="modelos-aditivos.html"><a href="modelos-aditivos.html#ejercicios"><i class="fa fa-check"></i><b>7.3.7</b> Ejercicios</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="mars.html"><a href="mars.html"><i class="fa fa-check"></i><b>7.4</b> Regresión spline adaptativa multivariante</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="mars.html"><a href="mars.html#mars-con-el-paquete-earth"><i class="fa fa-check"></i><b>7.4.1</b> MARS con el paquete <code>earth</code></a></li>
<li class="chapter" data-level="7.4.2" data-path="mars.html"><a href="mars.html#mars-con-el-paquete-caret"><i class="fa fa-check"></i><b>7.4.2</b> MARS con el paquete <code>caret</code></a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="projection-pursuit.html"><a href="projection-pursuit.html"><i class="fa fa-check"></i><b>7.5</b> Projection pursuit</a>
<ul>
<li class="chapter" data-level="7.5.1" data-path="projection-pursuit.html"><a href="projection-pursuit.html#ppr"><i class="fa fa-check"></i><b>7.5.1</b> Regresión por <em>projection pursuit</em></a></li>
<li class="chapter" data-level="7.5.2" data-path="projection-pursuit.html"><a href="projection-pursuit.html#implementación-en-r-1"><i class="fa fa-check"></i><b>7.5.2</b> Implementación en R</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="neural-nets.html"><a href="neural-nets.html"><i class="fa fa-check"></i><b>8</b> Redes neuronales</a>
<ul>
<li class="chapter" data-level="8.1" data-path="single-hidden-layer-feedforward-network.html"><a href="single-hidden-layer-feedforward-network.html"><i class="fa fa-check"></i><b>8.1</b> Single-hidden-layer feedforward network</a></li>
<li class="chapter" data-level="8.2" data-path="clasificación-con-ann.html"><a href="clasificación-con-ann.html"><i class="fa fa-check"></i><b>8.2</b> Clasificación con ANN</a></li>
<li class="chapter" data-level="8.3" data-path="implementación-en-r-2.html"><a href="implementación-en-r-2.html"><i class="fa fa-check"></i><b>8.3</b> Implementación en R</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="referencias.html"><a href="referencias.html"><i class="fa fa-check"></i>Referencias</a>
<ul>
<li class="chapter" data-level="" data-path="bibliografía-completa.html"><a href="bibliografía-completa.html"><i class="fa fa-check"></i>Bibliografía completa</a></li>
</ul></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Aprendizaje Estadístico</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="bagging-rf-r" class="section level2" number="3.3">
<h2><span class="header-section-number">3.3</span> Bagging y bosques aleatorios en R</h2>
<!-- 
Búsquedas en caret: bag, forest 
Ver [CRAN Task View: Machine Learning & Statistical Learning](https://cran.r-project.org/web/views/MachineLearning.html))
-->
<p>Estos algoritmos son de los más populares en AE y están implementados en numerosos paquetes de R, aunque la referencia es el paquete <a href="https://CRAN.R-project.org/package=randomForest"><code>randomForest</code></a> (que emplea el código Fortran desarrollado por Leo Breiman y Adele Cutler).
La función principal es <code>randomForest()</code> y se suele emplear de la forma:</p>
<p><code>randomForest(formula, data, ntree, mtry, nodesize, ...)</code></p>
<ul>
<li><p><code>formula</code> y <code>data</code> (opcional): permiten especificar la respuesta y las variables predictoras de la forma habitual (típicamente <code>respuesta ~ .</code>), aunque si el conjunto de datos es muy grande puede ser preferible emplear una matriz o un data.frame para establecer los predictores y un vector para la respuesta (sustituyendo estos argumentos por <code>x</code> e <code>y</code>).
Si la respuesta es un factor asumirá que se trata de un problema de clasificación y de regresión en caso contrario.</p></li>
<li><p><code>ntree</code>: número de árboles que se crecerán; por defecto 500.</p></li>
<li><p><code>mtry</code>: número de predictores seleccionados al azar en cada división; por defecto <code>max(floor(p/3), 1)</code> en el caso de regresión y <code>floor(sqrt(p))</code> en clasificación, siendo <code>p = ncol(x) = ncol(data) - 1</code> el número de predictores.</p></li>
<li><p><code>nodesize</code>: número mínimo de observaciones en un nodo terminal; por defecto 1 en clasificación y 5 en regresión (puede ser recomendable incrementarlo si el conjunto de datos es muy grande, para evitar posibles problemas de sobreajuste, disminuir el tiempo de computación y los requerimientos de memoria; también podría ser considerado como un hiperparámetro).</p></li>
</ul>
<p>Otros argumentos que pueden ser de interés<a href="#fn19" class="footnote-ref" id="fnref19"><sup>19</sup></a> son:</p>
<ul>
<li><p><code>maxnodes</code>: número máximo de nodos terminales (como alternativa para la establecer la complejidad).</p></li>
<li><p><code>importance = TRUE</code>: permite obtener medidas adicionales de importancia.</p></li>
<li><p><code>proximity = TRUE</code>: permite obtener una matriz de proximidades (componente <code>$proximity</code>) entre las observaciones (frecuencia con la que los pares de observaciones están en el mismo nodo terminal).</p></li>
<li><p><code>na.action = na.fail</code>: por defecto no admite datos faltantes con la interfaz de fórmulas. Si los hubiese, se podrían imputar estableciendo <code>na.action = na.roughfix</code> (empleando medias o modas) o llamando previamente a <code>rfImpute()</code> (que emplea proximidades obtenidas con un bosque aleatorio).</p></li>
</ul>
<p>Más detalles en la ayuda de esta función o en <a href="https://www.r-project.org/doc/Rnews/Rnews_2002-3.pdf">Liaw y Wiener (2002)</a>.</p>
<p>Entre las numerosas alternativas, además de las implementadas en paquetes que integran colecciones de métodos como <code>h2o</code> o <code>RWeka</code>, una de las más utilizadas son los bosques aleatorios con <em>conditional inference trees</em>, implementada en la función <code>cforest()</code> del paquete <a href="https://CRAN.R-project.org/package=party"><code>party</code></a>.</p>
<div id="ejemplo-clasificación-con-bagging" class="section level3" number="3.3.1">
<h3><span class="header-section-number">3.3.1</span> Ejemplo: Clasificación con bagging</h3>
<p>Como ejemplo consideraremos el conjunto de datos de calidad de vino empleado en la Sección <a href="cart-con-el-paquete-rpart.html#class-rpart">2.3.2</a> (para hacer comparaciones con el ajuste de un único árbol).</p>
<div class="sourceCode" id="cb145"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb145-1"><a href="bagging-rf-r.html#cb145-1" aria-hidden="true" tabindex="-1"></a><span class="fu">load</span>(<span class="st">&quot;data/winetaste.RData&quot;</span>)</span>
<span id="cb145-2"><a href="bagging-rf-r.html#cb145-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb145-3"><a href="bagging-rf-r.html#cb145-3" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> winetaste</span>
<span id="cb145-4"><a href="bagging-rf-r.html#cb145-4" aria-hidden="true" tabindex="-1"></a>nobs <span class="ot">&lt;-</span> <span class="fu">nrow</span>(df)</span>
<span id="cb145-5"><a href="bagging-rf-r.html#cb145-5" aria-hidden="true" tabindex="-1"></a>itrain <span class="ot">&lt;-</span> <span class="fu">sample</span>(nobs, <span class="fl">0.8</span> <span class="sc">*</span> nobs)</span>
<span id="cb145-6"><a href="bagging-rf-r.html#cb145-6" aria-hidden="true" tabindex="-1"></a>train <span class="ot">&lt;-</span> df[itrain, ]</span>
<span id="cb145-7"><a href="bagging-rf-r.html#cb145-7" aria-hidden="true" tabindex="-1"></a>test <span class="ot">&lt;-</span> df[<span class="sc">-</span>itrain, ]</span></code></pre></div>
<p>Al ser bagging con árboles un caso particular de bosques aleatorios, cuando <span class="math inline">\(m = p\)</span>, también podemos emplear <code>randomForest</code>:</p>
<!-- 
Pendiente: establecer nodesize=5 para reducir tiempos de computación? 
-->
<div class="sourceCode" id="cb146"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb146-1"><a href="bagging-rf-r.html#cb146-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(randomForest)</span>
<span id="cb146-2"><a href="bagging-rf-r.html#cb146-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">4</span>) <span class="co"># NOTA: Fijamos esta semilla para ilustrar dependencia</span></span>
<span id="cb146-3"><a href="bagging-rf-r.html#cb146-3" aria-hidden="true" tabindex="-1"></a>bagtrees <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(taste <span class="sc">~</span> ., <span class="at">data =</span> train, <span class="at">mtry =</span> <span class="fu">ncol</span>(train) <span class="sc">-</span> <span class="dv">1</span>)</span>
<span id="cb146-4"><a href="bagging-rf-r.html#cb146-4" aria-hidden="true" tabindex="-1"></a>bagtrees</span></code></pre></div>
<pre><code>## 
## Call:
##  randomForest(formula = taste ~ ., data = train, mtry = ncol(train) -      1) 
##                Type of random forest: classification
##                      Number of trees: 500
## No. of variables tried at each split: 11
## 
##         OOB estimate of  error rate: 23.5%
## Confusion matrix:
##      good bad class.error
## good  565  97   0.1465257
## bad   138 200   0.4082840</code></pre>
<p>Con el método <code>plot()</code> podemos examinar la convergencia del error en las muestras OOB (simplemente emplea <code>matplot()</code> para representar la componente <code>$err.rate</code> como se muestra en la Figura <a href="bagging-rf-r.html#fig:bagging-conv">3.1</a>):</p>
<div class="sourceCode" id="cb148"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb148-1"><a href="bagging-rf-r.html#cb148-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(bagtrees, <span class="at">main =</span> <span class="st">&quot;&quot;</span>)</span>
<span id="cb148-2"><a href="bagging-rf-r.html#cb148-2" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">&quot;right&quot;</span>, <span class="fu">colnames</span>(bagtrees<span class="sc">$</span>err.rate), <span class="at">lty =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>, <span class="at">col =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">6</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:bagging-conv"></span>
<img src="03-bagging_boosting_files/figure-html/bagging-conv-1.png" alt="Tasas de error OOB al usar bagging para la predicción de `winetaste$taste` (realizado empleando `randomForest()` con `mtry` igual al número de predictores)." width="80%" />
<p class="caption">
Figura 3.1: Tasas de error OOB al usar bagging para la predicción de <code>winetaste$taste</code> (realizado empleando <code>randomForest()</code> con <code>mtry</code> igual al número de predictores).
</p>
</div>
<p>Como vemos que los errores se estabilizan podríamos pensar que aparentemente hay convergencia (aunque situaciones de alta dependencia entre los árboles dificultarían su interpretación).</p>
<p>Con la función <code>getTree()</code> podemos extraer los árboles individuales.
Por ejemplo el siguiente código permite extraer la variable seleccionada para la primera división:</p>
<div class="sourceCode" id="cb149"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb149-1"><a href="bagging-rf-r.html#cb149-1" aria-hidden="true" tabindex="-1"></a><span class="co"># View(getTree(bagtrees, 1, labelVar=TRUE))</span></span>
<span id="cb149-2"><a href="bagging-rf-r.html#cb149-2" aria-hidden="true" tabindex="-1"></a>split_var_1 <span class="ot">&lt;-</span> <span class="fu">sapply</span>(<span class="fu">seq_len</span>(bagtrees<span class="sc">$</span>ntree),</span>
<span id="cb149-3"><a href="bagging-rf-r.html#cb149-3" aria-hidden="true" tabindex="-1"></a>                      <span class="cf">function</span>(i) <span class="fu">getTree</span>(bagtrees, i, <span class="at">labelVar=</span><span class="cn">TRUE</span>)[<span class="dv">1</span>, <span class="st">&quot;split var&quot;</span>])</span></code></pre></div>
<p>En este caso concreto podemos observar que siempre es la misma, lo que indicaría una alta dependencia entre los distintos árboles:</p>
<div class="sourceCode" id="cb150"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb150-1"><a href="bagging-rf-r.html#cb150-1" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(split_var_1)</span></code></pre></div>
<pre><code>## split_var_1
##              alcohol            chlorides          citric.acid 
##                  500                    0                    0 
##              density        fixed.acidity  free.sulfur.dioxide 
##                    0                    0                    0 
##                   pH       residual.sugar            sulphates 
##                    0                    0                    0 
## total.sulfur.dioxide     volatile.acidity 
##                    0                    0</code></pre>
<p>Por último evaluamos la precisión en la muestra de test:</p>
<div class="sourceCode" id="cb152"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb152-1"><a href="bagging-rf-r.html#cb152-1" aria-hidden="true" tabindex="-1"></a>pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(bagtrees, <span class="at">newdata =</span> test)</span>
<span id="cb152-2"><a href="bagging-rf-r.html#cb152-2" aria-hidden="true" tabindex="-1"></a>caret<span class="sc">::</span><span class="fu">confusionMatrix</span>(pred, test<span class="sc">$</span>taste)</span></code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction good bad
##       good  145  42
##       bad    21  42
##                                           
##                Accuracy : 0.748           
##                  95% CI : (0.6894, 0.8006)
##     No Information Rate : 0.664           
##     P-Value [Acc &gt; NIR] : 0.002535        
##                                           
##                   Kappa : 0.3981          
##                                           
##  Mcnemar&#39;s Test P-Value : 0.011743        
##                                           
##             Sensitivity : 0.8735          
##             Specificity : 0.5000          
##          Pos Pred Value : 0.7754          
##          Neg Pred Value : 0.6667          
##              Prevalence : 0.6640          
##          Detection Rate : 0.5800          
##    Detection Prevalence : 0.7480          
##       Balanced Accuracy : 0.6867          
##                                           
##        &#39;Positive&#39; Class : good            
## </code></pre>
</div>
<div id="ejemplo-clasif-rf" class="section level3" number="3.3.2">
<h3><span class="header-section-number">3.3.2</span> Ejemplo: Clasificación con bosques aleatorios</h3>
<p>Continuando con el ejemplo anterior, empleamos la función <code>randomForest()</code> con las opciones por defecto para ajustar un bosque aleatorio:</p>
<div class="sourceCode" id="cb154"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb154-1"><a href="bagging-rf-r.html#cb154-1" aria-hidden="true" tabindex="-1"></a><span class="co"># load(&quot;data/winetaste.RData&quot;)</span></span>
<span id="cb154-2"><a href="bagging-rf-r.html#cb154-2" aria-hidden="true" tabindex="-1"></a><span class="co"># set.seed(1)</span></span>
<span id="cb154-3"><a href="bagging-rf-r.html#cb154-3" aria-hidden="true" tabindex="-1"></a><span class="co"># df &lt;- winetaste</span></span>
<span id="cb154-4"><a href="bagging-rf-r.html#cb154-4" aria-hidden="true" tabindex="-1"></a><span class="co"># nobs &lt;- nrow(df)</span></span>
<span id="cb154-5"><a href="bagging-rf-r.html#cb154-5" aria-hidden="true" tabindex="-1"></a><span class="co"># itrain &lt;- sample(nobs, 0.8 * nobs)</span></span>
<span id="cb154-6"><a href="bagging-rf-r.html#cb154-6" aria-hidden="true" tabindex="-1"></a><span class="co"># train &lt;- df[itrain, ]</span></span>
<span id="cb154-7"><a href="bagging-rf-r.html#cb154-7" aria-hidden="true" tabindex="-1"></a><span class="co"># test &lt;- df[-itrain, ]</span></span>
<span id="cb154-8"><a href="bagging-rf-r.html#cb154-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb154-9"><a href="bagging-rf-r.html#cb154-9" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb154-10"><a href="bagging-rf-r.html#cb154-10" aria-hidden="true" tabindex="-1"></a>rf <span class="ot">&lt;-</span> <span class="fu">randomForest</span>(taste <span class="sc">~</span> ., <span class="at">data =</span> train)</span>
<span id="cb154-11"><a href="bagging-rf-r.html#cb154-11" aria-hidden="true" tabindex="-1"></a>rf</span></code></pre></div>
<pre><code>## 
## Call:
##  randomForest(formula = taste ~ ., data = train) 
##                Type of random forest: classification
##                      Number of trees: 500
## No. of variables tried at each split: 3
## 
##         OOB estimate of  error rate: 22%
## Confusion matrix:
##      good bad class.error
## good  578  84   0.1268882
## bad   136 202   0.4023669</code></pre>
<p>En este caso también observamos en la Figura <a href="bagging-rf-r.html#fig:rf-plot">3.2</a> que aparentemente hay convergencia y tampoco sería necesario incrementar el número de árboles:</p>
<div class="sourceCode" id="cb156"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb156-1"><a href="bagging-rf-r.html#cb156-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(rf,<span class="at">main=</span><span class="st">&quot;&quot;</span>)</span>
<span id="cb156-2"><a href="bagging-rf-r.html#cb156-2" aria-hidden="true" tabindex="-1"></a><span class="fu">legend</span>(<span class="st">&quot;right&quot;</span>, <span class="fu">colnames</span>(rf<span class="sc">$</span>err.rate), <span class="at">lty =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>, <span class="at">col =</span> <span class="dv">1</span><span class="sc">:</span><span class="dv">6</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:rf-plot"></span>
<img src="03-bagging_boosting_files/figure-html/rf-plot-1.png" alt="Tasas de error OOB al usar bosques aleatorios para la predicción de `winetaste$taste` (empleando `randomForest()` con las opciones por defecto)." width="80%" />
<p class="caption">
Figura 3.2: Tasas de error OOB al usar bosques aleatorios para la predicción de <code>winetaste$taste</code> (empleando <code>randomForest()</code> con las opciones por defecto).
</p>
</div>
<p>Podemos mostrar la importancia de las variables predictoras (utilizadas en el bosque aleatorio y sus sustituas) con la función <code>importance()</code> o representarlas con <code>varImpPlot()</code> (ver Figura <a href="bagging-rf-r.html#fig:rf-importance">3.3</a>):</p>
<div class="sourceCode" id="cb157"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb157-1"><a href="bagging-rf-r.html#cb157-1" aria-hidden="true" tabindex="-1"></a><span class="fu">importance</span>(rf)</span></code></pre></div>
<pre><code>##                      MeanDecreaseGini
## fixed.acidity                37.77155
## volatile.acidity             43.99769
## citric.acid                  41.50069
## residual.sugar               36.79932
## chlorides                    33.62100
## free.sulfur.dioxide          42.29122
## total.sulfur.dioxide         39.63738
## density                      45.38724
## pH                           32.31442
## sulphates                    30.32322
## alcohol                      63.89185</code></pre>
<div class="sourceCode" id="cb159"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb159-1"><a href="bagging-rf-r.html#cb159-1" aria-hidden="true" tabindex="-1"></a><span class="fu">varImpPlot</span>(rf)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:rf-importance"></span>
<img src="03-bagging_boosting_files/figure-html/rf-importance-1.png" alt="Importancia de las variables predictoras al emplear bosques aleatorios para la predicción de `winetaste$taste`." width="80%" />
<p class="caption">
Figura 3.3: Importancia de las variables predictoras al emplear bosques aleatorios para la predicción de <code>winetaste$taste</code>.
</p>
</div>
<p>Si evaluamos la precisión en la muestra de test podemos observar un ligero incremento en la precisión en comparación con el método anterior:</p>
<div class="sourceCode" id="cb160"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb160-1"><a href="bagging-rf-r.html#cb160-1" aria-hidden="true" tabindex="-1"></a>pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(rf, <span class="at">newdata =</span> test)</span>
<span id="cb160-2"><a href="bagging-rf-r.html#cb160-2" aria-hidden="true" tabindex="-1"></a>caret<span class="sc">::</span><span class="fu">confusionMatrix</span>(pred, test<span class="sc">$</span>taste)</span></code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction good bad
##       good  153  43
##       bad    13  41
##                                           
##                Accuracy : 0.776           
##                  95% CI : (0.7192, 0.8261)
##     No Information Rate : 0.664           
##     P-Value [Acc &gt; NIR] : 7.227e-05       
##                                           
##                   Kappa : 0.4494          
##                                           
##  Mcnemar&#39;s Test P-Value : 0.0001065       
##                                           
##             Sensitivity : 0.9217          
##             Specificity : 0.4881          
##          Pos Pred Value : 0.7806          
##          Neg Pred Value : 0.7593          
##              Prevalence : 0.6640          
##          Detection Rate : 0.6120          
##    Detection Prevalence : 0.7840          
##       Balanced Accuracy : 0.7049          
##                                           
##        &#39;Positive&#39; Class : good            
## </code></pre>
<p>Esta mejora sería debida a que en este caso la dependencia entre los árboles es menor:</p>
<div class="sourceCode" id="cb162"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb162-1"><a href="bagging-rf-r.html#cb162-1" aria-hidden="true" tabindex="-1"></a>split_var_1 <span class="ot">&lt;-</span> <span class="fu">sapply</span>(<span class="fu">seq_len</span>(rf<span class="sc">$</span>ntree),</span>
<span id="cb162-2"><a href="bagging-rf-r.html#cb162-2" aria-hidden="true" tabindex="-1"></a>                      <span class="cf">function</span>(i) <span class="fu">getTree</span>(rf, i, <span class="at">labelVar=</span><span class="cn">TRUE</span>)[<span class="dv">1</span>, <span class="st">&quot;split var&quot;</span>])</span>
<span id="cb162-3"><a href="bagging-rf-r.html#cb162-3" aria-hidden="true" tabindex="-1"></a><span class="fu">table</span>(split_var_1)</span></code></pre></div>
<pre><code>## split_var_1
##              alcohol            chlorides          citric.acid 
##                  150                   49                   38 
##              density        fixed.acidity  free.sulfur.dioxide 
##                  114                   23                   20 
##                   pH       residual.sugar            sulphates 
##                   11                    0                    5 
## total.sulfur.dioxide     volatile.acidity 
##                   49                   41</code></pre>
<p>El análisis e interpretación del modelo puede resultar más complicado en este tipo de métodos.
Para estudiar el efecto de los predictores en la respuesta se suelen emplear alguna de las herramientas descritas en la Sección <a href="analisis-modelos.html#analisis-modelos">1.5</a>.
Por ejemplo, empleando la función <code>pdp::partial()</code>, podemos generar gráficos PDP estimando los efectos individuales de los predictores (ver Figura <a href="bagging-rf-r.html#fig:rf-pdp-uni-plot">3.4</a>):</p>
<div class="sourceCode" id="cb164"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb164-1"><a href="bagging-rf-r.html#cb164-1" aria-hidden="true" tabindex="-1"></a><span class="co"># install.packages(&quot;pdp&quot;)</span></span>
<span id="cb164-2"><a href="bagging-rf-r.html#cb164-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(pdp)</span>
<span id="cb164-3"><a href="bagging-rf-r.html#cb164-3" aria-hidden="true" tabindex="-1"></a>pdp1 <span class="ot">&lt;-</span> <span class="fu">partial</span>(rf, <span class="st">&quot;alcohol&quot;</span>)</span>
<span id="cb164-4"><a href="bagging-rf-r.html#cb164-4" aria-hidden="true" tabindex="-1"></a>p1 <span class="ot">&lt;-</span> <span class="fu">plotPartial</span>(pdp1)</span>
<span id="cb164-5"><a href="bagging-rf-r.html#cb164-5" aria-hidden="true" tabindex="-1"></a>pdp2 <span class="ot">&lt;-</span> <span class="fu">partial</span>(rf, <span class="fu">c</span>(<span class="st">&quot;density&quot;</span>))</span>
<span id="cb164-6"><a href="bagging-rf-r.html#cb164-6" aria-hidden="true" tabindex="-1"></a>p2 <span class="ot">&lt;-</span> <span class="fu">plotPartial</span>(pdp2)</span>
<span id="cb164-7"><a href="bagging-rf-r.html#cb164-7" aria-hidden="true" tabindex="-1"></a><span class="fu">grid.arrange</span>(p1, p2, <span class="at">ncol =</span> <span class="dv">2</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:rf-pdp-uni-plot"></span>
<img src="images/rf-pdp-uni-1.png" alt="Efecto parcial del alcochol (panel izquierdo) y la densidad (panel derecho) sobre la respuesta." width="90%" />
<p class="caption">
Figura 3.4: Efecto parcial del alcochol (panel izquierdo) y la densidad (panel derecho) sobre la respuesta.
</p>
</div>
<p>O gráficos PDP considerando la interacción entre dos predictores (ver Figura <a href="bagging-rf-r.html#fig:rf-pdp-plot">3.5</a>) (cuidado, puede requerir de mucho tiempo de computación):</p>
<div class="sourceCode" id="cb165"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb165-1"><a href="bagging-rf-r.html#cb165-1" aria-hidden="true" tabindex="-1"></a>pdp12 <span class="ot">&lt;-</span> <span class="fu">partial</span>(rf, <span class="fu">c</span>(<span class="st">&quot;alcohol&quot;</span>, <span class="st">&quot;density&quot;</span>))</span>
<span id="cb165-2"><a href="bagging-rf-r.html#cb165-2" aria-hidden="true" tabindex="-1"></a><span class="fu">plotPartial</span>(pdp12)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:rf-pdp-plot"></span>
<img src="images/rf-pdp-1.png" alt="Efecto parcial de la interacción del alcochol y la densidad sobre la respuesta." width="80%" />
<p class="caption">
Figura 3.5: Efecto parcial de la interacción del alcochol y la densidad sobre la respuesta.
</p>
</div>
<p>Adicionalmente, estableciendo <code>ice = TRUE</code> se calculan las curvas de expectativa condicional individual (ICE). Estos gráficos ICE extienden los PDP, ya que además de mostrar la variación del promedio (ver línea roja en la Figura <a href="bagging-rf-r.html#fig:rf-ice-plot">3.6</a>), también muestra la variación de los valores predichos para cada observación (ver líneas en negro en la Figura <a href="bagging-rf-r.html#fig:rf-ice-plot">3.6</a>).</p>
<div class="sourceCode" id="cb166"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb166-1"><a href="bagging-rf-r.html#cb166-1" aria-hidden="true" tabindex="-1"></a>ice1 <span class="ot">&lt;-</span> <span class="fu">partial</span>(rf, <span class="at">pred.var =</span> <span class="st">&quot;alcohol&quot;</span>, <span class="at">ice =</span> <span class="cn">TRUE</span>)</span>
<span id="cb166-2"><a href="bagging-rf-r.html#cb166-2" aria-hidden="true" tabindex="-1"></a>ice2 <span class="ot">&lt;-</span> <span class="fu">partial</span>(rf, <span class="at">pred.var =</span> <span class="st">&quot;density&quot;</span>, <span class="at">ice =</span> <span class="cn">TRUE</span>)</span>
<span id="cb166-3"><a href="bagging-rf-r.html#cb166-3" aria-hidden="true" tabindex="-1"></a>p1 <span class="ot">&lt;-</span> <span class="fu">plotPartial</span>(ice1, <span class="at">alpha =</span> <span class="fl">0.5</span>)</span>
<span id="cb166-4"><a href="bagging-rf-r.html#cb166-4" aria-hidden="true" tabindex="-1"></a>p2 <span class="ot">&lt;-</span> <span class="fu">plotPartial</span>(ice2, <span class="at">alpha =</span> <span class="fl">0.5</span>)</span>
<span id="cb166-5"><a href="bagging-rf-r.html#cb166-5" aria-hidden="true" tabindex="-1"></a><span class="fu">grid.arrange</span>(p1, p2, <span class="at">ncol =</span> <span class="dv">2</span>)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:rf-ice-plot"></span>
<img src="images/rf-ice-1.png" alt="Efecto individual de cada observación de alcochol (panel izquierdo) y densidad (panel derecho) sobre la respuesta." width="80%" />
<p class="caption">
Figura 3.6: Efecto individual de cada observación de alcochol (panel izquierdo) y densidad (panel derecho) sobre la respuesta.
</p>
</div>
<p>Gráficos similares pueden crearse utilizando otros paquetes indicados en la Sección <a href="analisis-modelos.html#analisis-modelos">1.5</a>. En particular, el paquete <code>vivid</code> muestra en la diagonal del Figura <a href="bagging-rf-r.html#fig:rf-vivid-plot">3.7</a> la importancia de los cinco primeros predictores (<em>Vimp</em>) y fuera de la diagonal las interacciones 2 a 2 (<em>Vint</em>).</p>
<div class="sourceCode" id="cb167"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb167-1"><a href="bagging-rf-r.html#cb167-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(vivid)</span>
<span id="cb167-2"><a href="bagging-rf-r.html#cb167-2" aria-hidden="true" tabindex="-1"></a>fit_rf  <span class="ot">&lt;-</span> <span class="fu">vivi</span>(<span class="at">data =</span> train, <span class="at">fit =</span> rf, <span class="at">response =</span> <span class="st">&quot;taste&quot;</span>, <span class="at">importanceType =</span> <span class="st">&quot;%IncMSE&quot;</span>)</span>
<span id="cb167-3"><a href="bagging-rf-r.html#cb167-3" aria-hidden="true" tabindex="-1"></a><span class="fu">viviHeatmap</span>(<span class="at">mat =</span> fit_rf[<span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>,<span class="dv">1</span><span class="sc">:</span><span class="dv">5</span>])</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:rf-vivid-plot"></span>
<img src="images/rf-vivid-1.png" alt="Mapa de calor para la importancia e interaciones del ajuste de un bosque aleatorio usando vivid." width="80%" />
<p class="caption">
Figura 3.7: Mapa de calor para la importancia e interaciones del ajuste de un bosque aleatorio usando vivid.
</p>
</div>
<p>Alternativamente, también se pueden visualizar las relaciones mediante un gráfico de red (ver Figura <a href="bagging-rf-r.html#fig:rf-vivid-plot">3.7</a>).</p>
<div class="sourceCode" id="cb168"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb168-1"><a href="bagging-rf-r.html#cb168-1" aria-hidden="true" tabindex="-1"></a><span class="fu">require</span>(igraph)</span>
<span id="cb168-2"><a href="bagging-rf-r.html#cb168-2" aria-hidden="true" tabindex="-1"></a><span class="fu">viviNetwork</span>(<span class="at">mat =</span> fit_rf)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:rf-vivid2-plot"></span>
<img src="images/rf-vivid2-1.png" alt="Gráfico de red para la importancia e interaciones del ajuste de un bosque aleatorio usando vivid." width="80%" />
<p class="caption">
Figura 3.8: Gráfico de red para la importancia e interaciones del ajuste de un bosque aleatorio usando vivid.
</p>
</div>
<!-- 
Pendiente: 
En este caso también puede ser de utilidad el paquete [`randomForestExplainer`](https://modeloriented.github.io/randomForestExplainer), 
Pendiente: Análisis e interpretación del modelo
# install.packages("randomForestExplainer")
library(randomForestExplainer)
plot_min_depth_distribution(rf)
plot_min_depth_interactions(rf, k = 5) # solo 5 mejores iteraciones
-->
</div>
<div id="ejemplo-bosques-aleatorios-con-caret" class="section level3" number="3.3.3">
<h3><span class="header-section-number">3.3.3</span> Ejemplo: bosques aleatorios con <code>caret</code></h3>
<p>En paquete <code>caret</code> hay varias implementaciones de bagging y bosques aleatorios<a href="#fn20" class="footnote-ref" id="fnref20"><sup>20</sup></a>, incluyendo el algoritmo del paquete <code>randomForest</code> considerando como hiperparámetro el número de predictores seleccionados al azar en cada división <code>mtry</code>.
Para ajustar este modelo a una muestra de entrenamiento hay que establecer <code>method = "rf"</code> en la llamada a <code>train()</code>.</p>
<div class="sourceCode" id="cb169"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb169-1"><a href="bagging-rf-r.html#cb169-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(caret)</span>
<span id="cb169-2"><a href="bagging-rf-r.html#cb169-2" aria-hidden="true" tabindex="-1"></a><span class="co"># str(getModelInfo(&quot;rf&quot;, regex = FALSE))</span></span>
<span id="cb169-3"><a href="bagging-rf-r.html#cb169-3" aria-hidden="true" tabindex="-1"></a><span class="fu">modelLookup</span>(<span class="st">&quot;rf&quot;</span>)</span></code></pre></div>
<pre><code>##   model parameter                         label forReg forClass probModel
## 1    rf      mtry #Randomly Selected Predictors   TRUE     TRUE      TRUE</code></pre>
<div class="sourceCode" id="cb171"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb171-1"><a href="bagging-rf-r.html#cb171-1" aria-hidden="true" tabindex="-1"></a><span class="co"># load(&quot;data/winetaste.RData&quot;)</span></span>
<span id="cb171-2"><a href="bagging-rf-r.html#cb171-2" aria-hidden="true" tabindex="-1"></a><span class="co"># set.seed(1)</span></span>
<span id="cb171-3"><a href="bagging-rf-r.html#cb171-3" aria-hidden="true" tabindex="-1"></a><span class="co"># df &lt;- winetaste</span></span>
<span id="cb171-4"><a href="bagging-rf-r.html#cb171-4" aria-hidden="true" tabindex="-1"></a><span class="co"># nobs &lt;- nrow(df)</span></span>
<span id="cb171-5"><a href="bagging-rf-r.html#cb171-5" aria-hidden="true" tabindex="-1"></a><span class="co"># itrain &lt;- sample(nobs, 0.8 * nobs)</span></span>
<span id="cb171-6"><a href="bagging-rf-r.html#cb171-6" aria-hidden="true" tabindex="-1"></a><span class="co"># train &lt;- df[itrain, ]</span></span>
<span id="cb171-7"><a href="bagging-rf-r.html#cb171-7" aria-hidden="true" tabindex="-1"></a><span class="co"># test &lt;- df[-itrain, ]</span></span></code></pre></div>
<p>Con las opciones por defecto únicamente evalúa tres valores posibles del hiperparámetro (ver Figura <a href="bagging-rf-r.html#fig:rf-caret-train">3.9</a>).
Opcionalmente se podría aumentar el número valores a evaluar con <code>tuneLength</code> o directamente especificarlos con <code>tuneGrid</code>.
En cualquier caso el tiempo de computación puede ser demasiado alto, por lo que puede ser recomendable reducir el valor de <code>nodesize</code>, paralelizar los cálculos o emplear otros paquetes con implementaciones más eficientes.</p>
<div class="sourceCode" id="cb172"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb172-1"><a href="bagging-rf-r.html#cb172-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb172-2"><a href="bagging-rf-r.html#cb172-2" aria-hidden="true" tabindex="-1"></a>rf.caret <span class="ot">&lt;-</span> <span class="fu">train</span>(taste <span class="sc">~</span> ., <span class="at">data =</span> train, <span class="at">method =</span> <span class="st">&quot;rf&quot;</span>)</span>
<span id="cb172-3"><a href="bagging-rf-r.html#cb172-3" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(rf.caret)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:rf-caret-train"></span>
<img src="03-bagging_boosting_files/figure-html/rf-caret-train-1.png" alt="Evolución de la precisión de un bosque aleatorio dependiendo del número de predictores seleccionados." width="80%" />
<p class="caption">
Figura 3.9: Evolución de la precisión de un bosque aleatorio dependiendo del número de predictores seleccionados.
</p>
</div>
<p><span class="citation"><a href="#ref-breiman2001random" role="doc-biblioref">Breiman</a> (<a href="#ref-breiman2001random" role="doc-biblioref">2001a</a>)</span> sugiere emplear el valor por defecto para <code>mtry</code>, la mitad y el doble (ver Figura <a href="bagging-rf-r.html#fig:rf-caret-grid">3.10</a>):</p>
<div class="sourceCode" id="cb173"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb173-1"><a href="bagging-rf-r.html#cb173-1" aria-hidden="true" tabindex="-1"></a>mtry.class <span class="ot">&lt;-</span> <span class="fu">sqrt</span>(<span class="fu">ncol</span>(train) <span class="sc">-</span> <span class="dv">1</span>)</span>
<span id="cb173-2"><a href="bagging-rf-r.html#cb173-2" aria-hidden="true" tabindex="-1"></a>tuneGrid <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">mtry =</span> <span class="fu">floor</span>(<span class="fu">c</span>(mtry.class<span class="sc">/</span><span class="dv">2</span>, mtry.class, <span class="dv">2</span><span class="sc">*</span>mtry.class)))</span>
<span id="cb173-3"><a href="bagging-rf-r.html#cb173-3" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb173-4"><a href="bagging-rf-r.html#cb173-4" aria-hidden="true" tabindex="-1"></a>rf.caret <span class="ot">&lt;-</span> <span class="fu">train</span>(taste <span class="sc">~</span> ., <span class="at">data =</span> train,</span>
<span id="cb173-5"><a href="bagging-rf-r.html#cb173-5" aria-hidden="true" tabindex="-1"></a>                  <span class="at">method =</span> <span class="st">&quot;rf&quot;</span>, <span class="at">tuneGrid =</span> tuneGrid)</span>
<span id="cb173-6"><a href="bagging-rf-r.html#cb173-6" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(rf.caret)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:rf-caret-grid"></span>
<img src="03-bagging_boosting_files/figure-html/rf-caret-grid-1.png" alt="Evolución de la precisión de un bosque aleatorio con `caret` usando el argumento `tuneGrid`." width="80%" />
<p class="caption">
Figura 3.10: Evolución de la precisión de un bosque aleatorio con <code>caret</code> usando el argumento <code>tuneGrid</code>.
</p>
</div>
<!-- 
Pendiente: 
crear un método "rf2" en `caret` que incluya `nodesize` como hiperparámetro (para evitar posibles problemas de sobreajuste, disminuir el tiempo de computación en la evaluación y los requerimientos de memoria cuando el conjunto de datos es muy grande). Puede ser más cómodo hacerlo al margen de `caret`... 
-->

<div class="exercise">
<span id="exr:rf-tunegrid" class="exercise"><strong>Ejercicio 3.1  </strong></span>
</div>
<p>Como acabamos de ver, <code>caret</code> permite ajustar un bosque aleatorio considerando <code>mtry</code> como único hiperparámetro, pero nos podría interesar buscar también valores adecuados para otros parámetros, como por ejemplo <code>nodesize</code>.
Esto se puede realizar fácilmente empleando directamente la función <code>randomForest()</code>.
En primer lugar habría que construir la rejilla de búsqueda, con las combinaciones de los valores de los hiperparámetros que se quieren evaluar (para ello se puede utilizar la función <code>expand.grid()</code>).
Posteriormente se ajustaría un bosque aleatorio en la muestra de entrenamiento con cada una de las combinaciones (por ejemplo utilizando un bucle <code>for</code>) y se emplearía el error OOB para seleccionar la combinación óptima (al que podemos acceder empleando <code>with(fit, err.rate[ntree, "OOB"])</code> suponiendo que <code>fit</code> contiene el bosque aleatorio ajustado).</p>
<p>Continuando con el mismo conjunto de datos de calidad de vino, emplear la función <code>randomForest()</code> para ajustar un bosque aleatorio con el fin de clasificar la calidad del vino <code>taste</code>, considerando 500 árboles y empleando el error OOB para seleccionar los valores “óptimos” de los hiperparámetros considerando las posibles combinaciones de <code>mtry = floor(c(mtry.class/2, mtry.class, 2*mtry.class))</code> (siendo <code>mtry.class &lt;- sqrt(ncol(train) - 1)</code>) y <code>nodesize = c(1, 3, 5, 10)</code>.</p>
</div>
</div>
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent" line-spacing="2">
<div id="ref-breiman2001random" class="csl-entry">
Breiman, L. (2001a). Random forests. <em>Machine Learning</em>, <em>45</em>(1), 5-32.
</div>
</div>
<div class="footnotes">
<hr />
<ol start="19">
<li id="fn19"><p>Si se quiere minimizar el uso de memoria, por ejemplo mientras se seleccionan hiperparámetros, se puede establecer <code>keep.forest=FALSE</code>.<a href="bagging-rf-r.html#fnref19" class="footnote-back">↩︎</a></p></li>
<li id="fn20"><p>Se puede hacer una búsqueda en la tabla del <a href="https://topepo.github.io/caret/available-models.html">Capítulo 6: Available Models</a> del manual.<a href="bagging-rf-r.html#fnref20" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="bosques-aleatorios.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="boosting.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rubenfcasal/aprendizaje_estadistico/edit/master/03-bagging_boosting.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["aprendizaje_estadistico.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
