<!DOCTYPE html>
<html lang="es" xml:lang="es">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>7.5 Projection pursuit | Métodos predictivos de aprendizaje estadístico</title>
  <meta name="description" content="7.5 Projection pursuit | Métodos predictivos de aprendizaje estadístico con R." />
  <meta name="generator" content="bookdown 0.36 and GitBook 2.6.7" />

  <meta property="og:title" content="7.5 Projection pursuit | Métodos predictivos de aprendizaje estadístico" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="7.5 Projection pursuit | Métodos predictivos de aprendizaje estadístico con R." />
  <meta name="github-repo" content="rubenfcasal/aprendizaje_estadistico" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="7.5 Projection pursuit | Métodos predictivos de aprendizaje estadístico" />
  
  <meta name="twitter:description" content="7.5 Projection pursuit | Métodos predictivos de aprendizaje estadístico con R." />
  

<meta name="author" content="Rubén Fernández Casal (ruben.fcasal@udc.es)" />
<meta name="author" content="Julián Costa Bouzas (julian.costa@udc.es)" />
<meta name="author" content="Manuel Oviedo de la Fuente (manuel.oviedo@udc.es)" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="mars.html"/>
<link rel="next" href="neural-nets.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/htmlwidgets-1.6.2/htmlwidgets.js"></script>
<link href="libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet" />
<script src="libs/datatables-binding-0.30/datatables.js"></script>
<link href="libs/dt-core-1.13.4/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.13.4/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.13.4/js/jquery.dataTables.min.js"></script>
<link href="libs/crosstalk-1.2.0/css/crosstalk.min.css" rel="stylesheet" />
<script src="libs/crosstalk-1.2.0/js/crosstalk.min.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Métodos predictivos de aprendizaje estadístico</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Bienvenida</a></li>
<li class="chapter" data-level="" data-path="prólogo.html"><a href="prólogo.html"><i class="fa fa-check"></i>Prólogo</a>
<ul>
<li class="chapter" data-level="" data-path="el-lenguaje-de-programación-r.html"><a href="el-lenguaje-de-programación-r.html"><i class="fa fa-check"></i>El lenguaje de programación R</a></li>
<li class="chapter" data-level="" data-path="organización.html"><a href="organización.html"><i class="fa fa-check"></i>Organización</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="intro-AE.html"><a href="intro-AE.html"><i class="fa fa-check"></i><b>1</b> Introducción al aprendizaje estadístico</a>
<ul>
<li class="chapter" data-level="1.1" data-path="aprendizaje-estadístico-vs.-aprendizaje-automático.html"><a href="aprendizaje-estadístico-vs.-aprendizaje-automático.html"><i class="fa fa-check"></i><b>1.1</b> Aprendizaje estadístico vs. aprendizaje automático</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="aprendizaje-estadístico-vs.-aprendizaje-automático.html"><a href="aprendizaje-estadístico-vs.-aprendizaje-automático.html#las-dos-culturas"><i class="fa fa-check"></i><b>1.1.1</b> Las dos culturas</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="métodos-de-aprendizaje-estadístico.html"><a href="métodos-de-aprendizaje-estadístico.html"><i class="fa fa-check"></i><b>1.2</b> Métodos de aprendizaje estadístico</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="métodos-de-aprendizaje-estadístico.html"><a href="métodos-de-aprendizaje-estadístico.html#notacion"><i class="fa fa-check"></i><b>1.2.1</b> Notación y terminología</a></li>
<li class="chapter" data-level="1.2.2" data-path="métodos-de-aprendizaje-estadístico.html"><a href="métodos-de-aprendizaje-estadístico.html#metodos-pkgs"><i class="fa fa-check"></i><b>1.2.2</b> Métodos (de aprendizaje supervisado) y paquetes de R</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="const-eval.html"><a href="const-eval.html"><i class="fa fa-check"></i><b>1.3</b> Construcción y evaluación de los modelos</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="const-eval.html"><a href="const-eval.html#bias-variance"><i class="fa fa-check"></i><b>1.3.1</b> Equilibrio entre sesgo y varianza: infraajuste y sobreajuste</a></li>
<li class="chapter" data-level="1.3.2" data-path="const-eval.html"><a href="const-eval.html#entrenamiento-test"><i class="fa fa-check"></i><b>1.3.2</b> Datos de entrenamiento y datos de test</a></li>
<li class="chapter" data-level="1.3.3" data-path="const-eval.html"><a href="const-eval.html#cv"><i class="fa fa-check"></i><b>1.3.3</b> Selección de hiperparámetros mediante validación cruzada</a></li>
<li class="chapter" data-level="1.3.4" data-path="const-eval.html"><a href="const-eval.html#eval-reg"><i class="fa fa-check"></i><b>1.3.4</b> Evaluación de un método de regresión</a></li>
<li class="chapter" data-level="1.3.5" data-path="const-eval.html"><a href="const-eval.html#eval-class"><i class="fa fa-check"></i><b>1.3.5</b> Evaluación de un método de clasificación</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="dimen-curse.html"><a href="dimen-curse.html"><i class="fa fa-check"></i><b>1.4</b> La maldición de la dimensionalidad</a></li>
<li class="chapter" data-level="1.5" data-path="analisis-modelos.html"><a href="analisis-modelos.html"><i class="fa fa-check"></i><b>1.5</b> Análisis e interpretación de los modelos</a></li>
<li class="chapter" data-level="1.6" data-path="caret.html"><a href="caret.html"><i class="fa fa-check"></i><b>1.6</b> Introducción al paquete <code>caret</code></a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="clasicos.html"><a href="clasicos.html"><i class="fa fa-check"></i><b>2</b> Métodos clásicos de estadística</a>
<ul>
<li class="chapter" data-level="2.1" data-path="rlm.html"><a href="rlm.html"><i class="fa fa-check"></i><b>2.1</b> Regresión lineal múltiple</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="rlm.html"><a href="rlm.html#colinealidad"><i class="fa fa-check"></i><b>2.1.1</b> El problema de la colinealidad</a></li>
<li class="chapter" data-level="2.1.2" data-path="rlm.html"><a href="rlm.html#seleccion-rlm"><i class="fa fa-check"></i><b>2.1.2</b> Selección de variables explicativas</a></li>
<li class="chapter" data-level="2.1.3" data-path="rlm.html"><a href="rlm.html#analisis-rlm"><i class="fa fa-check"></i><b>2.1.3</b> Análisis e interpretación del modelo</a></li>
<li class="chapter" data-level="2.1.4" data-path="rlm.html"><a href="rlm.html#eval-rlm"><i class="fa fa-check"></i><b>2.1.4</b> Evaluación de la precisión</a></li>
<li class="chapter" data-level="2.1.5" data-path="rlm.html"><a href="rlm.html#selec-ae-rlm"><i class="fa fa-check"></i><b>2.1.5</b> Selección del modelo mediante remuestreo</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="reg-glm.html"><a href="reg-glm.html"><i class="fa fa-check"></i><b>2.2</b> Modelos lineales generalizados</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="reg-glm.html"><a href="reg-glm.html#seleccion-glm"><i class="fa fa-check"></i><b>2.2.1</b> Selección de variables explicativas</a></li>
<li class="chapter" data-level="2.2.2" data-path="reg-glm.html"><a href="reg-glm.html#analisis-glm"><i class="fa fa-check"></i><b>2.2.2</b> Análisis e interpretación del modelo</a></li>
<li class="chapter" data-level="2.2.3" data-path="reg-glm.html"><a href="reg-glm.html#glm-bfan"><i class="fa fa-check"></i><b>2.2.3</b> Evaluación de la precisión</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="generadores.html"><a href="generadores.html"><i class="fa fa-check"></i><b>2.3</b> Otros métodos de clasificación</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="generadores.html"><a href="generadores.html#clas-lda"><i class="fa fa-check"></i><b>2.3.1</b> Análisis discriminante lineal</a></li>
<li class="chapter" data-level="2.3.2" data-path="generadores.html"><a href="generadores.html#clas-qda"><i class="fa fa-check"></i><b>2.3.2</b> Análisis discriminante cuadrático</a></li>
<li class="chapter" data-level="2.3.3" data-path="generadores.html"><a href="generadores.html#bayes"><i class="fa fa-check"></i><b>2.3.3</b> Bayes naíf</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="trees.html"><a href="trees.html"><i class="fa fa-check"></i><b>3</b> Árboles de decisión</a>
<ul>
<li class="chapter" data-level="3.1" data-path="árboles-de-regresión-cart.html"><a href="árboles-de-regresión-cart.html"><i class="fa fa-check"></i><b>3.1</b> Árboles de regresión CART</a></li>
<li class="chapter" data-level="3.2" data-path="árboles-de-clasificación-cart.html"><a href="árboles-de-clasificación-cart.html"><i class="fa fa-check"></i><b>3.2</b> Árboles de clasificación CART</a></li>
<li class="chapter" data-level="3.3" data-path="tree-rpart.html"><a href="tree-rpart.html"><i class="fa fa-check"></i><b>3.3</b> CART con el paquete <code>rpart</code></a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="tree-rpart.html"><a href="tree-rpart.html#reg-rpart"><i class="fa fa-check"></i><b>3.3.1</b> Ejemplo: regresión</a></li>
<li class="chapter" data-level="3.3.2" data-path="tree-rpart.html"><a href="tree-rpart.html#class-rpart"><i class="fa fa-check"></i><b>3.3.2</b> Ejemplo: modelo de clasificación</a></li>
<li class="chapter" data-level="3.3.3" data-path="tree-rpart.html"><a href="tree-rpart.html#interfaz-de-caret"><i class="fa fa-check"></i><b>3.3.3</b> Interfaz de <code>caret</code></a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="alternativas-a-los-árboles-cart.html"><a href="alternativas-a-los-árboles-cart.html"><i class="fa fa-check"></i><b>3.4</b> Alternativas a los árboles CART</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="bagging-boosting.html"><a href="bagging-boosting.html"><i class="fa fa-check"></i><b>4</b> Bagging y boosting</a>
<ul>
<li class="chapter" data-level="4.1" data-path="bagging.html"><a href="bagging.html"><i class="fa fa-check"></i><b>4.1</b> Bagging</a></li>
<li class="chapter" data-level="4.2" data-path="rf.html"><a href="rf.html"><i class="fa fa-check"></i><b>4.2</b> Bosques aleatorios</a></li>
<li class="chapter" data-level="4.3" data-path="bagging-rf-r.html"><a href="bagging-rf-r.html"><i class="fa fa-check"></i><b>4.3</b> Bagging y bosques aleatorios en R</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="bagging-rf-r.html"><a href="bagging-rf-r.html#ejemplo-clasificación-con-bagging"><i class="fa fa-check"></i><b>4.3.1</b> Ejemplo: clasificación con bagging</a></li>
<li class="chapter" data-level="4.3.2" data-path="bagging-rf-r.html"><a href="bagging-rf-r.html#ejemplo-clasif-rf"><i class="fa fa-check"></i><b>4.3.2</b> Ejemplo: clasificación con bosques aleatorios</a></li>
<li class="chapter" data-level="4.3.3" data-path="bagging-rf-r.html"><a href="bagging-rf-r.html#ejemplo-bosques-aleatorios-con-caret"><i class="fa fa-check"></i><b>4.3.3</b> Ejemplo: bosques aleatorios con <code>caret</code></a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="boosting.html"><a href="boosting.html"><i class="fa fa-check"></i><b>4.4</b> Boosting</a></li>
<li class="chapter" data-level="4.5" data-path="boosting-r.html"><a href="boosting-r.html"><i class="fa fa-check"></i><b>4.5</b> Boosting en R</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="boosting-r.html"><a href="boosting-r.html#ejemplo-clasificación-con-el-paquete-ada"><i class="fa fa-check"></i><b>4.5.1</b> Ejemplo: clasificación con el paquete <code>ada</code></a></li>
<li class="chapter" data-level="4.5.2" data-path="boosting-r.html"><a href="boosting-r.html#ejemplo-regresión-con-el-paquete-gbm"><i class="fa fa-check"></i><b>4.5.2</b> Ejemplo: regresión con el paquete <code>gbm</code></a></li>
<li class="chapter" data-level="4.5.3" data-path="boosting-r.html"><a href="boosting-r.html#xgb-caret"><i class="fa fa-check"></i><b>4.5.3</b> Ejemplo: XGBoost con el paquete <code>caret</code></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="svm.html"><a href="svm.html"><i class="fa fa-check"></i><b>5</b> Máquinas de soporte vectorial</a>
<ul>
<li class="chapter" data-level="5.1" data-path="clasificadores-de-máximo-margen.html"><a href="clasificadores-de-máximo-margen.html"><i class="fa fa-check"></i><b>5.1</b> Clasificadores de máximo margen</a></li>
<li class="chapter" data-level="5.2" data-path="clasificadores-de-soporte-vectorial.html"><a href="clasificadores-de-soporte-vectorial.html"><i class="fa fa-check"></i><b>5.2</b> Clasificadores de soporte vectorial</a></li>
<li class="chapter" data-level="5.3" data-path="máquinas-de-soporte-vectorial.html"><a href="máquinas-de-soporte-vectorial.html"><i class="fa fa-check"></i><b>5.3</b> Máquinas de soporte vectorial</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="máquinas-de-soporte-vectorial.html"><a href="máquinas-de-soporte-vectorial.html#regresión-con-svm"><i class="fa fa-check"></i><b>5.3.1</b> Regresión con SVM</a></li>
<li class="chapter" data-level="5.3.2" data-path="máquinas-de-soporte-vectorial.html"><a href="máquinas-de-soporte-vectorial.html#ventajas-e-incovenientes"><i class="fa fa-check"></i><b>5.3.2</b> Ventajas e incovenientes</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="svm-kernlab.html"><a href="svm-kernlab.html"><i class="fa fa-check"></i><b>5.4</b> SVM en R</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="ext-glm.html"><a href="ext-glm.html"><i class="fa fa-check"></i><b>6</b> Extensiones de los modelos lineales (generalizados)</a>
<ul>
<li class="chapter" data-level="6.1" data-path="shrinkage.html"><a href="shrinkage.html"><i class="fa fa-check"></i><b>6.1</b> Métodos de regularización</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="shrinkage.html"><a href="shrinkage.html#implementación-en-r"><i class="fa fa-check"></i><b>6.1.1</b> Implementación en R</a></li>
<li class="chapter" data-level="6.1.2" data-path="shrinkage.html"><a href="shrinkage.html#ejemplo-ridge-regression"><i class="fa fa-check"></i><b>6.1.2</b> Ejemplo: <em>ridge regression</em></a></li>
<li class="chapter" data-level="6.1.3" data-path="shrinkage.html"><a href="shrinkage.html#ejemplo-lasso"><i class="fa fa-check"></i><b>6.1.3</b> Ejemplo: LASSO</a></li>
<li class="chapter" data-level="6.1.4" data-path="shrinkage.html"><a href="shrinkage.html#ejemplo-elastic-net"><i class="fa fa-check"></i><b>6.1.4</b> Ejemplo: <em>elastic net</em></a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="pca-pls.html"><a href="pca-pls.html"><i class="fa fa-check"></i><b>6.2</b> Métodos de reducción de la dimensión</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="pca-pls.html"><a href="pca-pls.html#regresión-por-componentes-principales-pcr"><i class="fa fa-check"></i><b>6.2.1</b> Regresión por componentes principales (PCR)</a></li>
<li class="chapter" data-level="6.2.2" data-path="pca-pls.html"><a href="pca-pls.html#regresión-por-mínimos-cuadrados-parciales-plsr"><i class="fa fa-check"></i><b>6.2.2</b> Regresión por mínimos cuadrados parciales (PLSR)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="reg-np.html"><a href="reg-np.html"><i class="fa fa-check"></i><b>7</b> Regresión no paramétrica</a>
<ul>
<li class="chapter" data-level="7.1" data-path="reg-local.html"><a href="reg-local.html"><i class="fa fa-check"></i><b>7.1</b> Regresión local</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="reg-local.html"><a href="reg-local.html#reg-knn"><i class="fa fa-check"></i><b>7.1.1</b> Vecinos más próximos</a></li>
<li class="chapter" data-level="7.1.2" data-path="reg-local.html"><a href="reg-local.html#reg-locpol"><i class="fa fa-check"></i><b>7.1.2</b> Regresión polinómica local</a></li>
<li class="chapter" data-level="7.1.3" data-path="reg-local.html"><a href="reg-local.html#regresión-polinómica-local-robusta"><i class="fa fa-check"></i><b>7.1.3</b> Regresión polinómica local robusta</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="splines.html"><a href="splines.html"><i class="fa fa-check"></i><b>7.2</b> Splines</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="splines.html"><a href="splines.html#reg-splines"><i class="fa fa-check"></i><b>7.2.1</b> Splines de regresión</a></li>
<li class="chapter" data-level="7.2.2" data-path="splines.html"><a href="splines.html#splines-de-suavizado"><i class="fa fa-check"></i><b>7.2.2</b> Splines de suavizado</a></li>
<li class="chapter" data-level="7.2.3" data-path="splines.html"><a href="splines.html#splines-penalizados"><i class="fa fa-check"></i><b>7.2.3</b> Splines penalizados</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="reg-gam.html"><a href="reg-gam.html"><i class="fa fa-check"></i><b>7.3</b> Modelos aditivos</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="reg-gam.html"><a href="reg-gam.html#superficies-de-predicción"><i class="fa fa-check"></i><b>7.3.1</b> Superficies de predicción</a></li>
<li class="chapter" data-level="7.3.2" data-path="reg-gam.html"><a href="reg-gam.html#anova-gam"><i class="fa fa-check"></i><b>7.3.2</b> Comparación y selección de modelos</a></li>
<li class="chapter" data-level="7.3.3" data-path="reg-gam.html"><a href="reg-gam.html#mgcv-diagnosis"><i class="fa fa-check"></i><b>7.3.3</b> Diagnosis del modelo</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="mars.html"><a href="mars.html"><i class="fa fa-check"></i><b>7.4</b> Regresión spline adaptativa multivariante</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="mars.html"><a href="mars.html#mars-con-el-paquete-earth"><i class="fa fa-check"></i><b>7.4.1</b> MARS con el paquete <code>earth</code></a></li>
<li class="chapter" data-level="7.4.2" data-path="mars.html"><a href="mars.html#mars-con-el-paquete-caret"><i class="fa fa-check"></i><b>7.4.2</b> MARS con el paquete <code>caret</code></a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="pursuit.html"><a href="pursuit.html"><i class="fa fa-check"></i><b>7.5</b> Projection pursuit</a>
<ul>
<li class="chapter" data-level="7.5.1" data-path="pursuit.html"><a href="pursuit.html#ppr"><i class="fa fa-check"></i><b>7.5.1</b> Regresión por projection pursuit</a></li>
<li class="chapter" data-level="7.5.2" data-path="pursuit.html"><a href="pursuit.html#implementación-en-r-1"><i class="fa fa-check"></i><b>7.5.2</b> Implementación en R</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="neural-nets.html"><a href="neural-nets.html"><i class="fa fa-check"></i><b>8</b> Redes neuronales</a>
<ul>
<li class="chapter" data-level="8.1" data-path="single-hidden-layer-feedforward-network.html"><a href="single-hidden-layer-feedforward-network.html"><i class="fa fa-check"></i><b>8.1</b> Single-hidden-layer feedforward network</a></li>
<li class="chapter" data-level="8.2" data-path="clasificación-con-ann.html"><a href="clasificación-con-ann.html"><i class="fa fa-check"></i><b>8.2</b> Clasificación con ANN</a></li>
<li class="chapter" data-level="8.3" data-path="implementación-en-r-2.html"><a href="implementación-en-r-2.html"><i class="fa fa-check"></i><b>8.3</b> Implementación en R</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliografía.html"><a href="bibliografía.html"><i class="fa fa-check"></i>Bibliografía</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Métodos predictivos de aprendizaje estadístico</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="pursuit" class="section level2 hasAnchor" number="7.5">
<h2><span class="header-section-number">7.5</span> Projection pursuit<a href="pursuit.html#pursuit" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<p><em>Projection pursuit</em> <span class="citation">(<a href="#ref-friedman1974projection" role="doc-biblioref">Friedman y Tukey, 1974</a>)</span> es una técnica de análisis exploratorio de datos multivariantes que busca proyecciones lineales de los datos en espacios de dimensión baja, siguiendo una idea originalmente propuesta en <span class="citation">Kruskal (<a href="#ref-kruskal1969toward" role="doc-biblioref">1969</a>)</span>.
Inicialmente se presentó como una técnica gráfica y por ese motivo buscaba proyecciones de dimensión 1 o 2 (proyecciones en rectas o planos), resultando que las direcciones interesantes son aquellas con distribución no normal.
La motivación es que cuando se realizan transformaciones lineales lo habitual es que el resultado tenga la apariencia de una distribución normal (por el teorema central del límite), lo cual oculta las singularidades de los datos originales.
Se supone que los datos son una trasformación lineal de componentes no gaussianas (variables latentes) y la idea es deshacer esta transformación mediante la optimización de una función objetivo, que en este contexto recibe el nombre de <em>projection index</em>.
Aunque con orígenes distintos, <em>projection pursuit</em> es muy similar a <em>independent component analysis</em> <span class="citation">(<a href="#ref-comon1994independent" role="doc-biblioref">Comon, 1994</a>)</span>, una técnica de reducción de la dimensión que, en lugar de buscar como es habitual componentes incorreladas (ortogonales), busca componentes independientes y con distribución no normal <span class="citation">(ver por ejemplo la documentación del paquete <a href="https://CRAN.R-project.org/package=fastICA" role="doc-biblioref"><code>fastICA</code></a>, <a href="#ref-R-fastICA" role="doc-biblioref">Marchini et al., 2021</a>)</span>.</p>
<p>Hay extensiones de <em>projection pursuit</em> para regresión, clasificación, estimación de la función de densidad, etc.</p>
<div id="ppr" class="section level3 hasAnchor" number="7.5.1">
<h3><span class="header-section-number">7.5.1</span> Regresión por projection pursuit<a href="pursuit.html#ppr" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>En el método original de <em>projection pursuit regression</em> <span class="citation">(PPR, <a href="#ref-friedman1981projection" role="doc-biblioref">Friedman y Stuetzle, 1981</a>)</span> se considera el siguiente modelo semiparamétrico
<span class="math display">\[m(\mathbf{x}) = \sum_{m=1}^M g_m (\alpha_{1m}x_1 + \alpha_{2m}x_2 + \ldots + \alpha_{pm}x_p)\]</span>
siendo <span class="math inline">\(\boldsymbol{\alpha}_m = (\alpha_{1m}, \alpha_{2m}, \ldots, \alpha_{pm})\)</span> vectores de parámetros (desconocidos) de módulo unitario y <span class="math inline">\(g_m\)</span> funciones suaves (desconocidas), denominadas funciones <em>ridge</em>.</p>
<p>Con esta aproximación se obtiene un modelo muy general que evita los problemas de la maldición de la dimensionalidad.
De hecho, se trata de un <em>aproximador universal</em>: con <span class="math inline">\(M\)</span> suficientemente grande y eligiendo adecuadamente las componentes se podría aproximar cualquier función continua.
Sin embargo, el modelo resultante puede ser muy difícil de interpretar, salvo en el caso de <span class="math inline">\(M=1\)</span>, que se corresponde con el denominado <em>single index model</em> empleado habitualmente en econometría, pero que solo es algo más general que el modelo de regresión lineal múltiple.</p>
<p>El ajuste se este tipo de modelos es en principio un problema muy complejo.
Hay que estimar las funciones univariantes <span class="math inline">\(g_m\)</span> (utilizando un método de suavizado) y los parámetros <span class="math inline">\(\alpha_{im}\)</span>, utilizando como criterio de error <span class="math inline">\(\mbox{RSS}\)</span>.
En la práctica, se resuelve utilizando un proceso iterativo en el que se van fijando sucesivamente los valores de los parámetros y las funciones <em>ridge</em> (si son estimadas empleando un método que también proporcione estimaciones de su derivada, las actualizaciones de los parámetros se pueden obtener por mínimos cuadrados ponderados).</p>
<p>También se han desarrollado extensiones del método original para el caso de respuesta multivariante:
<span class="math display">\[m_i(\mathbf{x}) = \beta_{i0} + \sum_{m=1}^M \beta_{im} g_m (\alpha_{1m}x_1 + \alpha_{2m}x_2 + \ldots + \alpha_{pm}x_p)\]</span>
reescalando las funciones <em>rigde</em> de forma que tengan media cero y varianza unidad sobre las proyecciones de las observaciones.</p>
<p>Este procedimiento de regresión está muy relacionado con las redes de neuronas artificiales que han sido objeto de mayor estudio y desarrollo en los últimos años.
Estos métodos se tratarán en el Capítulo <a href="neural-nets.html#neural-nets">8</a>.</p>
</div>
<div id="implementación-en-r-1" class="section level3 hasAnchor" number="7.5.2">
<h3><span class="header-section-number">7.5.2</span> Implementación en R<a href="pursuit.html#implementación-en-r-1" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>El método PPR (con respuesta multivariante) está implementado en la función <code>ppr()</code> del paquete base<a href="#fn67" class="footnote-ref" id="fnref67"><sup>67</sup></a> de R, y es también la empleada por el método <code>"ppr"</code> de <code>caret</code>:</p>
<div class="sourceCode" id="cb465"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb465-1"><a href="pursuit.html#cb465-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ppr</span>(formula, data, nterms, <span class="at">max.terms =</span> nterms, <span class="at">optlevel =</span> <span class="dv">2</span>,</span>
<span id="cb465-2"><a href="pursuit.html#cb465-2" aria-hidden="true" tabindex="-1"></a>    <span class="at">sm.method =</span> <span class="fu">c</span>(<span class="st">&quot;supsmu&quot;</span>, <span class="st">&quot;spline&quot;</span>, <span class="st">&quot;gcvspline&quot;</span>),</span>
<span id="cb465-3"><a href="pursuit.html#cb465-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">bass =</span> <span class="dv">0</span>, <span class="at">span =</span> <span class="dv">0</span>, <span class="at">df =</span> <span class="dv">5</span>, <span class="at">gcvpen =</span> <span class="dv">1</span>, ...)</span></code></pre></div>
<p>Esta función va añadiendo términos <em>ridge</em> hasta un máximo de <code>max.terms</code> y posteriormente emplea un método hacia atrás para seleccionar <code>nterms</code> (el argumento <code>optlevel</code> controla cómo se vuelven a reajustar los términos en cada iteración).
Por defecto, emplea el <em>super suavizador</em> de Friedman (función <code>supsmu()</code>, con parámetros <code>bass</code> y <code>span</code>), aunque también admite splines (función <code>smooth.spline()</code>, fijando los grados de libertad con <code>df</code> o seleccionándolos mediante GCV).
Para más detalles, ver <code>help(ppr)</code>.</p>
<p>A continuación, retomamos el ejemplo del conjunto de datos <code>earth::Ozone1</code>.
En primer lugar ajustamos un modelo PPR con dos términos <span class="citation">(incrementando el suavizado por defecto de <code>supsmu()</code> siguiendo la recomendación de <a href="#ref-MASS" role="doc-biblioref">Venables y Ripley, 2002</a>)</span>:</p>
<div class="sourceCode" id="cb466"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb466-1"><a href="pursuit.html#cb466-1" aria-hidden="true" tabindex="-1"></a>ppreg <span class="ot">&lt;-</span> <span class="fu">ppr</span>(O3 <span class="sc">~</span> ., <span class="at">nterms =</span> <span class="dv">2</span>, <span class="at">data =</span> train, <span class="at">bass =</span> <span class="dv">2</span>)</span></code></pre></div>
<p>Si realizamos un resumen del resultado, se muestran las estimaciones de los coeficientes <span class="math inline">\(\alpha_{jm}\)</span> de las proyecciones lineales y de los coeficientes <span class="math inline">\(\beta_{im}\)</span> de las componentes rigde, que podrían interpretarse como una medida de su importancia.
En este caso, la primera componente no paramétrica es la que tiene mayor peso en la predicción.</p>
<!-- 
las estimaciones de los coeficientes permiten interpretarlas como variables latentes 
-->
<div class="sourceCode" id="cb467"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb467-1"><a href="pursuit.html#cb467-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(ppreg)</span></code></pre></div>
<pre><code>## Call:
## ppr(formula = O3 ~ ., data = train, nterms = 2, bass = 2)
## 
## Goodness of fit:
## 2 terms 
##  4033.7 
## 
## Projection direction vectors (&#39;alpha&#39;):
##          term 1     term 2    
## vh       -0.0166178  0.0474171
## wind     -0.3178679 -0.5442661
## humidity  0.2384546 -0.7864837
## temp      0.8920518 -0.0125634
## ibh      -0.0017072 -0.0017942
## dpg       0.0334769  0.2859562
## ibt       0.2055363  0.0269849
## vis      -0.0262552 -0.0141736
## doy      -0.0448190 -0.0104052
## 
## Coefficients of ridge terms (&#39;beta&#39;):
## term 1 term 2 
## 6.7904 1.5312</code></pre>
<p>Podemos representar las funciones rigde con método <code>plot()</code> (ver Figura <a href="pursuit.html#fig:ppr-plot">7.23</a>):</p>
<div class="sourceCode" id="cb469"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb469-1"><a href="pursuit.html#cb469-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(ppreg)</span></code></pre></div>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ppr-plot"></span>
<img src="07-regresion_np_files/figure-html/ppr-plot-1.png" alt="Estimaciones de las funciones ridge del ajuste PPR." width="95%" />
<p class="caption">
Figura 7.23: Estimaciones de las funciones <em>ridge</em> del ajuste PPR.
</p>
</div>
<p>En este caso, se estimaría que la primera componente lineal tiene aproximadamente un efecto cuadrático positivo, con un incremento en la pendiente a partir de un valor en torno a <span class="math inline">\(-30\)</span>, y la segunda un efecto cuadrático con un cambio de pendiente de positivo a negativo en torno a 225.</p>
<p>Por último evaluamos las predicciones en la muestra de test:</p>
<div class="sourceCode" id="cb470"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb470-1"><a href="pursuit.html#cb470-1" aria-hidden="true" tabindex="-1"></a>pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(ppreg, <span class="at">newdata =</span> test)</span>
<span id="cb470-2"><a href="pursuit.html#cb470-2" aria-hidden="true" tabindex="-1"></a>obs <span class="ot">&lt;-</span> test<span class="sc">$</span>O3</span>
<span id="cb470-3"><a href="pursuit.html#cb470-3" aria-hidden="true" tabindex="-1"></a><span class="fu">accuracy</span>(pred, obs)</span></code></pre></div>
<pre><code>##        me      rmse       mae       mpe      mape r.squared 
##   0.48198   3.23301   2.59415  -6.12031  34.87285   0.83846</code></pre>
<!-- 
pred.plot(pred, obs, main = "Observado frente a predicciones",
     xlab = "Predicción", ylab = "Observado")
-->
<p>Empleamos también el método <code>"ppr"</code> de <code>caret</code> para seleccionar automáticamente el número de términos:</p>
<div class="sourceCode" id="cb472"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb472-1"><a href="pursuit.html#cb472-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(caret)</span>
<span id="cb472-2"><a href="pursuit.html#cb472-2" aria-hidden="true" tabindex="-1"></a><span class="fu">modelLookup</span>(<span class="st">&quot;ppr&quot;</span>)</span></code></pre></div>
<pre><code>##   model parameter   label forReg forClass probModel
## 1   ppr    nterms # Terms   TRUE    FALSE     FALSE</code></pre>
<div class="sourceCode" id="cb474"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb474-1"><a href="pursuit.html#cb474-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb474-2"><a href="pursuit.html#cb474-2" aria-hidden="true" tabindex="-1"></a>caret.ppr <span class="ot">&lt;-</span> <span class="fu">train</span>(O3 <span class="sc">~</span> ., <span class="at">data =</span> train, <span class="at">method =</span> <span class="st">&quot;ppr&quot;</span>, </span>
<span id="cb474-3"><a href="pursuit.html#cb474-3" aria-hidden="true" tabindex="-1"></a>                   <span class="at">trControl =</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">&quot;cv&quot;</span>, <span class="at">number =</span> <span class="dv">10</span>))</span>
<span id="cb474-4"><a href="pursuit.html#cb474-4" aria-hidden="true" tabindex="-1"></a>caret.ppr</span></code></pre></div>
<pre><code>## Projection Pursuit Regression 
## 
## 264 samples
##   9 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 238, 238, 238, 236, 237, 239, ... 
## Resampling results across tuning parameters:
## 
##   nterms  RMSE    Rsquared  MAE   
##   1       4.3660  0.70690   3.3067
##   2       4.4793  0.69147   3.4549
##   3       4.6249  0.66441   3.5689
## 
## RMSE was used to select the optimal model using the smallest value.
## The final value used for the model was nterms = 1.</code></pre>
<!-- 
ppr-caret, out.width="70%", fig.cap='(ref:ppr-caret)'

(ver Figura \@ref(fig:ppr-caret))

(ref:ppr-caret) Errores RMSE de validación cruzada de los modelos PPR en función del numero de términos `nterms`, resaltando el valor óptimo.

bass = 2,
ggplot(caret.ppr, highlight = TRUE)
# varImp(caret.ppr) # emplea una medida genérica de importancia
-->
<p>En este caso, se selecciona un único término <em>ridge</em>.
Podríamos analizar el modelo final ajustado de forma análoga (ver Figura <a href="pursuit.html#fig:ppr-caret-plot">7.24</a>):</p>

<div class="sourceCode" id="cb476"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb476-1"><a href="pursuit.html#cb476-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(caret.ppr<span class="sc">$</span>finalModel)</span></code></pre></div>
<pre><code>## Call:
## ppr(x = as.matrix(x), y = y, nterms = param$nterms)
## 
## Goodness of fit:
## 1 terms 
##  4436.7 
## 
## Projection direction vectors (&#39;alpha&#39;):
##         vh       wind   humidity       temp        ibh        dpg 
## -0.0160915 -0.1678913  0.3517739  0.9073015 -0.0018289  0.0269015 
##        ibt        vis        doy 
##  0.1480212 -0.0264704 -0.0357039 
## 
## Coefficients of ridge terms (&#39;beta&#39;):
## term 1 
##  6.854</code></pre>
<div class="sourceCode" id="cb478"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb478-1"><a href="pursuit.html#cb478-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(caret.ppr<span class="sc">$</span>finalModel) </span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ppr-caret-plot"></span>
<img src="07-regresion_np_files/figure-html/ppr-caret-plot-1.png" alt="Estimación de la función ridge del ajuste PPR (con selección óptima del número de componentes)." width="75%" />
<p class="caption">
Figura 7.24: Estimación de la función <em>ridge</em> del ajuste PPR (con selección óptima del número de componentes).
</p>
</div>
<p>Si estudiamos las predicciones en la muestra de test, la proporción de variabilidad explicada es similar a la obtenida anteriormente con dos componentes <em>ridge</em>:</p>
<div class="sourceCode" id="cb479"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb479-1"><a href="pursuit.html#cb479-1" aria-hidden="true" tabindex="-1"></a>pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(caret.ppr, <span class="at">newdata =</span> test)</span>
<span id="cb479-2"><a href="pursuit.html#cb479-2" aria-hidden="true" tabindex="-1"></a><span class="fu">accuracy</span>(pred, obs)</span></code></pre></div>
<pre><code>##        me      rmse       mae       mpe      mape r.squared 
##   0.31359   3.36529   2.70616 -10.75327  33.83336   0.82497</code></pre>
<p>Para ajustar un modelo <em>single index</em> también se podría emplear la función <a href="https://rdrr.io/pkg/np/man/np.singleindex.html"><code>npindex()</code></a> del paquete <a href="https://github.com/JeffreyRacine/R-Package-np"><code>np</code></a> <span class="citation">(que implementa el método de <a href="#ref-ichimura1993" role="doc-biblioref">Ichimura, 1993</a>, considerando un estimador local constante)</span>, aunque en este caso ni el tiempo de computación ni el resultado es satisfactorio<a href="#fn68" class="footnote-ref" id="fnref68"><sup>68</sup></a>:</p>
<div class="sourceCode" id="cb481"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb481-1"><a href="pursuit.html#cb481-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(np)</span>
<span id="cb481-2"><a href="pursuit.html#cb481-2" aria-hidden="true" tabindex="-1"></a>formula <span class="ot">&lt;-</span> O3 <span class="sc">~</span> vh <span class="sc">+</span> wind <span class="sc">+</span> humidity <span class="sc">+</span> temp <span class="sc">+</span> ibh <span class="sc">+</span> dpg <span class="sc">+</span> ibt <span class="sc">+</span> vis <span class="sc">+</span> doy</span>
<span id="cb481-3"><a href="pursuit.html#cb481-3" aria-hidden="true" tabindex="-1"></a>bw <span class="ot">&lt;-</span> <span class="fu">npindexbw</span>(formula, <span class="at">data =</span> train, <span class="at">optim.method =</span> <span class="st">&quot;BFGS&quot;</span>, <span class="at">nmulti =</span> <span class="dv">1</span>) </span></code></pre></div>
<!-- 
# Por defecto nmulti = 5 
summary(bw) 
-->
<div class="sourceCode" id="cb482"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb482-1"><a href="pursuit.html#cb482-1" aria-hidden="true" tabindex="-1"></a>sindex <span class="ot">&lt;-</span> <span class="fu">npindex</span>(<span class="at">bws =</span> bw, <span class="at">gradients =</span> <span class="cn">TRUE</span>)</span>
<span id="cb482-2"><a href="pursuit.html#cb482-2" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(sindex)</span></code></pre></div>
<pre><code>## 
## Single Index Model
## Regression Data: 264 training points, in 9 variable(s)
## 
##       vh  wind humidity  temp     ibh    dpg    ibt      vis     doy
## Beta:  1 10.85   6.2642 8.856 0.09266 4.0038 5.6625 -0.66145 -1.1185
## Bandwidth: 13.797
## Kernel Regression Estimator: Local-Constant
## 
## Residual standard error: 3.2614
## R-squared: 0.83391
## 
## Continuous Kernel Type: Second-Order Gaussian
## No. Continuous Explanatory Vars.: 1</code></pre>
<p>Al representar la función <em>ridge</em> se observa que, aparentemente, la ventana seleccionada produce un infrasuavizado (sobreajuste; ver Figura <a href="pursuit.html#fig:npindex-plot">7.25</a>):</p>

<div class="sourceCode" id="cb484"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb484-1"><a href="pursuit.html#cb484-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(bw)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:npindex-plot"></span>
<img src="07-regresion_np_files/figure-html/npindex-plot-1.png" alt="Estimación de la función ridge del modelo single index ajustado." width="75%" />
<p class="caption">
Figura 7.25: Estimación de la función <em>ridge</em> del modelo <em>single index</em> ajustado.
</p>
</div>
<p>Si analizamos la eficiencia de las predicciones en la muestra de test, la proporción de variabilidad explicada es mucho menor que la del modelo ajustado con la función <code>ppr()</code>:</p>
<div class="sourceCode" id="cb485"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb485-1"><a href="pursuit.html#cb485-1" aria-hidden="true" tabindex="-1"></a>pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(sindex, <span class="at">newdata =</span> test)</span>
<span id="cb485-2"><a href="pursuit.html#cb485-2" aria-hidden="true" tabindex="-1"></a><span class="fu">accuracy</span>(pred, obs)</span></code></pre></div>
<pre><code>##        me      rmse       mae       mpe      mape r.squared 
##   0.35026   4.77239   3.63679  -8.82255  38.24191   0.64801</code></pre>
<div class="exercise">
<p><span id="exr:bodyfat-ppr" class="exercise"><strong>Ejercicio 7.9  </strong></span>Continuando con los ejercicios <a href="mars.html#exr:bodyfat-mars">7.6</a> y <a href="mars.html#exr:bodyfat-mgcv">7.7</a> anteriores, con los datos de grasa corporal <a href="https://rubenfcasal.github.io/mpae/reference/bodyfat.html"><code>mpae::bodyfat</code></a>, ajusta un modelo de regresión por <em>projection pursuit</em> empleando el método <code>"ppr"</code> de <code>caret</code>, seleccionando el número de términos <em>ridge</em> <code>nterms = 1:2</code> y fijando el suavizado máximo <code>bass = 10</code>.
Obtén los coeficientes del modelo, representa las funciones <em>ridge</em> y evalúa las predicciones en la muestra de test (gráfico y medidas de error).
Comparar los resultados con los obtenidos en los ejercicios anteriores.</p>
</div>

</div>
</div>
<!-- </div> -->
<h3>Bibliografía<a href="bibliografía.html#bibliografía" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent" line-spacing="2">
<div id="ref-comon1994independent" class="csl-entry">
Comon, P. (1994). Independent component analysis, a new concept? <em>Signal Processing</em>, <em>36</em>(3), 287-314. <a href="https://doi.org/10.1016/0165-1684(94)90029-9">https://doi.org/10.1016/0165-1684(94)90029-9</a>
</div>
<div id="ref-friedman1981projection" class="csl-entry">
Friedman, J., y Stuetzle, W. (1981). Projection pursuit regression. <em>Journal of the American Statistical Association</em>, <em>76</em>(376), 817-823. <a href="https://doi.org/10.1080/01621459.1981.10477729">https://doi.org/10.1080/01621459.1981.10477729</a>
</div>
<div id="ref-friedman1974projection" class="csl-entry">
Friedman, J., y Tukey, J. (1974). A projection pursuit algorithm for exploratory data analysis. <em>IEEE Transactions on computers</em>, <em>100</em>(9), 881-890. <a href="https://doi.org/10.1109/t-c.1974.224051">https://doi.org/10.1109/t-c.1974.224051</a>
</div>
<div id="ref-ichimura1993" class="csl-entry">
Ichimura, H. (1993). Semiparametric least squares (SLS) and weighted SLS estimation of single-index models. <em>Journal of Econometrics</em>, <em>58</em>(1), 71-120. <a href="https://doi.org/10.1016/0304-4076(93)90114-K">https://doi.org/10.1016/0304-4076(93)90114-K</a>
</div>
<div id="ref-kruskal1969toward" class="csl-entry">
Kruskal, J. B. (1969). Toward a practical method which helps uncover the structure of a set of multivariate observations by finding the linear transformation which optimizes a new <span>«index of condensation»</span>. <em>Statistical Computation</em>, 427-440. <a href="https://doi.org/10.1016/b978-0-12-498150-8.50024-0">https://doi.org/10.1016/b978-0-12-498150-8.50024-0</a>
</div>
<div id="ref-R-fastICA" class="csl-entry">
Marchini, J. L., Heaton, C., y Ripley, B. D. (2021). <em><span>fastICA: FastICA Algorithms to Perform ICA and Projection Pursuit</span></em>. <a href="https://cran.r-project.org/package=fastICA">https://cran.r-project.org/package=fastICA</a>
</div>
<div id="ref-MASS" class="csl-entry">
Venables, W. N., y Ripley, B. D. (2002). <em>Modern Applied Statistics with S</em> (4a. ed.). Springer. <a href="https://www.stats.ox.ac.uk/pub/MASS4/">https://www.stats.ox.ac.uk/pub/MASS4/</a>
</div>
</div>
<div class="footnotes">
<hr />
<ol start="67">
<li id="fn67"><p>Basada en la función <code>ppreg()</code> de S-PLUS e implementado en R por B.D. Ripley, inicialmente para el paquete <code>MASS</code>.<a href="pursuit.html#fnref67" class="footnote-back">↩︎</a></p></li>
<li id="fn68"><p>No admite una fórmula del tipo <code>respuesta ~ .</code>, al intentar ejecutar <code>npindexbw(O3 ~ ., data = train)</code> se produciría un error. Para solventarlo tendríamos que escribir la expresión explícita de la fórmula, por ejemplo con la ayuda de <code>reformulate(setdiff(colnames(train), "O3"), response = "O3")</code>. Aparte de esto, el valor por defecto de <code>nmulti = 5</code>, número de reinicios con punto de partida aleatorio del algoritmo de optimización, puede producir que el tiempo de computación sea excesivo.
Otro inconveniente es que los resultados de texto contienen caracteres inválidos para compilar en LaTeX y pueden aparecer errores al generar informes.<a href="pursuit.html#fnref68" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="mars.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="neural-nets.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rubenfcasal/book_mpae/edit/master/07-regresion_np.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
