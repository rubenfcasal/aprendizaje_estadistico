<!DOCTYPE html>
<html lang="es" xml:lang="es">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>4.5 Boosting en R | Métodos predictivos de aprendizaje estadístico</title>
  <meta name="description" content="4.5 Boosting en R | Métodos predictivos de aprendizaje estadístico con R." />
  <meta name="generator" content="bookdown 0.36 and GitBook 2.6.7" />

  <meta property="og:title" content="4.5 Boosting en R | Métodos predictivos de aprendizaje estadístico" />
  <meta property="og:type" content="book" />
  
  <meta property="og:description" content="4.5 Boosting en R | Métodos predictivos de aprendizaje estadístico con R." />
  <meta name="github-repo" content="rubenfcasal/aprendizaje_estadistico" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="4.5 Boosting en R | Métodos predictivos de aprendizaje estadístico" />
  
  <meta name="twitter:description" content="4.5 Boosting en R | Métodos predictivos de aprendizaje estadístico con R." />
  

<meta name="author" content="Rubén Fernández Casal (ruben.fcasal@udc.es)" />
<meta name="author" content="Julián Costa Bouzas (julian.costa@udc.es)" />
<meta name="author" content="Manuel Oviedo de la Fuente (manuel.oviedo@udc.es)" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="boosting.html"/>
<link rel="next" href="svm.html"/>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.1.0/anchor-sections.css" rel="stylesheet" />
<link href="libs/anchor-sections-1.1.0/anchor-sections-hash.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.1.0/anchor-sections.js"></script>
<script src="libs/htmlwidgets-1.6.2/htmlwidgets.js"></script>
<link href="libs/datatables-css-0.0.0/datatables-crosstalk.css" rel="stylesheet" />
<script src="libs/datatables-binding-0.30/datatables.js"></script>
<link href="libs/dt-core-1.13.4/css/jquery.dataTables.min.css" rel="stylesheet" />
<link href="libs/dt-core-1.13.4/css/jquery.dataTables.extra.css" rel="stylesheet" />
<script src="libs/dt-core-1.13.4/js/jquery.dataTables.min.js"></script>
<link href="libs/crosstalk-1.2.0/css/crosstalk.min.css" rel="stylesheet" />
<script src="libs/crosstalk-1.2.0/js/crosstalk.min.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
  
  div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
</style>
<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="./">Métodos predictivos de aprendizaje estadístico</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Bienvenida</a></li>
<li class="chapter" data-level="" data-path="prólogo.html"><a href="prólogo.html"><i class="fa fa-check"></i>Prólogo</a>
<ul>
<li class="chapter" data-level="" data-path="el-lenguaje-de-programación-r.html"><a href="el-lenguaje-de-programación-r.html"><i class="fa fa-check"></i>El lenguaje de programación R</a></li>
<li class="chapter" data-level="" data-path="organización.html"><a href="organización.html"><i class="fa fa-check"></i>Organización</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="intro-AE.html"><a href="intro-AE.html"><i class="fa fa-check"></i><b>1</b> Introducción al aprendizaje estadístico</a>
<ul>
<li class="chapter" data-level="1.1" data-path="aprendizaje-estadístico-vs.-aprendizaje-automático.html"><a href="aprendizaje-estadístico-vs.-aprendizaje-automático.html"><i class="fa fa-check"></i><b>1.1</b> Aprendizaje estadístico vs. aprendizaje automático</a>
<ul>
<li class="chapter" data-level="1.1.1" data-path="aprendizaje-estadístico-vs.-aprendizaje-automático.html"><a href="aprendizaje-estadístico-vs.-aprendizaje-automático.html#las-dos-culturas"><i class="fa fa-check"></i><b>1.1.1</b> Las dos culturas</a></li>
</ul></li>
<li class="chapter" data-level="1.2" data-path="métodos-de-aprendizaje-estadístico.html"><a href="métodos-de-aprendizaje-estadístico.html"><i class="fa fa-check"></i><b>1.2</b> Métodos de aprendizaje estadístico</a>
<ul>
<li class="chapter" data-level="1.2.1" data-path="métodos-de-aprendizaje-estadístico.html"><a href="métodos-de-aprendizaje-estadístico.html#notacion"><i class="fa fa-check"></i><b>1.2.1</b> Notación y terminología</a></li>
<li class="chapter" data-level="1.2.2" data-path="métodos-de-aprendizaje-estadístico.html"><a href="métodos-de-aprendizaje-estadístico.html#metodos-pkgs"><i class="fa fa-check"></i><b>1.2.2</b> Métodos (de aprendizaje supervisado) y paquetes de R</a></li>
</ul></li>
<li class="chapter" data-level="1.3" data-path="const-eval.html"><a href="const-eval.html"><i class="fa fa-check"></i><b>1.3</b> Construcción y evaluación de los modelos</a>
<ul>
<li class="chapter" data-level="1.3.1" data-path="const-eval.html"><a href="const-eval.html#bias-variance"><i class="fa fa-check"></i><b>1.3.1</b> Equilibrio entre sesgo y varianza: infraajuste y sobreajuste</a></li>
<li class="chapter" data-level="1.3.2" data-path="const-eval.html"><a href="const-eval.html#entrenamiento-test"><i class="fa fa-check"></i><b>1.3.2</b> Datos de entrenamiento y datos de test</a></li>
<li class="chapter" data-level="1.3.3" data-path="const-eval.html"><a href="const-eval.html#cv"><i class="fa fa-check"></i><b>1.3.3</b> Selección de hiperparámetros mediante validación cruzada</a></li>
<li class="chapter" data-level="1.3.4" data-path="const-eval.html"><a href="const-eval.html#eval-reg"><i class="fa fa-check"></i><b>1.3.4</b> Evaluación de un método de regresión</a></li>
<li class="chapter" data-level="1.3.5" data-path="const-eval.html"><a href="const-eval.html#eval-class"><i class="fa fa-check"></i><b>1.3.5</b> Evaluación de un método de clasificación</a></li>
</ul></li>
<li class="chapter" data-level="1.4" data-path="dimen-curse.html"><a href="dimen-curse.html"><i class="fa fa-check"></i><b>1.4</b> La maldición de la dimensionalidad</a></li>
<li class="chapter" data-level="1.5" data-path="analisis-modelos.html"><a href="analisis-modelos.html"><i class="fa fa-check"></i><b>1.5</b> Análisis e interpretación de los modelos</a></li>
<li class="chapter" data-level="1.6" data-path="caret.html"><a href="caret.html"><i class="fa fa-check"></i><b>1.6</b> Introducción al paquete <code>caret</code></a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="clasicos.html"><a href="clasicos.html"><i class="fa fa-check"></i><b>2</b> Métodos clásicos de estadística</a>
<ul>
<li class="chapter" data-level="2.1" data-path="rlm.html"><a href="rlm.html"><i class="fa fa-check"></i><b>2.1</b> Regresión lineal múltiple</a>
<ul>
<li class="chapter" data-level="2.1.1" data-path="rlm.html"><a href="rlm.html#colinealidad"><i class="fa fa-check"></i><b>2.1.1</b> El problema de la colinealidad</a></li>
<li class="chapter" data-level="2.1.2" data-path="rlm.html"><a href="rlm.html#seleccion-rlm"><i class="fa fa-check"></i><b>2.1.2</b> Selección de variables explicativas</a></li>
<li class="chapter" data-level="2.1.3" data-path="rlm.html"><a href="rlm.html#analisis-rlm"><i class="fa fa-check"></i><b>2.1.3</b> Análisis e interpretación del modelo</a></li>
<li class="chapter" data-level="2.1.4" data-path="rlm.html"><a href="rlm.html#eval-rlm"><i class="fa fa-check"></i><b>2.1.4</b> Evaluación de la precisión</a></li>
<li class="chapter" data-level="2.1.5" data-path="rlm.html"><a href="rlm.html#selec-ae-rlm"><i class="fa fa-check"></i><b>2.1.5</b> Selección del modelo mediante remuestreo</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="reg-glm.html"><a href="reg-glm.html"><i class="fa fa-check"></i><b>2.2</b> Modelos lineales generalizados</a>
<ul>
<li class="chapter" data-level="2.2.1" data-path="reg-glm.html"><a href="reg-glm.html#seleccion-glm"><i class="fa fa-check"></i><b>2.2.1</b> Selección de variables explicativas</a></li>
<li class="chapter" data-level="2.2.2" data-path="reg-glm.html"><a href="reg-glm.html#analisis-glm"><i class="fa fa-check"></i><b>2.2.2</b> Análisis e interpretación del modelo</a></li>
<li class="chapter" data-level="2.2.3" data-path="reg-glm.html"><a href="reg-glm.html#glm-bfan"><i class="fa fa-check"></i><b>2.2.3</b> Evaluación de la precisión</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="generadores.html"><a href="generadores.html"><i class="fa fa-check"></i><b>2.3</b> Otros métodos de clasificación</a>
<ul>
<li class="chapter" data-level="2.3.1" data-path="generadores.html"><a href="generadores.html#clas-lda"><i class="fa fa-check"></i><b>2.3.1</b> Análisis discriminante lineal</a></li>
<li class="chapter" data-level="2.3.2" data-path="generadores.html"><a href="generadores.html#clas-qda"><i class="fa fa-check"></i><b>2.3.2</b> Análisis discriminante cuadrático</a></li>
<li class="chapter" data-level="2.3.3" data-path="generadores.html"><a href="generadores.html#bayes"><i class="fa fa-check"></i><b>2.3.3</b> Bayes naíf</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="trees.html"><a href="trees.html"><i class="fa fa-check"></i><b>3</b> Árboles de decisión</a>
<ul>
<li class="chapter" data-level="3.1" data-path="árboles-de-regresión-cart.html"><a href="árboles-de-regresión-cart.html"><i class="fa fa-check"></i><b>3.1</b> Árboles de regresión CART</a></li>
<li class="chapter" data-level="3.2" data-path="árboles-de-clasificación-cart.html"><a href="árboles-de-clasificación-cart.html"><i class="fa fa-check"></i><b>3.2</b> Árboles de clasificación CART</a></li>
<li class="chapter" data-level="3.3" data-path="tree-rpart.html"><a href="tree-rpart.html"><i class="fa fa-check"></i><b>3.3</b> CART con el paquete <code>rpart</code></a>
<ul>
<li class="chapter" data-level="3.3.1" data-path="tree-rpart.html"><a href="tree-rpart.html#reg-rpart"><i class="fa fa-check"></i><b>3.3.1</b> Ejemplo: regresión</a></li>
<li class="chapter" data-level="3.3.2" data-path="tree-rpart.html"><a href="tree-rpart.html#class-rpart"><i class="fa fa-check"></i><b>3.3.2</b> Ejemplo: modelo de clasificación</a></li>
<li class="chapter" data-level="3.3.3" data-path="tree-rpart.html"><a href="tree-rpart.html#interfaz-de-caret"><i class="fa fa-check"></i><b>3.3.3</b> Interfaz de <code>caret</code></a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="alternativas-a-los-árboles-cart.html"><a href="alternativas-a-los-árboles-cart.html"><i class="fa fa-check"></i><b>3.4</b> Alternativas a los árboles CART</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="bagging-boosting.html"><a href="bagging-boosting.html"><i class="fa fa-check"></i><b>4</b> Bagging y boosting</a>
<ul>
<li class="chapter" data-level="4.1" data-path="bagging.html"><a href="bagging.html"><i class="fa fa-check"></i><b>4.1</b> Bagging</a></li>
<li class="chapter" data-level="4.2" data-path="rf.html"><a href="rf.html"><i class="fa fa-check"></i><b>4.2</b> Bosques aleatorios</a></li>
<li class="chapter" data-level="4.3" data-path="bagging-rf-r.html"><a href="bagging-rf-r.html"><i class="fa fa-check"></i><b>4.3</b> Bagging y bosques aleatorios en R</a>
<ul>
<li class="chapter" data-level="4.3.1" data-path="bagging-rf-r.html"><a href="bagging-rf-r.html#ejemplo-clasificación-con-bagging"><i class="fa fa-check"></i><b>4.3.1</b> Ejemplo: clasificación con bagging</a></li>
<li class="chapter" data-level="4.3.2" data-path="bagging-rf-r.html"><a href="bagging-rf-r.html#ejemplo-clasif-rf"><i class="fa fa-check"></i><b>4.3.2</b> Ejemplo: clasificación con bosques aleatorios</a></li>
<li class="chapter" data-level="4.3.3" data-path="bagging-rf-r.html"><a href="bagging-rf-r.html#ejemplo-bosques-aleatorios-con-caret"><i class="fa fa-check"></i><b>4.3.3</b> Ejemplo: bosques aleatorios con <code>caret</code></a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="boosting.html"><a href="boosting.html"><i class="fa fa-check"></i><b>4.4</b> Boosting</a></li>
<li class="chapter" data-level="4.5" data-path="boosting-r.html"><a href="boosting-r.html"><i class="fa fa-check"></i><b>4.5</b> Boosting en R</a>
<ul>
<li class="chapter" data-level="4.5.1" data-path="boosting-r.html"><a href="boosting-r.html#ejemplo-clasificación-con-el-paquete-ada"><i class="fa fa-check"></i><b>4.5.1</b> Ejemplo: clasificación con el paquete <code>ada</code></a></li>
<li class="chapter" data-level="4.5.2" data-path="boosting-r.html"><a href="boosting-r.html#ejemplo-regresión-con-el-paquete-gbm"><i class="fa fa-check"></i><b>4.5.2</b> Ejemplo: regresión con el paquete <code>gbm</code></a></li>
<li class="chapter" data-level="4.5.3" data-path="boosting-r.html"><a href="boosting-r.html#xgb-caret"><i class="fa fa-check"></i><b>4.5.3</b> Ejemplo: XGBoost con el paquete <code>caret</code></a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="svm.html"><a href="svm.html"><i class="fa fa-check"></i><b>5</b> Máquinas de soporte vectorial</a>
<ul>
<li class="chapter" data-level="5.1" data-path="clasificadores-de-máximo-margen.html"><a href="clasificadores-de-máximo-margen.html"><i class="fa fa-check"></i><b>5.1</b> Clasificadores de máximo margen</a></li>
<li class="chapter" data-level="5.2" data-path="clasificadores-de-soporte-vectorial.html"><a href="clasificadores-de-soporte-vectorial.html"><i class="fa fa-check"></i><b>5.2</b> Clasificadores de soporte vectorial</a></li>
<li class="chapter" data-level="5.3" data-path="máquinas-de-soporte-vectorial.html"><a href="máquinas-de-soporte-vectorial.html"><i class="fa fa-check"></i><b>5.3</b> Máquinas de soporte vectorial</a>
<ul>
<li class="chapter" data-level="5.3.1" data-path="máquinas-de-soporte-vectorial.html"><a href="máquinas-de-soporte-vectorial.html#regresión-con-svm"><i class="fa fa-check"></i><b>5.3.1</b> Regresión con SVM</a></li>
<li class="chapter" data-level="5.3.2" data-path="máquinas-de-soporte-vectorial.html"><a href="máquinas-de-soporte-vectorial.html#ventajas-e-incovenientes"><i class="fa fa-check"></i><b>5.3.2</b> Ventajas e incovenientes</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="svm-kernlab.html"><a href="svm-kernlab.html"><i class="fa fa-check"></i><b>5.4</b> SVM en R</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="ext-glm.html"><a href="ext-glm.html"><i class="fa fa-check"></i><b>6</b> Extensiones de los modelos lineales (generalizados)</a>
<ul>
<li class="chapter" data-level="6.1" data-path="shrinkage.html"><a href="shrinkage.html"><i class="fa fa-check"></i><b>6.1</b> Métodos de regularización</a>
<ul>
<li class="chapter" data-level="6.1.1" data-path="shrinkage.html"><a href="shrinkage.html#implementación-en-r"><i class="fa fa-check"></i><b>6.1.1</b> Implementación en R</a></li>
<li class="chapter" data-level="6.1.2" data-path="shrinkage.html"><a href="shrinkage.html#ejemplo-ridge-regression"><i class="fa fa-check"></i><b>6.1.2</b> Ejemplo: <em>ridge regression</em></a></li>
<li class="chapter" data-level="6.1.3" data-path="shrinkage.html"><a href="shrinkage.html#ejemplo-lasso"><i class="fa fa-check"></i><b>6.1.3</b> Ejemplo: LASSO</a></li>
<li class="chapter" data-level="6.1.4" data-path="shrinkage.html"><a href="shrinkage.html#ejemplo-elastic-net"><i class="fa fa-check"></i><b>6.1.4</b> Ejemplo: <em>elastic net</em></a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="pca-pls.html"><a href="pca-pls.html"><i class="fa fa-check"></i><b>6.2</b> Métodos de reducción de la dimensión</a>
<ul>
<li class="chapter" data-level="6.2.1" data-path="pca-pls.html"><a href="pca-pls.html#regresión-por-componentes-principales-pcr"><i class="fa fa-check"></i><b>6.2.1</b> Regresión por componentes principales (PCR)</a></li>
<li class="chapter" data-level="6.2.2" data-path="pca-pls.html"><a href="pca-pls.html#regresión-por-mínimos-cuadrados-parciales-plsr"><i class="fa fa-check"></i><b>6.2.2</b> Regresión por mínimos cuadrados parciales (PLSR)</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="7" data-path="reg-np.html"><a href="reg-np.html"><i class="fa fa-check"></i><b>7</b> Regresión no paramétrica</a>
<ul>
<li class="chapter" data-level="7.1" data-path="reg-local.html"><a href="reg-local.html"><i class="fa fa-check"></i><b>7.1</b> Regresión local</a>
<ul>
<li class="chapter" data-level="7.1.1" data-path="reg-local.html"><a href="reg-local.html#reg-knn"><i class="fa fa-check"></i><b>7.1.1</b> Vecinos más próximos</a></li>
<li class="chapter" data-level="7.1.2" data-path="reg-local.html"><a href="reg-local.html#reg-locpol"><i class="fa fa-check"></i><b>7.1.2</b> Regresión polinómica local</a></li>
<li class="chapter" data-level="7.1.3" data-path="reg-local.html"><a href="reg-local.html#regresión-polinómica-local-robusta"><i class="fa fa-check"></i><b>7.1.3</b> Regresión polinómica local robusta</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="splines.html"><a href="splines.html"><i class="fa fa-check"></i><b>7.2</b> Splines</a>
<ul>
<li class="chapter" data-level="7.2.1" data-path="splines.html"><a href="splines.html#reg-splines"><i class="fa fa-check"></i><b>7.2.1</b> Splines de regresión</a></li>
<li class="chapter" data-level="7.2.2" data-path="splines.html"><a href="splines.html#splines-de-suavizado"><i class="fa fa-check"></i><b>7.2.2</b> Splines de suavizado</a></li>
<li class="chapter" data-level="7.2.3" data-path="splines.html"><a href="splines.html#splines-penalizados"><i class="fa fa-check"></i><b>7.2.3</b> Splines penalizados</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="reg-gam.html"><a href="reg-gam.html"><i class="fa fa-check"></i><b>7.3</b> Modelos aditivos</a>
<ul>
<li class="chapter" data-level="7.3.1" data-path="reg-gam.html"><a href="reg-gam.html#superficies-de-predicción"><i class="fa fa-check"></i><b>7.3.1</b> Superficies de predicción</a></li>
<li class="chapter" data-level="7.3.2" data-path="reg-gam.html"><a href="reg-gam.html#anova-gam"><i class="fa fa-check"></i><b>7.3.2</b> Comparación y selección de modelos</a></li>
<li class="chapter" data-level="7.3.3" data-path="reg-gam.html"><a href="reg-gam.html#mgcv-diagnosis"><i class="fa fa-check"></i><b>7.3.3</b> Diagnosis del modelo</a></li>
</ul></li>
<li class="chapter" data-level="7.4" data-path="mars.html"><a href="mars.html"><i class="fa fa-check"></i><b>7.4</b> Regresión spline adaptativa multivariante</a>
<ul>
<li class="chapter" data-level="7.4.1" data-path="mars.html"><a href="mars.html#mars-con-el-paquete-earth"><i class="fa fa-check"></i><b>7.4.1</b> MARS con el paquete <code>earth</code></a></li>
<li class="chapter" data-level="7.4.2" data-path="mars.html"><a href="mars.html#mars-con-el-paquete-caret"><i class="fa fa-check"></i><b>7.4.2</b> MARS con el paquete <code>caret</code></a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="pursuit.html"><a href="pursuit.html"><i class="fa fa-check"></i><b>7.5</b> Projection pursuit</a>
<ul>
<li class="chapter" data-level="7.5.1" data-path="pursuit.html"><a href="pursuit.html#ppr"><i class="fa fa-check"></i><b>7.5.1</b> Regresión por projection pursuit</a></li>
<li class="chapter" data-level="7.5.2" data-path="pursuit.html"><a href="pursuit.html#implementación-en-r-1"><i class="fa fa-check"></i><b>7.5.2</b> Implementación en R</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="8" data-path="neural-nets.html"><a href="neural-nets.html"><i class="fa fa-check"></i><b>8</b> Redes neuronales</a>
<ul>
<li class="chapter" data-level="8.1" data-path="single-hidden-layer-feedforward-network.html"><a href="single-hidden-layer-feedforward-network.html"><i class="fa fa-check"></i><b>8.1</b> Single-hidden-layer feedforward network</a></li>
<li class="chapter" data-level="8.2" data-path="clasificación-con-ann.html"><a href="clasificación-con-ann.html"><i class="fa fa-check"></i><b>8.2</b> Clasificación con ANN</a></li>
<li class="chapter" data-level="8.3" data-path="implementación-en-r-2.html"><a href="implementación-en-r-2.html"><i class="fa fa-check"></i><b>8.3</b> Implementación en R</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="bibliografía.html"><a href="bibliografía.html"><i class="fa fa-check"></i>Bibliografía</a></li>
<li class="divider"></li>
<li><a href="https://github.com/rstudio/bookdown" target="blank">Published with bookdown</a></li>

</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Métodos predictivos de aprendizaje estadístico</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="boosting-r" class="section level2 hasAnchor" number="4.5">
<h2><span class="header-section-number">4.5</span> Boosting en R<a href="boosting-r.html#boosting-r" class="anchor-section" aria-label="Anchor link to header"></a></h2>
<!-- 
Búsquedas en caret: boost 
Ver [CRAN Task View: Machine Learning & Statistical Learning](https://cran.r-project.org/web/views/MachineLearning.html))
-->
<p>Los métodos boosting están entre los más populares en aprendizaje estadístico. Están implementados en numerosos paquetes de R, por ejemplo: <a href="https://CRAN.R-project.org/package=ada"><code>ada</code></a>, <a href="https://CRAN.R-project.org/package=adabag"><code>adabag</code></a>, <a href="https://CRAN.R-project.org/package=mboost"><code>mboost</code></a>, <a href="https://CRAN.R-project.org/package=gbm"><code>gbm</code></a>, <a href="https://github.com/dmlc/xgboost/tree/master/R-package"><code>xgboost</code></a>.</p>
<div id="ejemplo-clasificación-con-el-paquete-ada" class="section level3 hasAnchor" number="4.5.1">
<h3><span class="header-section-number">4.5.1</span> Ejemplo: clasificación con el paquete <code>ada</code><a href="boosting-r.html#ejemplo-clasificación-con-el-paquete-ada" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>La función <code>ada()</code> del paquete <a href="https://CRAN.R-project.org/package=ada"><code>ada</code></a> <span class="citation">(<a href="#ref-culp2006ada" role="doc-biblioref">Culp et al., 2006</a>)</span> implementa diversos métodos boosting, incluyendo el algoritmo original AdaBoost.
Emplea <code>rpart</code> para la construcción de los árboles, aunque solo admite respuestas dicotómicas y dos funciones de pérdida (exponencial y logística).
Además, un posible problema al emplear esta función es que ordena alfabéticamente los niveles del factor, lo que puede llevar a una mala interpretación de los resultados.</p>
<p>Los principales parámetros son los siguientes:</p>
<div class="sourceCode" id="cb272"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb272-1"><a href="boosting-r.html#cb272-1" aria-hidden="true" tabindex="-1"></a><span class="fu">ada</span>(formula, data, <span class="at">loss =</span> <span class="fu">c</span>(<span class="st">&quot;exponential&quot;</span>, <span class="st">&quot;logistic&quot;</span>),</span>
<span id="cb272-2"><a href="boosting-r.html#cb272-2" aria-hidden="true" tabindex="-1"></a>    <span class="at">type =</span> <span class="fu">c</span>(<span class="st">&quot;discrete&quot;</span>, <span class="st">&quot;real&quot;</span>, <span class="st">&quot;gentle&quot;</span>), <span class="at">iter =</span> <span class="dv">50</span>, </span>
<span id="cb272-3"><a href="boosting-r.html#cb272-3" aria-hidden="true" tabindex="-1"></a>    <span class="at">nu =</span> <span class="fl">0.1</span>, <span class="at">bag.frac =</span> <span class="fl">0.5</span>, ...)</span></code></pre></div>
<ul>
<li><p><code>formula</code> y <code>data</code> (opcional): permiten especificar la respuesta y las variables predictoras de la forma habitual (típicamente <code>respuesta ~ .</code>; también admite matrices <code>x</code> e <code>y</code> en lugar de fórmulas).</p></li>
<li><p><code>loss</code>: función de pérdida; por defecto <code>"exponential"</code> (algoritmo AdaBoost).</p></li>
<li><p><code>type</code>: algoritmo boosting; por defecto <code>"discrete"</code> que implementa el algoritmo AdaBoost original que predice la variable respuesta. Otras alternativas son <code>"real"</code>, que implementa el algoritmo <em>Real AdaBoost</em> <span class="citation">(<a href="#ref-friedman2000additive" role="doc-biblioref">Friedman et al., 2000</a>)</span> que permite estimar las probabilidades, y <code>"gentle"</code>, una versión modificada del anterior que emplea un método Newton de optimización por pasos (en lugar de optimización exacta).</p></li>
<li><p><code>iter</code>: número de iteraciones boosting; por defecto 50.</p></li>
<li><p><code>nu</code>: parámetro de regularización <span class="math inline">\(\lambda\)</span>; por defecto 0.1. Disminuyendo este parámetro es de esperar que se obtenga una mejora en la precisión de las predicciones, pero requeriría aumentar <code>iter,</code> aumentando notablemente el tiempo de computación y los requerimientos de memoria.</p></li>
<li><p><code>bag.frac</code>: proporción de observaciones seleccionadas al azar para crecer cada árbol; 0.5 por defecto.</p></li>
<li><p><code>...</code>: argumentos adicionales para <code>rpart.control</code>; por defecto <code>rpart.control(maxdepth = 1, cp = -1, minsplit = 0, xval = 0)</code>.</p></li>
</ul>
<p>A modo de ejemplo consideraremos el conjunto de datos de calidad de vino empleado en las secciones <a href="tree-rpart.html#class-rpart">3.3.2</a> y <a href="bagging-rf-r.html#bagging-rf-r">4.3</a>. Para evitar problemas reordenamos alfabéticamente los niveles de la respuesta.</p>
<div class="sourceCode" id="cb273"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb273-1"><a href="boosting-r.html#cb273-1" aria-hidden="true" tabindex="-1"></a><span class="co"># data(winetaste, package = &quot;mpae&quot;)</span></span>
<span id="cb273-2"><a href="boosting-r.html#cb273-2" aria-hidden="true" tabindex="-1"></a><span class="co"># Reordenar alfabéticamente los niveles de winetaste$taste</span></span>
<span id="cb273-3"><a href="boosting-r.html#cb273-3" aria-hidden="true" tabindex="-1"></a>winetaste<span class="sc">$</span>taste <span class="ot">&lt;-</span> <span class="fu">factor</span>(<span class="fu">as.character</span>(winetaste<span class="sc">$</span>taste))</span>
<span id="cb273-4"><a href="boosting-r.html#cb273-4" aria-hidden="true" tabindex="-1"></a><span class="co"># Partición de los datos</span></span>
<span id="cb273-5"><a href="boosting-r.html#cb273-5" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb273-6"><a href="boosting-r.html#cb273-6" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> winetaste</span>
<span id="cb273-7"><a href="boosting-r.html#cb273-7" aria-hidden="true" tabindex="-1"></a>nobs <span class="ot">&lt;-</span> <span class="fu">nrow</span>(df)</span>
<span id="cb273-8"><a href="boosting-r.html#cb273-8" aria-hidden="true" tabindex="-1"></a>itrain <span class="ot">&lt;-</span> <span class="fu">sample</span>(nobs, <span class="fl">0.8</span> <span class="sc">*</span> nobs)</span>
<span id="cb273-9"><a href="boosting-r.html#cb273-9" aria-hidden="true" tabindex="-1"></a>train <span class="ot">&lt;-</span> df[itrain, ]</span>
<span id="cb273-10"><a href="boosting-r.html#cb273-10" aria-hidden="true" tabindex="-1"></a>test <span class="ot">&lt;-</span> df[<span class="sc">-</span>itrain, ]</span></code></pre></div>
<p>El siguiente código llama a la función <code>ada()</code> con la opción para estimar probabilidades (<code>type = "real"</code>, <em>Real AdaBoost</em>), considerando interacciones de segundo orden entre los predictores (<code>maxdepth = 2</code>), disminuyendo ligeramente el valor del parámetro de aprendizaje y aumentando el número de iteraciones:</p>
<div class="sourceCode" id="cb274"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb274-1"><a href="boosting-r.html#cb274-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ada)</span>
<span id="cb274-2"><a href="boosting-r.html#cb274-2" aria-hidden="true" tabindex="-1"></a>control <span class="ot">&lt;-</span> <span class="fu">rpart.control</span>(<span class="at">maxdepth =</span> <span class="dv">2</span>, <span class="at">cp =</span> <span class="dv">0</span>, <span class="at">minsplit =</span> <span class="dv">10</span>, <span class="at">xval =</span> <span class="dv">0</span>)</span>
<span id="cb274-3"><a href="boosting-r.html#cb274-3" aria-hidden="true" tabindex="-1"></a>ada.boost <span class="ot">&lt;-</span> <span class="fu">ada</span>(taste <span class="sc">~</span> ., <span class="at">data =</span> train, <span class="at">type =</span> <span class="st">&quot;real&quot;</span>,</span>
<span id="cb274-4"><a href="boosting-r.html#cb274-4" aria-hidden="true" tabindex="-1"></a>             <span class="at">control =</span> control, <span class="at">iter =</span> <span class="dv">100</span>, <span class="at">nu =</span> <span class="fl">0.05</span>)</span>
<span id="cb274-5"><a href="boosting-r.html#cb274-5" aria-hidden="true" tabindex="-1"></a>ada.boost</span></code></pre></div>
<pre><code>## Call:
## ada(taste ~ ., data = train, type = &quot;real&quot;, control = control, 
##     iter = 100, nu = 0.05)
## 
## Loss: exponential Method: real   Iteration: 100 
## 
## Final Confusion Matrix for Data:
##           Final Prediction
## True value bad good
##       bad  162  176
##       good  46  616
## 
## Train Error: 0.222 
## 
## Out-Of-Bag Error:  0.233  iteration= 99 
## 
## Additional Estimates of number of iterations:
## 
## train.err1 train.kap1 
##         93         93</code></pre>
<p>Con el método <code>plot()</code> podemos representar la evolución del error de clasificación al aumentar el número de iteraciones (ver Figura <a href="boosting-r.html#fig:ada-plot">4.11</a>):</p>

<div class="sourceCode" id="cb276"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb276-1"><a href="boosting-r.html#cb276-1" aria-hidden="true" tabindex="-1"></a><span class="fu">plot</span>(ada.boost)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:ada-plot"></span>
<img src="04-bagging_boosting_files/figure-html/ada-plot-1.png" alt="Evolución de la tasa de error utilizando ada()." width="75%" />
<p class="caption">
Figura 4.11: Evolución de la tasa de error utilizando <code>ada()</code>.
</p>
</div>
<!-- 
Con la función `varplot()` podemos representar la importancia de las variables (y almacenarla empleando `type = "scores"`): 


```r
res <- varplot(ada.boost, type = "scores")
```

<img src="04-bagging_boosting_files/figure-html/unnamed-chunk-2-1.png" width="75%" style="display: block; margin: auto;" />

```r
res
```

```
##              density total.sulfur.dioxide            chlorides 
##             0.075183             0.068864             0.065863 
##                   pH       residual.sugar        fixed.acidity 
##             0.060489             0.056722             0.056057 
##          citric.acid     volatile.acidity            sulphates 
##             0.055510             0.050749             0.049152 
##  free.sulfur.dioxide              alcohol 
##             0.047991             0.045227
```
-->
<p>Podemos evaluar la precisión en la muestra de test empleando el procedimiento habitual:</p>
<div class="sourceCode" id="cb277"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb277-1"><a href="boosting-r.html#cb277-1" aria-hidden="true" tabindex="-1"></a>pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(ada.boost, <span class="at">newdata =</span> test)</span>
<span id="cb277-2"><a href="boosting-r.html#cb277-2" aria-hidden="true" tabindex="-1"></a>caret<span class="sc">::</span><span class="fu">confusionMatrix</span>(pred, test<span class="sc">$</span>taste, <span class="at">positive =</span> <span class="st">&quot;good&quot;</span>)</span></code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction bad good
##       bad   34   16
##       good  50  150
##                                        
##                Accuracy : 0.736        
##                  95% CI : (0.677, 0.79)
##     No Information Rate : 0.664        
##     P-Value [Acc &gt; NIR] : 0.00861      
##                                        
##                   Kappa : 0.343        
##                                        
##  Mcnemar&#39;s Test P-Value : 4.87e-05     
##                                        
##             Sensitivity : 0.904        
##             Specificity : 0.405        
##          Pos Pred Value : 0.750        
##          Neg Pred Value : 0.680        
##              Prevalence : 0.664        
##          Detection Rate : 0.600        
##    Detection Prevalence : 0.800        
##       Balanced Accuracy : 0.654        
##                                        
##        &#39;Positive&#39; Class : good         
## </code></pre>
<p>Para obtener las estimaciones de las probabilidades, habría que establecer <code>type = "probs"</code> al predecir (devolverá una matriz en la que cada columna se corresponde con un nivel):</p>
<div class="sourceCode" id="cb279"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb279-1"><a href="boosting-r.html#cb279-1" aria-hidden="true" tabindex="-1"></a>p.est <span class="ot">&lt;-</span> <span class="fu">predict</span>(ada.boost, <span class="at">newdata =</span> test, <span class="at">type =</span> <span class="st">&quot;probs&quot;</span>)</span>
<span id="cb279-2"><a href="boosting-r.html#cb279-2" aria-hidden="true" tabindex="-1"></a><span class="fu">head</span>(p.est)</span></code></pre></div>
<pre><code>##        [,1]    [,2]
## 1  0.498771 0.50123
## 4  0.309222 0.69078
## 9  0.027743 0.97226
## 10 0.045962 0.95404
## 12 0.442744 0.55726
## 16 0.373759 0.62624</code></pre>
<p>Este procedimiento también está implementado en el paquete <code>caret</code> seleccionando el método <code>"ada"</code>, que considera como hiperparámetros:</p>
<div class="sourceCode" id="cb281"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb281-1"><a href="boosting-r.html#cb281-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(caret)</span>
<span id="cb281-2"><a href="boosting-r.html#cb281-2" aria-hidden="true" tabindex="-1"></a><span class="fu">modelLookup</span>(<span class="st">&quot;ada&quot;</span>)</span></code></pre></div>
<pre><code>##   model parameter          label forReg forClass probModel
## 1   ada      iter         #Trees  FALSE     TRUE      TRUE
## 2   ada  maxdepth Max Tree Depth  FALSE     TRUE      TRUE
## 3   ada        nu  Learning Rate  FALSE     TRUE      TRUE</code></pre>
<p>Por defecto la función <code>train()</code> solo considera nueve combinaciones de hiperparámetros (en lugar de las 27 que cabría esperar):</p>
<div class="sourceCode" id="cb283"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb283-1"><a href="boosting-r.html#cb283-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb283-2"><a href="boosting-r.html#cb283-2" aria-hidden="true" tabindex="-1"></a>trControl <span class="ot">&lt;-</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">&quot;cv&quot;</span>, <span class="at">number =</span> <span class="dv">5</span>)</span>
<span id="cb283-3"><a href="boosting-r.html#cb283-3" aria-hidden="true" tabindex="-1"></a>caret.ada0 <span class="ot">&lt;-</span> <span class="fu">train</span>(taste <span class="sc">~</span> ., <span class="at">method =</span> <span class="st">&quot;ada&quot;</span>, <span class="at">data =</span> train, </span>
<span id="cb283-4"><a href="boosting-r.html#cb283-4" aria-hidden="true" tabindex="-1"></a>                    <span class="at">trControl =</span> trControl)</span>
<span id="cb283-5"><a href="boosting-r.html#cb283-5" aria-hidden="true" tabindex="-1"></a>caret.ada0</span></code></pre></div>
<pre><code>## Boosted Classification Trees 
## 
## 1000 samples
##   11 predictor
##    2 classes: &#39;bad&#39;, &#39;good&#39; 
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 800, 801, 800, 800, 799 
## Resampling results across tuning parameters:
## 
##   maxdepth  iter  Accuracy  Kappa  
##   1          50   0.71001   0.24035
##   1         100   0.72203   0.28249
##   1         150   0.73603   0.33466
##   2          50   0.75298   0.38729
##   2         100   0.75397   0.40196
##   2         150   0.75597   0.41420
##   3          50   0.75700   0.41128
##   3         100   0.75503   0.41500
##   3         150   0.76500   0.44088
## 
## Tuning parameter &#39;nu&#39; was held constant at a value of 0.1
## Accuracy was used to select the optimal model using the largest value.
## The final values used for the model were iter = 150, maxdepth = 3 and
##  nu = 0.1.</code></pre>
<p>En la salida anterior, se observa que el parámetro <code>nu</code> se ha fijado en 0.1, por lo que solo se tienen los resultados para las combinaciones de <code>maxdepth</code> e <code>iter</code>.
Se puede aumentar el número de combinaciones empleando <code>tuneLength</code> o <code>tuneGrid</code>, pero la búsqueda en una rejilla completa puede incrementar considerablemente el tiempo de computación.
Por este motivo, se suelen seguir procedimientos alternativos de búsqueda. Por ejemplo, fijar la tasa de aprendizaje (inicialmente a un valor alto) para seleccionar primero un número de interaciones y la complejidad del árbol, y posteriormente fijar estos valores para seleccionar una nueva tasa de aprendizaje (repitiendo el proceso, si es necesario, hasta conseguir convergencia).</p>
<div class="sourceCode" id="cb285"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb285-1"><a href="boosting-r.html#cb285-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb285-2"><a href="boosting-r.html#cb285-2" aria-hidden="true" tabindex="-1"></a>tuneGrid <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">iter =</span> <span class="dv">150</span>, <span class="at">maxdepth =</span> <span class="dv">3</span>,</span>
<span id="cb285-3"><a href="boosting-r.html#cb285-3" aria-hidden="true" tabindex="-1"></a>                       <span class="at">nu =</span> <span class="fu">c</span>(<span class="fl">0.3</span>, <span class="fl">0.1</span>, <span class="fl">0.05</span>, <span class="fl">0.01</span>, <span class="fl">0.005</span>))</span>
<span id="cb285-4"><a href="boosting-r.html#cb285-4" aria-hidden="true" tabindex="-1"></a>caret.ada1 <span class="ot">&lt;-</span> <span class="fu">train</span>(taste <span class="sc">~</span> ., <span class="at">method =</span> <span class="st">&quot;ada&quot;</span>, <span class="at">data =</span> train,</span>
<span id="cb285-5"><a href="boosting-r.html#cb285-5" aria-hidden="true" tabindex="-1"></a>                    <span class="at">tuneGrid =</span> tuneGrid, <span class="at">trControl =</span> trControl)</span>
<span id="cb285-6"><a href="boosting-r.html#cb285-6" aria-hidden="true" tabindex="-1"></a>caret.ada1</span></code></pre></div>
<pre><code>## Boosted Classification Trees 
## 
## 1000 samples
##   11 predictor
##    2 classes: &#39;bad&#39;, &#39;good&#39; 
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 800, 801, 800, 800, 799 
## Resampling results across tuning parameters:
## 
##   nu     Accuracy  Kappa  
##   0.005  0.74397   0.37234
##   0.010  0.74398   0.37260
##   0.050  0.75598   0.41168
##   0.100  0.76198   0.43652
##   0.300  0.75801   0.44051
## 
## Tuning parameter &#39;iter&#39; was held constant at a value of 150
## 
## Tuning parameter &#39;maxdepth&#39; was held constant at a value of 3
## Accuracy was used to select the optimal model using the largest value.
## The final values used for the model were iter = 150, maxdepth = 3 and
##  nu = 0.1.</code></pre>
<p>Por último, podemos evaluar la precisión del modelo en la muestra de test:</p>
<div class="sourceCode" id="cb287"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb287-1"><a href="boosting-r.html#cb287-1" aria-hidden="true" tabindex="-1"></a><span class="fu">confusionMatrix</span>(<span class="fu">predict</span>(caret.ada1, <span class="at">newdata =</span> test), </span>
<span id="cb287-2"><a href="boosting-r.html#cb287-2" aria-hidden="true" tabindex="-1"></a>                test<span class="sc">$</span>taste, <span class="at">positive =</span> <span class="st">&quot;good&quot;</span>)</span></code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction bad good
##       bad   40   21
##       good  44  145
##                                         
##                Accuracy : 0.74          
##                  95% CI : (0.681, 0.793)
##     No Information Rate : 0.664         
##     P-Value [Acc &gt; NIR] : 0.00584       
##                                         
##                   Kappa : 0.375         
##                                         
##  Mcnemar&#39;s Test P-Value : 0.00636       
##                                         
##             Sensitivity : 0.873         
##             Specificity : 0.476         
##          Pos Pred Value : 0.767         
##          Neg Pred Value : 0.656         
##              Prevalence : 0.664         
##          Detection Rate : 0.580         
##    Detection Prevalence : 0.756         
##       Balanced Accuracy : 0.675         
##                                         
##        &#39;Positive&#39; Class : good          
## </code></pre>
<div class="exercise">
<p><span id="exr:bfan-ada-caret" class="exercise"><strong>Ejercicio 4.3  </strong></span>Continuando con el Ejercicio <a href="bagging-rf-r.html#exr:bfan-rf-caret">4.2</a> y utilizando la misma partición, vuelve a clasificar los individuos según su nivel de grasa corporal (<code>bfan</code>), pero ahora empleando boosting mediante el método <code>"ada"</code> del paquete <code>caret</code>:</p>
<ol style="list-style-type: lower-alpha">
<li><p>Selecciona los valores “óptimos” de los hiperparámetros considerando las posibles combinaciones de <code>iter =  c(75, 150)</code>, <code>maxdepth = 1:2</code> y <code>nu = c(0.5, 0.25, 0.1)</code>, mediante validación cruzada con 5 grupos. Representa la precisión de CV dependiendo de los valores de los hiperparámetros.</p></li>
<li><p>Representa la evolución del error de clasificación al aumentar el número de iteraciones del algoritmo.</p></li>
<li><p>Estudia la importancia de los predictores y el efecto del más importante.</p></li>
<li><p>Evalúa la precisión de las predicciones en la muestra de test y compara los resultados con los obtenidos en el Ejercicio <a href="bagging-rf-r.html#exr:bfan-rf-caret">4.2</a>.</p></li>
</ol>
</div>
</div>
<div id="ejemplo-regresión-con-el-paquete-gbm" class="section level3 hasAnchor" number="4.5.2">
<h3><span class="header-section-number">4.5.2</span> Ejemplo: regresión con el paquete <code>gbm</code><a href="boosting-r.html#ejemplo-regresión-con-el-paquete-gbm" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>El paquete <a href="https://CRAN.R-project.org/package=gbm"><code>gbm</code></a> <span class="citation">(<a href="#ref-R-gbm" role="doc-biblioref">B. Greenwell et al., 2022</a>)</span> implementa el algoritmo SGB de <span class="citation">Friedman (<a href="#ref-friedman2002stochastic" role="doc-biblioref">2002</a>)</span> y admite varios tipos de respuesta considerando distintas funciones de pérdida (aunque en el caso de variables dicotómicas estas deben<a href="#fn45" class="footnote-ref" id="fnref45"><sup>45</sup></a> tomar valores en <span class="math inline">\(\{0, 1\}\)</span>).
La función principal es <a href="https://rdrr.io/pkg/gbm/man/gbm.html"><code>gbm()</code></a> y se suelen considerar los siguientes argumentos:</p>
<div class="sourceCode" id="cb289"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb289-1"><a href="boosting-r.html#cb289-1" aria-hidden="true" tabindex="-1"></a><span class="fu">gbm</span>( formula, <span class="at">distribution =</span> <span class="st">&quot;bernoulli&quot;</span>, data, <span class="at">n.trees =</span> <span class="dv">100</span>, </span>
<span id="cb289-2"><a href="boosting-r.html#cb289-2" aria-hidden="true" tabindex="-1"></a>     <span class="at">interaction.depth =</span> <span class="dv">1</span>, <span class="at">n.minobsinnode =</span> <span class="dv">10</span>, <span class="at">shrinkage =</span> <span class="fl">0.1</span>, </span>
<span id="cb289-3"><a href="boosting-r.html#cb289-3" aria-hidden="true" tabindex="-1"></a>     <span class="at">bag.fraction =</span> <span class="fl">0.5</span>, <span class="at">cv.folds =</span> <span class="dv">0</span>, <span class="at">n.cores =</span> <span class="cn">NULL</span>)</span></code></pre></div>
<ul>
<li><p><code>formula</code> y <code>data</code> (opcional): permiten especificar la respuesta y las variables predictoras de la forma habitual (típicamente <code>respuesta ~ .</code>; también está disponible una interfaz con matrices <code>gbm.fit()</code>).</p></li>
<li><p><code>distribution</code> (opcional): texto con el nombre de la distribución (o lista con el nombre en <code>name</code> y parámetros adicionales en los demás componentes) que determina la función de pérdida.
Si se omite, se establecerá a partir del tipo de la respuesta: <code>"bernouilli"</code> (regresión logística) si es una variable dicotómica 0/1, <code>"multinomial"</code> (regresión multinomial) si es un factor (no se recomienda) y <code>"gaussian"</code> (error cuadrático) en caso contrario.
Otras opciones que pueden ser de interés son: <code>"laplace"</code> (error absoluto), <code>"adaboost"</code> (pérdida exponencial para respuestas dicotómicas 0/1), <code>"huberized"</code> (pérdida de Huber para respuestas dicotómicas 0/1), <code>"poisson"</code> (regresión de Poisson) y <code>"quantile"</code> (regresión cuantil).</p></li>
<li><p><code>ntrees</code>: iteraciones/número de árboles que se crecerán; por defecto 100 (se puede emplear la función <code>gbm.perf()</code> para seleccionar un valor “óptimo”).</p></li>
<li><p><code>interaction.depth</code>: profundidad de los árboles; por defecto 1 (modelo aditivo).</p></li>
<li><p><code>n.minobsinnode</code>: número mínimo de observaciones en un nodo terminal; por defecto 10.</p></li>
<li><p><code>shrinkage</code>: parámetro de regularización <span class="math inline">\(\lambda\)</span>; por defecto 0.1.</p></li>
<li><p><code>bag.fraction</code>: proporción de observaciones seleccionadas al azar para crecer cada árbol; por defecto 0.5.</p></li>
<li><p><code>cv.folds</code>: número de grupos para validación cruzada; por defecto 0 (no se hace validación cruzada). Si se asigna un valor mayor que 1, se realizará validación cruzada y se devolverá el error en la componente <code>$cv.error</code> (se puede emplear para seleccionar hiperparámetros).</p></li>
<li><p><code>n.cores</code>: número de núcleos para el procesamiento en paralelo.</p></li>
</ul>
<p>Como ejemplo emplearemos el conjunto de datos <a href="https://rubenfcasal.github.io/mpae/reference/winequality.html"><code>winequality</code></a>, considerando la variable <code>quality</code> como respuesta:</p>
<div class="sourceCode" id="cb290"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb290-1"><a href="boosting-r.html#cb290-1" aria-hidden="true" tabindex="-1"></a><span class="fu">data</span>(winequality, <span class="at">package =</span> <span class="st">&quot;mpae&quot;</span>)</span>
<span id="cb290-2"><a href="boosting-r.html#cb290-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb290-3"><a href="boosting-r.html#cb290-3" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> winequality</span>
<span id="cb290-4"><a href="boosting-r.html#cb290-4" aria-hidden="true" tabindex="-1"></a>nobs <span class="ot">&lt;-</span> <span class="fu">nrow</span>(df)</span>
<span id="cb290-5"><a href="boosting-r.html#cb290-5" aria-hidden="true" tabindex="-1"></a>itrain <span class="ot">&lt;-</span> <span class="fu">sample</span>(nobs, <span class="fl">0.8</span> <span class="sc">*</span> nobs)</span>
<span id="cb290-6"><a href="boosting-r.html#cb290-6" aria-hidden="true" tabindex="-1"></a>train <span class="ot">&lt;-</span> df[itrain, ]</span>
<span id="cb290-7"><a href="boosting-r.html#cb290-7" aria-hidden="true" tabindex="-1"></a>test <span class="ot">&lt;-</span> df[<span class="sc">-</span>itrain, ]</span></code></pre></div>
<p>Ajustamos el modelo SGB:</p>
<div class="sourceCode" id="cb291"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb291-1"><a href="boosting-r.html#cb291-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(gbm)</span>
<span id="cb291-2"><a href="boosting-r.html#cb291-2" aria-hidden="true" tabindex="-1"></a>gbm.fit <span class="ot">&lt;-</span> <span class="fu">gbm</span>(quality <span class="sc">~</span> ., <span class="at">data =</span> train) <span class="co">#  distribution = &quot;gaussian&quot;</span></span></code></pre></div>
<pre><code>## Distribution not specified, assuming gaussian ...</code></pre>
<div class="sourceCode" id="cb293"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb293-1"><a href="boosting-r.html#cb293-1" aria-hidden="true" tabindex="-1"></a>gbm.fit</span></code></pre></div>
<pre><code>## gbm(formula = quality ~ ., data = train)
## A gradient boosted model with gaussian loss function.
## 100 iterations were performed.
## There were 11 predictors of which 11 had non-zero influence.</code></pre>
<p>El método <code>summary()</code> calcula las medidas de influencia de los predictores y las representa gráficamente (ver Figura <a href="boosting-r.html#fig:gbm-summary">4.12</a>):</p>

<div class="sourceCode" id="cb295"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb295-1"><a href="boosting-r.html#cb295-1" aria-hidden="true" tabindex="-1"></a><span class="fu">summary</span>(gbm.fit)</span></code></pre></div>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:gbm-summary"></span>
<img src="04-bagging_boosting_files/figure-html/gbm-summary-1.png" alt="Importancia de las variables predictoras (con los valores por defecto de gbm())." width="75%" />
<p class="caption">
Figura 4.12: Importancia de las variables predictoras (con los valores por defecto de <code>gbm()</code>).
</p>
</div>
<pre><code>##                                       var rel.inf
## alcohol                           alcohol 40.9080
## volatile.acidity         volatile.acidity 13.8391
## free.sulfur.dioxide   free.sulfur.dioxide 11.4883
## fixed.acidity               fixed.acidity  7.9147
## citric.acid                   citric.acid  6.7659
## total.sulfur.dioxide total.sulfur.dioxide  4.8083
## residual.sugar             residual.sugar  4.7586
## chlorides                       chlorides  3.4245
## sulphates                       sulphates  3.0860
## density                           density  1.9184
## pH                                     pH  1.0882</code></pre>
<p>Para estudiar el efecto de un predictor se pueden generar gráficos de los efectos parciales mediante el método <code>plot()</code>, que llama internamente a las herramientas del paquete <code>pdp</code>.
Por ejemplo, en la Figura <a href="boosting-r.html#fig:gbm-plot">4.13</a> se representan los efectos parciales de los dos predictores más importantes:</p>
<!-- 
Pendiente: 
mejorar la resolución 
-->
<div class="sourceCode" id="cb297"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb297-1"><a href="boosting-r.html#cb297-1" aria-hidden="true" tabindex="-1"></a>p1 <span class="ot">&lt;-</span> <span class="fu">plot</span>(gbm.fit, <span class="at">i =</span> <span class="st">&quot;alcohol&quot;</span>)</span>
<span id="cb297-2"><a href="boosting-r.html#cb297-2" aria-hidden="true" tabindex="-1"></a>p2 <span class="ot">&lt;-</span> <span class="fu">plot</span>(gbm.fit, <span class="at">i =</span> <span class="st">&quot;volatile.acidity&quot;</span>)</span>
<span id="cb297-3"><a href="boosting-r.html#cb297-3" aria-hidden="true" tabindex="-1"></a>gridExtra<span class="sc">::</span><span class="fu">grid.arrange</span>(p1, p2, <span class="at">ncol =</span> <span class="dv">2</span>)</span></code></pre></div>

<div class="figure" style="text-align: center"><span style="display:block;" id="fig:gbm-plot"></span>
<img src="images/gbm-plot-uni-1.png" alt="Efecto parcial del alcohol (panel izquierdo) y la acidez volátil (panel derecho) sobre la respuesta, en el modelo SGB ajustado." width="95%" />
<p class="caption">
Figura 4.13: Efecto parcial del alcohol (panel izquierdo) y la acidez volátil (panel derecho) sobre la respuesta, en el modelo SGB ajustado.
</p>
</div>
<p>Finalmente, podemos evaluar la precisión en la muestra de test empleando el código habitual:</p>
<div class="sourceCode" id="cb298"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb298-1"><a href="boosting-r.html#cb298-1" aria-hidden="true" tabindex="-1"></a>pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(gbm.fit, <span class="at">newdata =</span> test)</span>
<span id="cb298-2"><a href="boosting-r.html#cb298-2" aria-hidden="true" tabindex="-1"></a>obs <span class="ot">&lt;-</span> test<span class="sc">$</span>quality</span>
<span id="cb298-3"><a href="boosting-r.html#cb298-3" aria-hidden="true" tabindex="-1"></a><span class="fu">accuracy</span>(pred, obs)</span></code></pre></div>
<pre><code>##        me      rmse       mae       mpe      mape r.squared 
## -0.014637  0.758621  0.611044 -2.007021 10.697537  0.299176</code></pre>
<p>Este procedimiento también está implementado en el paquete <code>caret</code> seleccionando el método <code>"gbm"</code>, que considera 4 hiperparámetros:</p>
<div class="sourceCode" id="cb300"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb300-1"><a href="boosting-r.html#cb300-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(caret)</span>
<span id="cb300-2"><a href="boosting-r.html#cb300-2" aria-hidden="true" tabindex="-1"></a><span class="fu">modelLookup</span>(<span class="st">&quot;gbm&quot;</span>)</span></code></pre></div>
<pre><code>##   model         parameter                   label forReg forClass
## 1   gbm           n.trees   # Boosting Iterations   TRUE     TRUE
## 2   gbm interaction.depth          Max Tree Depth   TRUE     TRUE
## 3   gbm         shrinkage               Shrinkage   TRUE     TRUE
## 4   gbm    n.minobsinnode Min. Terminal Node Size   TRUE     TRUE
##   probModel
## 1      TRUE
## 2      TRUE
## 3      TRUE
## 4      TRUE</code></pre>
<p>Aunque por defecto la función <code>train()</code> solo considera nueve combinaciones de hiperparámetros.
Para hacer una búsqueda más completa se podría seguir un procedimiento análogo al empleado con el método anterior.
Primero, seleccionamos los hiperparámetros <code>interaction.depth</code> y <code>n.trees</code> (con las opciones por defecto, manteniendo <code>shrinkage</code> y <code>n.minobsinnode</code> fijos, aunque sin imprimir el progreso durante la búsqueda):</p>
<div class="sourceCode" id="cb302"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb302-1"><a href="boosting-r.html#cb302-1" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb302-2"><a href="boosting-r.html#cb302-2" aria-hidden="true" tabindex="-1"></a>trControl <span class="ot">&lt;-</span> <span class="fu">trainControl</span>(<span class="at">method =</span> <span class="st">&quot;cv&quot;</span>, <span class="at">number =</span> <span class="dv">5</span>)</span>
<span id="cb302-3"><a href="boosting-r.html#cb302-3" aria-hidden="true" tabindex="-1"></a>caret.gbm0 <span class="ot">&lt;-</span> <span class="fu">train</span>(quality <span class="sc">~</span> ., <span class="at">method =</span> <span class="st">&quot;gbm&quot;</span>, <span class="at">data =</span> train,</span>
<span id="cb302-4"><a href="boosting-r.html#cb302-4" aria-hidden="true" tabindex="-1"></a>                   <span class="at">trControl =</span> trControl, <span class="at">verbose =</span> <span class="cn">FALSE</span>)</span>
<span id="cb302-5"><a href="boosting-r.html#cb302-5" aria-hidden="true" tabindex="-1"></a>caret.gbm0</span></code></pre></div>
<pre><code>## Stochastic Gradient Boosting 
## 
## 1000 samples
##   11 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 800, 801, 800, 800, 799 
## Resampling results across tuning parameters:
## 
##   interaction.depth  n.trees  RMSE     Rsquared  MAE    
##   1                   50      0.74641  0.29178   0.59497
##   1                  100      0.72583  0.31710   0.57518
##   1                  150      0.72472  0.31972   0.57194
##   2                   50      0.71982  0.33077   0.57125
##   2                  100      0.71750  0.33329   0.56474
##   2                  150      0.72582  0.32220   0.57131
##   3                   50      0.72417  0.31964   0.57226
##   3                  100      0.72721  0.31913   0.57544
##   3                  150      0.73114  0.31529   0.57850
## 
## Tuning parameter &#39;shrinkage&#39; was held constant at a value of 0.1
## 
## Tuning parameter &#39;n.minobsinnode&#39; was held constant at a value of 10
## RMSE was used to select the optimal model using the smallest value.
## The final values used for the model were n.trees =
##  100, interaction.depth = 2, shrinkage = 0.1 and n.minobsinnode = 10.</code></pre>
<p>A continuación elegimos <code>shrinkage</code>, fijando la selección previa de <code>interaction.depth</code> y <code>n.trees</code> (también se podría incluir <code>n.minobsinnode</code> en la búsqueda, pero lo mantenemos fijo para reducir el tiempo de computación):</p>
<div class="sourceCode" id="cb304"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb304-1"><a href="boosting-r.html#cb304-1" aria-hidden="true" tabindex="-1"></a>tuneGrid <span class="ot">&lt;-</span> <span class="fu">data.frame</span>(<span class="at">n.trees =</span>  <span class="dv">100</span>, <span class="at">interaction.depth =</span> <span class="dv">2</span>, </span>
<span id="cb304-2"><a href="boosting-r.html#cb304-2" aria-hidden="true" tabindex="-1"></a>              <span class="at">n.minobsinnode =</span> <span class="dv">10</span>, <span class="at">shrinkage =</span> <span class="fu">c</span>(<span class="fl">0.3</span>, <span class="fl">0.1</span>, <span class="fl">0.05</span>, <span class="fl">0.01</span>, <span class="fl">0.005</span>))</span>
<span id="cb304-3"><a href="boosting-r.html#cb304-3" aria-hidden="true" tabindex="-1"></a>caret.gbm1 <span class="ot">&lt;-</span> <span class="fu">train</span>(quality <span class="sc">~</span> ., <span class="at">method =</span> <span class="st">&quot;gbm&quot;</span>, <span class="at">data =</span> train,</span>
<span id="cb304-4"><a href="boosting-r.html#cb304-4" aria-hidden="true" tabindex="-1"></a>                  <span class="at">tuneGrid =</span> tuneGrid, <span class="at">trControl =</span> trControl, <span class="at">verbose =</span> <span class="cn">FALSE</span>)</span>
<span id="cb304-5"><a href="boosting-r.html#cb304-5" aria-hidden="true" tabindex="-1"></a>caret.gbm1</span></code></pre></div>
<pre><code>## Stochastic Gradient Boosting 
## 
## 1000 samples
##   11 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 800, 800, 801, 799, 800 
## Resampling results across tuning parameters:
## 
##   shrinkage  RMSE     Rsquared  MAE    
##   0.005      0.81549  0.24191   0.62458
##   0.010      0.78443  0.26030   0.61286
##   0.050      0.72070  0.32755   0.57073
##   0.100      0.71248  0.34076   0.56317
##   0.300      0.77208  0.26138   0.60918
## 
## Tuning parameter &#39;n.trees&#39; was held constant at a value of 100
## 
## Tuning parameter &#39;interaction.depth&#39; was held constant at a value of
##  2
## Tuning parameter &#39;n.minobsinnode&#39; was held constant at a value of 10
## RMSE was used to select the optimal model using the smallest value.
## The final values used for the model were n.trees =
##  100, interaction.depth = 2, shrinkage = 0.1 and n.minobsinnode = 10.</code></pre>
<!-- varImp(caret.gbm1) -->
<p>Por último, evaluamos el modelo resultante en la muestra de test:</p>
<div class="sourceCode" id="cb306"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb306-1"><a href="boosting-r.html#cb306-1" aria-hidden="true" tabindex="-1"></a>pred <span class="ot">&lt;-</span> <span class="fu">predict</span>(caret.gbm1, <span class="at">newdata =</span> test)</span>
<span id="cb306-2"><a href="boosting-r.html#cb306-2" aria-hidden="true" tabindex="-1"></a><span class="fu">accuracy</span>(pred, obs)</span></code></pre></div>
<pre><code>##        me      rmse       mae       mpe      mape r.squared 
## -0.020136  0.740377  0.601728 -1.984661 10.530266  0.332479</code></pre>
<!-- 
Pendiente: ejercicio regresión con el conjunto de datos Boston empleando error absoluto para evitar la influencia de datos atípicos. 
-->
<div class="exercise">
<p><span id="exr:bfan-gbm" class="exercise"><strong>Ejercicio 4.4  </strong></span>Repite los pasos del ejemplo anterior (empleando el método <code>gbm</code> del paquete <code>caret</code>, seleccionando primero los hiperparámetros <code>interaction.depth</code> y <code>n.trees</code> con las opciones por defecto, y posteriormente <code>shrinkage</code> fijando la selección previa de los otros parámetros), empleando el conjunto de datos <a href="https://rubenfcasal.github.io/mpae/reference/bodyfat.html"><code>bodyfat</code></a> del paquete <a href="https://CRAN.R-project.org/package=mpae"><code>mpae</code></a> y considerando como respuesta la variable <code>bodyfat</code> (porcentaje de grasa corporal).</p>
</div>
</div>
<div id="xgb-caret" class="section level3 hasAnchor" number="4.5.3">
<h3><span class="header-section-number">4.5.3</span> Ejemplo: XGBoost con el paquete <code>caret</code><a href="boosting-r.html#xgb-caret" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<p>El método boosting implementado en el paquete <a href="https://github.com/dmlc/xgboost/tree/master/R-package"><code>xgboost</code></a> <span class="citation">(<a href="#ref-R-xgboost" role="doc-biblioref">Chen et al., 2023</a>)</span> es uno de los más populares hoy en día.
Esta implementación proporciona parámetros adicionales de regularización para controlar la complejidad del modelo y tratar de evitar el sobreajuste.
También incluye criterios de parada para detener la evaluación del modelo cuando los árboles adicionales no ofrecen ninguna mejora.
El paquete dispone de una interfaz simple, <code>xgboost()</code>, y otra más avanzada, <code>xgb.train()</code>, que admite funciones de pérdida y evaluación personalizadas.
Normalmente es necesario un preprocesado de los datos antes de llamar a estas funciones, ya que requieren de una matriz para los predictores y de un vector para la respuesta; además, en el caso de que la respuesta sea dicotómica debe tomar valores en <span class="math inline">\(\{0, 1\}\)</span>). Por tanto, es necesario recodificar las variables categóricas como numéricas.
Por este motivo, puede ser preferible emplear la interfaz de <code>caret</code>.</p>
<p>El algoritmo estándar <em>XGBoost</em>, que emplea árboles como modelo base, está implementado en el método <code>"xgbTree"</code> de <code>caret</code><a href="#fn46" class="footnote-ref" id="fnref46"><sup>46</sup></a>:</p>
<div class="sourceCode" id="cb308"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb308-1"><a href="boosting-r.html#cb308-1" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(caret)</span>
<span id="cb308-2"><a href="boosting-r.html#cb308-2" aria-hidden="true" tabindex="-1"></a><span class="co"># names(getModelInfo(&quot;xgb&quot;))</span></span>
<span id="cb308-3"><a href="boosting-r.html#cb308-3" aria-hidden="true" tabindex="-1"></a><span class="fu">modelLookup</span>(<span class="st">&quot;xgbTree&quot;</span>)</span></code></pre></div>
<pre><code>##     model        parameter                          label forReg
## 1 xgbTree          nrounds          # Boosting Iterations   TRUE
## 2 xgbTree        max_depth                 Max Tree Depth   TRUE
## 3 xgbTree              eta                      Shrinkage   TRUE
## 4 xgbTree            gamma         Minimum Loss Reduction   TRUE
## 5 xgbTree colsample_bytree     Subsample Ratio of Columns   TRUE
## 6 xgbTree min_child_weight Minimum Sum of Instance Weight   TRUE
## 7 xgbTree        subsample           Subsample Percentage   TRUE
##   forClass probModel
## 1     TRUE      TRUE
## 2     TRUE      TRUE
## 3     TRUE      TRUE
## 4     TRUE      TRUE
## 5     TRUE      TRUE
## 6     TRUE      TRUE
## 7     TRUE      TRUE</code></pre>
<p>Este método considera los siguientes hiperparámetros:</p>
<ul>
<li><p><code>"nrounds"</code>: número de iteraciones boosting.</p></li>
<li><p><code>"max_depth"</code>: profundidad máxima del árbol; por defecto 6.</p></li>
<li><p><code>"eta"</code>: parámetro de regularización <span class="math inline">\(\lambda\)</span>; por defecto 0.3.</p></li>
<li><p><code>"gamma"</code>: mínima reducción de la pérdida para hacer una partición adicional en un nodo del árbol; por defecto 0.</p></li>
<li><p><code>"colsample_bytree"</code>: proporción de predictores seleccionados al azar para crecer cada árbol; por defecto 1.</p></li>
<li><p><code>"min_child_weight"</code>: suma mínima de peso (hessiana) para hacer una partición adicional en un nodo del árbol; por defecto 1.</p></li>
<li><p><code>"subsample"</code>: proporción de observaciones seleccionadas al azar en cada iteración boosting; por defecto 1.</p></li>
</ul>
<p>Para más información sobre parámetros adicionales, se puede consultar la ayuda de <a href="https://rdrr.io/pkg/xgboost/man/xgb.train.html"><code>xgboost::xgboost()</code></a> o la lista detallada disponible en la Sección <a href="https://xgboost.readthedocs.io/en/latest/parameter.html">XGBoost Parameters</a> del <a href="https://xgboost.readthedocs.io">manual de XGBoost</a>.</p>
<p>A modo de ejemplo, consideraremos un problema de clasificación empleando de nuevo el conjunto de datos de calidad de vino:</p>
<div class="sourceCode" id="cb310"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb310-1"><a href="boosting-r.html#cb310-1" aria-hidden="true" tabindex="-1"></a><span class="co"># data(winetaste, package = &quot;mpae&quot;)</span></span>
<span id="cb310-2"><a href="boosting-r.html#cb310-2" aria-hidden="true" tabindex="-1"></a><span class="fu">set.seed</span>(<span class="dv">1</span>)</span>
<span id="cb310-3"><a href="boosting-r.html#cb310-3" aria-hidden="true" tabindex="-1"></a>df <span class="ot">&lt;-</span> winetaste</span>
<span id="cb310-4"><a href="boosting-r.html#cb310-4" aria-hidden="true" tabindex="-1"></a>nobs <span class="ot">&lt;-</span> <span class="fu">nrow</span>(df)</span>
<span id="cb310-5"><a href="boosting-r.html#cb310-5" aria-hidden="true" tabindex="-1"></a>itrain <span class="ot">&lt;-</span> <span class="fu">sample</span>(nobs, <span class="fl">0.8</span> <span class="sc">*</span> nobs)</span>
<span id="cb310-6"><a href="boosting-r.html#cb310-6" aria-hidden="true" tabindex="-1"></a>train <span class="ot">&lt;-</span> df[itrain, ]</span>
<span id="cb310-7"><a href="boosting-r.html#cb310-7" aria-hidden="true" tabindex="-1"></a>test <span class="ot">&lt;-</span> df[<span class="sc">-</span>itrain, ]</span></code></pre></div>
<p>En este caso, la función <code>train()</code> considera por defecto 108 combinaciones de hiperparámetros y el tiempo de computación puede ser excesivo<a href="#fn47" class="footnote-ref" id="fnref47"><sup>47</sup></a> (en este caso sería recomendable emplear computación en paralelo, ver por ejemplo el <a href="https://topepo.github.io/caret/parallel-processing.html">Capítulo 9</a> del <a href="https://topepo.github.io/caret/">manual de caret</a>, e incluso con búsqueda aleatoria en lugar de evaluar en una rejilla completa, incluyendo <code>search = "random"</code> en <code>trainControl()</code><a href="#fn48" class="footnote-ref" id="fnref48"><sup>48</sup></a>):</p>
<div class="sourceCode" id="cb311"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb311-1"><a href="boosting-r.html#cb311-1" aria-hidden="true" tabindex="-1"></a>caret.xgb <span class="ot">&lt;-</span> <span class="fu">train</span>(taste <span class="sc">~</span> ., <span class="at">method =</span> <span class="st">&quot;xgbTree&quot;</span>, <span class="at">data =</span> train,</span>
<span id="cb311-2"><a href="boosting-r.html#cb311-2" aria-hidden="true" tabindex="-1"></a>                   <span class="at">trControl =</span> trControl, <span class="at">verbosity =</span> <span class="dv">0</span>)</span>
<span id="cb311-3"><a href="boosting-r.html#cb311-3" aria-hidden="true" tabindex="-1"></a>caret.xgb</span></code></pre></div>
<pre><code>## eXtreme Gradient Boosting 
## 
## 1000 samples
##   11 predictor
##    2 classes: &#39;bad&#39;, &#39;good&#39; 
## 
## No pre-processing
## Resampling: Cross-Validated (5 fold) 
## Summary of sample sizes: 799, 801, 801, 799, 800 
## Resampling results across tuning parameters:
## 
##   eta  max_depth  colsample_bytree  subsample  nrounds  Accuracy
##   0.3  1          0.6               0.50        50      0.74795 
##   0.3  1          0.6               0.50       100      0.75096 
##   0.3  1          0.6               0.50       150      0.74802 
##   0.3  1          0.6               0.75        50      0.73895 
##   0.3  1          0.6               0.75       100      0.74996 
##   0.3  1          0.6               0.75       150      0.75199 
##   0.3  1          0.6               1.00        50      0.74794 
##   0.3  1          0.6               1.00       100      0.74395 
##   Kappa  
##   0.39977
##   0.42264
##   0.41424
##   0.37757
##   0.41789
##   0.41944
##   0.39332
##   0.39468
##  [ reached getOption(&quot;max.print&quot;) -- omitted 100 rows ]
## 
## Tuning parameter &#39;gamma&#39; was held constant at a value of 0
## 
## Tuning parameter &#39;min_child_weight&#39; was held constant at a value of 1
## Accuracy was used to select the optimal model using the largest value.
## The final values used for the model were nrounds = 100, max_depth =
##  2, eta = 0.4, gamma = 0, colsample_bytree = 0.8, min_child_weight =
##  1 and subsample = 1.</code></pre>
<p>Al imprimir el resultado del ajuste, observamos que fija los valores de los hiperparámetros <code>gamma</code> y <code>min_child_weight</code>.
Adicionalmente, se podría seguir una estrategia de selección de los hiperparámetros similar a la empleada en los métodos anteriores, alternando la búsqueda de los valores óptimos de distintos grupos de hiperparámetros.</p>
<div class="sourceCode" id="cb313"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb313-1"><a href="boosting-r.html#cb313-1" aria-hidden="true" tabindex="-1"></a>caret.xgb<span class="sc">$</span>bestTune</span></code></pre></div>
<pre><code>##    nrounds max_depth eta gamma colsample_bytree min_child_weight subsample
## 89     100         2 0.4     0              0.8                1         1</code></pre>
<p>En este caso, en un siguiente paso, podríamos seleccionar <code>gamma</code> y <code>min_child_weight</code> manteniendo fijos <code>nrounds = 100</code>, <code>max_depth = 2</code>, <code>eta = 0.4</code>, <code>colsample_bytree = 0.8</code> y <code>subsample = 1</code>.</p>
<p>Al finalizar, evaluaríamos el modelo resultante en la muestra de test:</p>
<div class="sourceCode" id="cb315"><pre class="sourceCode r"><code class="sourceCode r"><span id="cb315-1"><a href="boosting-r.html#cb315-1" aria-hidden="true" tabindex="-1"></a><span class="fu">confusionMatrix</span>(<span class="fu">predict</span>(caret.xgb, <span class="at">newdata =</span> test), test<span class="sc">$</span>taste)</span></code></pre></div>
<pre><code>## Confusion Matrix and Statistics
## 
##           Reference
## Prediction bad good
##       bad   38   19
##       good  46  147
##                                         
##                Accuracy : 0.74          
##                  95% CI : (0.681, 0.793)
##     No Information Rate : 0.664         
##     P-Value [Acc &gt; NIR] : 0.00584       
##                                         
##                   Kappa : 0.367         
##                                         
##  Mcnemar&#39;s Test P-Value : 0.00126       
##                                         
##             Sensitivity : 0.452         
##             Specificity : 0.886         
##          Pos Pred Value : 0.667         
##          Neg Pred Value : 0.762         
##              Prevalence : 0.336         
##          Detection Rate : 0.152         
##    Detection Prevalence : 0.228         
##       Balanced Accuracy : 0.669         
##                                         
##        &#39;Positive&#39; Class : bad           
## </code></pre>
<div class="exercise">
<p><span id="exr:xgb-tune-iter" class="exercise"><strong>Ejercicio 4.5  </strong></span>Considera el ajuste anterior <code>caret.xgb</code> como un paso inicial en la selección de hiperparámetros y busca valores óptimos para todos ellos de forma iterativa hasta convergencia.</p>
</div>

</div>
</div>
<!-- </div> -->
<h3>Bibliografía<a href="bibliografía.html#bibliografía" class="anchor-section" aria-label="Anchor link to header"></a></h3>
<div id="refs" class="references csl-bib-body hanging-indent" line-spacing="2">
<div id="ref-R-xgboost" class="csl-entry">
Chen, T., He, T., Benesty, M., Khotilovich, V., Tang, Y., Cho, H., Chen, K., Mitchell, R., Cano, I., Zhou, T., et al. (2023). <em><span>xgboost: Extreme Gradient Boosting</span></em>. <a href="https://cran.r-project.org/package=xgboost">https://cran.r-project.org/package=xgboost</a>
</div>
<div id="ref-culp2006ada" class="csl-entry">
Culp, M., Johnson, K., y Michailidis, G. (2006). ada: An R Package for Stochastic Boosting. <em>Journal of Statistical Software</em>, <em>17</em>(2), 1-27. <a href="https://doi.org/10.18637/jss.v017.i02">https://doi.org/10.18637/jss.v017.i02</a>
</div>
<div id="ref-friedman2002stochastic" class="csl-entry">
Friedman, J. (2002). Stochastic gradient boosting. <em>Computational Statistics &amp; data analysis</em>, <em>38</em>(4), 367-378. <a href="https://doi.org/10.1016/S0167-9473(01)00065-2">https://doi.org/10.1016/S0167-9473(01)00065-2</a>
</div>
<div id="ref-friedman2000additive" class="csl-entry">
Friedman, J., Hastie, T., y Tibshirani, R. (2000). Additive logistic regression: a statistical view of boosting (with discussion and a rejoinder by the authors). <em>The Annals of Statistics</em>, <em>28</em>(2), 337-407. <a href="https://doi.org/10.1214/aos/1016218223">https://doi.org/10.1214/aos/1016218223</a>
</div>
<div id="ref-R-gbm" class="csl-entry">
Greenwell, B., Boehmke, B., Cunningham, J., y Developers, G. (2022). <em><span>gbm: Generalized Boosted Regression Models</span></em>. <a href="https://cran.r-project.org/package=gbm">https://cran.r-project.org/package=gbm</a>
</div>
<div id="ref-vinayak2015dart" class="csl-entry">
Vinayak, R. K., y Gilad-Bachrach, R. (2015). Dart: Dropouts meet multiple additive regression trees. <em>Artificial Intelligence and Statistics</em>, 489-497.
</div>
</div>
<div class="footnotes">
<hr />
<ol start="45">
<li id="fn45"><p>Se puede evitar este inconveniente empleando la interfaz de <code>caret</code>.<a href="boosting-r.html#fnref45" class="footnote-back">↩︎</a></p></li>
<li id="fn46"><p>Otras alternativas son: <code>"xgbDART"</code> que también emplean árboles como modelo base, pero incluye el método DART <span class="citation">(<a href="#ref-vinayak2015dart" role="doc-biblioref">Vinayak y Gilad-Bachrach, 2015</a>)</span> para evitar sobreajuste (básicamente descarta árboles al azar en la secuencia), y <code>"xgbLinear"</code> que emplea modelos lineales.<a href="boosting-r.html#fnref46" class="footnote-back">↩︎</a></p></li>
<li id="fn47"><p>Además, se establece <code>verbosity = 0</code> para evitar (cientos de) mensajes de advertencia:
<code>WARNING: src/c_api/c_api.cc:935: "ntree_limit" is deprecated, use "iteration_range" instead</code>.<a href="boosting-r.html#fnref47" class="footnote-back">↩︎</a></p></li>
<li id="fn48"><p>El parámetro <code>tuneLength</code> especificaría el número total de combinaciones de parámetros que se evaluarían.<a href="boosting-r.html#fnref48" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="boosting.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="svm.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": "https://github.com/rubenfcasal/book_mpae/edit/master/04-bagging_boosting.Rmd",
"text": "Edit"
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": null,
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.9/latest.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
